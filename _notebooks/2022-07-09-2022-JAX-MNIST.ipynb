{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-series",
   "metadata": {},
   "source": [
    "# MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723eea1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "burning-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List, NamedTuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-language",
   "metadata": {},
   "source": [
    "## Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a5526e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NamedTuple):\n",
    "    name: str\n",
    "    init: Callable \n",
    "    forward: Callable \n",
    "    def __repr__(self):\n",
    "        return self.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "dfef812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(input_dim: int, output_dim: int):\n",
    "    def init():    \n",
    "        key = jax.random.PRNGKey(1234)\n",
    "        params = {\n",
    "            'weights': jax.random.normal(key, (output_dim, input_dim)) * jnp.sqrt(2.0 / input_dim),\n",
    "            'bias': jnp.zeros(output_dim)\n",
    "        }\n",
    "        return params        \n",
    "    def forward(params, x):\n",
    "        W, b = params['weights'], params['bias']\n",
    "        return jnp.dot(W, x) + b \n",
    "\n",
    "    return Model(init=init, forward=forward, name=f'Linear({input_dim}, {output_dim})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5edfe732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sequential(*layers):\n",
    "    def init():\n",
    "        params = []\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, Model):\n",
    "                params.append(layer.init())\n",
    "        return params    \n",
    "    def forward(params, x):\n",
    "        activation = x\n",
    "        i = 0\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, Model):\n",
    "                activation = layer.forward(params[i], activation)\n",
    "                i += 1\n",
    "            else:\n",
    "                activation = layer(activation)\n",
    "        return activation\n",
    "    return Model(init=init, forward=forward, name='Sequential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f2c8d",
   "metadata": {},
   "source": [
    "The type of model we're looking to build can be represented in Keras, as follows:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28,28)),\n",
    "  keras.layers.Dense(128, activation=keras.activations.relu),\n",
    "  keras.layers.Dense(10, activation=keras.activations.softmax)                          \n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c75f09b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential\n"
     ]
    }
   ],
   "source": [
    "# maybe the parameters buried in the model are a problem, we need to differentiate with respect to them...\n",
    "model = Sequential(\n",
    "    Linear(28*28, 128),\n",
    "    jax.nn.relu,\n",
    "    Linear(128, 10)   \n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-spirit",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The loss function takes the current set of parameters, the model, and the data and calculates the distance between the model\n",
    "predictions and the targets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "instructional-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters pulled from model, this seems really clunky ...\n",
    "@jax.value_and_grad\n",
    "def cross_entropy_loss(params, forward, X, y_one_hot):\n",
    "    logits = jax.vmap(forward, in_axes=(None, 0))(params, X) \n",
    "    logsoftmax = logits - jax.nn.logsumexp(logits)\n",
    "    loss = -(logsoftmax * y_one_hot).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1541b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init()\n",
    "y = jax.nn.one_hot([0,2,1,3,4], 10)\n",
    "a, b = cross_entropy_loss(params, model.forward, np.random.randn(5, 28*28), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-knock",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training loop tunes the model parameters for a specified number of epochs.  For now the parameters are adjusted once per epoch, after the entire dataset is consumed by the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "breeding-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, lr=1e-2, num_epochs=50, loss=mse_loss):\n",
    "    \n",
    "    X, y = data\n",
    "    loss_vals = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "    # initialize the model parameters\n",
    "    params = {'w': jnp.array([0,0,0], dtype=jnp.float32), 'b': 0.0}\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        loss_i, grad_params_i = loss(params, model, X, y)\n",
    "        for key in params:\n",
    "            params[key] -= lr*grad_params_i[key]\n",
    "    \n",
    "        loss_vals[i] = loss_i\n",
    "    return loss_vals, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-finder",
   "metadata": {},
   "source": [
    "## Performance Curve\n",
    "\n",
    "Let's see the trend in the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-hudson",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66e6e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(10, 3)\n",
    "w = np.random.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5f2b063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95615652, -0.60910943,  0.47719404,  0.50628421, -0.57886369],\n",
       "       [ 3.09498684,  1.70024379, -1.01957485,  2.25813896, -0.37952626],\n",
       "       [-3.67871561, -0.1351104 ,  0.04294664, -1.84479421,  0.98852387],\n",
       "       [-1.00596024,  0.46315551,  0.67104569,  3.13116358, -1.09329311],\n",
       "       [ 2.07878921,  1.88358723, -1.42939824,  0.84352964,  0.39912581],\n",
       "       [ 2.91327168, -1.07647233,  0.73089571,  1.0445742 , -1.1687821 ],\n",
       "       [ 1.03321149,  0.51671102,  0.70734573,  4.30933842, -1.72017219],\n",
       "       [-2.73799462, -1.54710401,  1.83484434,  1.17423297, -1.08464385],\n",
       "       [-1.6715637 , -2.74092988,  1.91974295, -1.18807747, -0.68628707],\n",
       "       [ 1.21170605,  0.15311168, -0.1815407 ,  0.30359954, -0.13915325]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, np.transpose(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9b25ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9561565  -0.6091094   0.47719404  0.50628424 -0.5788637 ]\n",
      " [ 3.0949867   1.7002438  -1.0195749   2.258139   -0.37952614]\n",
      " [-3.6787155  -0.13511032  0.04294658 -1.8447943   0.9885239 ]\n",
      " [-1.0059603   0.46315545  0.67104566  3.1311636  -1.093293  ]\n",
      " [ 2.0787892   1.8835871  -1.4293982   0.8435297   0.39912578]\n",
      " [ 2.9132717  -1.0764723   0.7308957   1.044574   -1.1687821 ]\n",
      " [ 1.0332114   0.516711    0.7073457   4.309338   -1.7201722 ]\n",
      " [-2.7379947  -1.547104    1.8348444   1.174233   -1.0846438 ]\n",
      " [-1.6715636  -2.7409298   1.919743   -1.1880776  -0.686287  ]\n",
      " [ 1.2117062   0.15311167 -0.1815407   0.3035995  -0.13915324]]\n"
     ]
    }
   ],
   "source": [
    "def lin(x): return jnp.dot(w, x)\n",
    "\n",
    "yy = jax.vmap(lin)(X)\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb8757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
