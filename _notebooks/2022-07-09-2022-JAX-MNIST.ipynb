{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-series",
   "metadata": {},
   "source": [
    "# Fashion MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723eea1",
   "metadata": {},
   "source": [
    "My main objective here is to develop a Keras like API for JAX, and use it on the so-called Fashion MNIST dataset.\n",
    "\n",
    "The type of model we're looking to build can be represented in Keras, as follows:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28,28)),\n",
    "  keras.layers.Dense(128, activation=keras.activations.relu),\n",
    "  keras.layers.Dense(10, activation=keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "burning-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 13:57:52.214672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/bin:/usr/local/lib:\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optax \n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import Tuple, List, Any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c7223",
   "metadata": {},
   "source": [
    "## Grabbing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6f0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4884b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 60000\n",
      "Number of test samples = 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training samples = {len(y_train)}') \n",
    "print(f'Number of test samples = {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905a2971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = X_train[0]\n",
    "np.set_printoptions(linewidth=200)\n",
    "plt.imshow(X, cmap='gray')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f925bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-language",
   "metadata": {},
   "source": [
    "## Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5526e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model: \n",
    "    def forward(self, params, X, y):\n",
    "        raise NotImplementedError\n",
    "    def compile(self, loss, optimizer, metrics, params):\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer \n",
    "        self.metrics = metrics \n",
    "        self.params = params\n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        train_dataset = train_dataset.shuffle(100).batch(32)\n",
    "        \n",
    "        loss = self.loss \n",
    "        optimizer = self.optimizer \n",
    "        params = self.params\n",
    "        # Get the parameters from the model ...\n",
    "\n",
    "        # Taken from optax tutorial code ...\n",
    "        opt_state = self.optimizer.init(params)\n",
    "        \n",
    "        def step(params, opt_state, batch, labels):\n",
    "            loss_value, grads = jax.value_and_grad(loss)(params, self.forward, batch, labels)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return params, opt_state, loss_value\n",
    "\n",
    "        # one epoch\n",
    "        for i, (batch, labels) in enumerate(train_dataset.as_numpy_iterator()):\n",
    "            loss_value, _ = jax.value_and_grad(loss)(params, self.forward, batch, labels)\n",
    "            params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
    "            if i % 100 == 0:\n",
    "                print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "377559ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network: pass\n",
    "class Function: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb0c53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Network):\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    input_shape: Tuple[int]\n",
    "\n",
    "    def __init__(self, num_features, activation=jax.nn.relu):\n",
    "        self.num_features = num_features \n",
    "        self.activation = activation\n",
    "    def setup(self, input_shape, seed=1234):\n",
    "        self.input_shape = input_shape\n",
    "        shape = (self.num_features, self.input_shape)\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "\n",
    "        self.w = jax.random.normal(key, shape) * jnp.sqrt(2.0 / input_shape)\n",
    "        self.b = jnp.zeros(self.num_features)\n",
    "    @property\n",
    "    def params(self):\n",
    "        return {'w': self.w, 'b': self.b}\n",
    "    def __call__(self, x):\n",
    "        return self.activation(jnp.dot(self.w, x) + self.b) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4609b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_Dense(layer: Dense):\n",
    "    params = (layer.w, layer.b)\n",
    "    extra_stuff = {\n",
    "        'num_features': layer.num_features,\n",
    "        'activation': layer.activation,\n",
    "        'input_shape': layer.input_shape\n",
    "    }\n",
    "\n",
    "    return params, extra_stuff\n",
    "\n",
    "def unflatten_Dense(extra_stuff, params) -> Dense:\n",
    "    layer = Dense(num_features=extra_stuff['num_features'], activation=extra_stuff['activation'])\n",
    "    layer.w = params[0]\n",
    "    layer.b = params[1]\n",
    "    layer.input_shape = extra_stuff['input_shape']\n",
    "    return layer\n",
    "    \n",
    "jax.tree_util.register_pytree_node(\n",
    "    Dense, flatten_Dense, unflatten_Dense    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76849af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([DeviceArray([[-0.77767605, -1.6257328 ,  0.8562147 ],\n",
       "               [ 0.2656164 , -0.91964155,  0.33663693],\n",
       "               [ 1.6229765 ,  0.22684166, -1.2966851 ],\n",
       "               [ 0.23250706,  1.2053852 ,  0.23687442],\n",
       "               [-0.3150987 ,  0.7502025 ,  1.0367401 ],\n",
       "               [-0.76967674,  0.45772657, -1.1808424 ],\n",
       "               [-1.3382986 , -0.559498  ,  0.28776148],\n",
       "               [ 0.20669922, -1.457551  ,  0.82045996],\n",
       "               [-0.27901036,  0.01738252,  0.06019784],\n",
       "               [ 0.00342628,  0.37365758,  0.5010083 ]], dtype=float32),\n",
       "  DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)],\n",
       " PyTreeDef(CustomNode(<class '__main__.Dense'>[{'num_features': 10, 'activation': <jax._src.custom_derivatives.custom_jvp object at 0x7fae45f02e50>, 'input_shape': 3}], [*, *])))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = Dense(10)\n",
    "l.setup(input_shape=3)\n",
    "jax.tree_util.tree_flatten(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1482a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Function):\n",
    "    input_shape: Tuple[int]\n",
    "    num_features: int \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_features= np.prod(input_shape) \n",
    "    def __call__(self, x):\n",
    "        return jnp.reshape(x, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c6eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten_Flatten(layer: Flatten):\n",
    "    params = []\n",
    "    extra_stuff = { 'input_shape': layer.input_shape }\n",
    "    return params, extra_stuff\n",
    "\n",
    "def unflatten_Flatten(extra_stuff, params) -> Dense:\n",
    "    return Flatten(input_shape=extra_stuff['input_shape'])\n",
    "    \n",
    "jax.tree_util.register_pytree_node(\n",
    "    Flatten, flatten_Flatten, unflatten_Flatten    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e03ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " PyTreeDef(CustomNode(<class '__main__.Flatten'>[{'input_shape': (28, 28)}], [])))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = Flatten((28,28))\n",
    "jax.tree_util.tree_flatten(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8490f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Network):\n",
    "    layers: List\n",
    "    params: Any \n",
    "    input_shape: Tuple[int]\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        self.params = []\n",
    "    def setup(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        input_shape = self.input_shape\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Network):\n",
    "                layer.setup(input_shape)\n",
    "                self.params.append(layer.params)\n",
    "                input_shape = layer.num_features\n",
    "            elif isinstance(layer, Flatten): \n",
    "                input_shape = layer.num_features   \n",
    "    def __call__(self, x):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0ca33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 5, 7], PyTreeDef([{'a': *, 'b': *}, {'c': *}]))\n"
     ]
    }
   ],
   "source": [
    "print(jax.tree_util.tree_flatten([{'a': 1, 'b': 5}, {'c': 7}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be83e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to revise this inorder to get the parameters back into the model\n",
    "def flatten_Sequential(model: Sequential):\n",
    "    params = model.params\n",
    "    extra_stuff = {'layers': model.layers}\n",
    "    return params, extra_stuff\n",
    "\n",
    "def unflatten_Sequential(extra_stuff, params) -> Dense:\n",
    "    layers = extra_stuff['layers']\n",
    "    model = Sequential(*layers)\n",
    "    model.setup(input_shape=extra_stuff['input_shape'])\n",
    "    return model\n",
    "    \n",
    "jax.tree_util.register_pytree_node(\n",
    "    Sequential, flatten_Sequential, unflatten_Sequential    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e871ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation=jax.nn.relu),\n",
    "    Dense(10, activation = jax.nn.softmax)   \n",
    ")\n",
    "model.setup(input_shape=(28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "159a80c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.00503162, -0.11710759,  0.05479915, ..., -0.08478684, -0.01258498, -0.0106819 ],\n",
       "             [-0.02930931, -0.06342698, -0.07211312, ..., -0.06885359,  0.05326153, -0.00705581],\n",
       "             [-0.01227331, -0.08667105,  0.01609307, ..., -0.07811324,  0.01763334,  0.00712718],\n",
       "             ...,\n",
       "             [-0.00622975, -0.08475864, -0.0552172 , ...,  0.05935797,  0.0743569 , -0.00805993],\n",
       "             [ 0.10648176, -0.02254054, -0.03166944, ..., -0.01719078,  0.00110678,  0.0392415 ],\n",
       "             [-0.037219  , -0.02834321,  0.00695875, ..., -0.0069246 ,  0.04329436,  0.07048796]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params\n",
    "l = jax.tree_util.tree_flatten(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f2c8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "advisory-spirit",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b689993",
   "metadata": {},
   "source": [
    "In Equinox and Treex, loss functions in which the model is the first argument are jitted.  I can't get this to work now (I think), because I'm not \n",
    "converting my module instantiation to a PyTree.  This seems to be the secret: Treex and Equinox found a way to convert their versions of modules to pytrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eaf9840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': DeviceArray([[ 0.4955482 ,  0.31323454, -0.16431722, -0.1778598 , -0.37309384, -0.27601704,  0.6786144 , -0.83603036, -0.37456733, -0.38529804],\n",
       "              [-0.51664037,  0.23452531,  0.39358795, -0.81366616, -0.30139554,  0.12572241, -0.27614236,  0.04341735,  1.0657046 ,  1.0121646 ],\n",
       "              [-0.02414936,  0.7453478 , -0.28615332, -0.196245  ,  0.33748606,  0.9765807 , -0.12719607,  0.329515  , -0.4071786 ,  0.12246487],\n",
       "              [-0.68219537,  0.03157214,  0.57286984, -0.44964808,  0.3903244 ,  0.06237474, -0.3355008 ,  1.0056379 , -0.29798067, -0.05773307],\n",
       "              [-0.98469573, -0.32533288,  0.1938749 , -0.286879  , -0.3740165 , -1.1384696 , -1.29627   ,  0.1767095 , -0.39158744, -0.80702335],\n",
       "              [ 0.45510042,  0.4024033 , -0.0216945 ,  0.2830267 ,  0.0508937 , -0.0493672 ,  0.16445382,  0.7305577 ,  0.59985423, -0.28474948],\n",
       "              [ 0.06098274, -0.40136126,  0.16223292, -0.4865787 ,  0.04620648, -0.25004682, -0.24693362, -0.08435134,  0.13579482, -0.7653634 ],\n",
       "              [ 0.37850198, -0.21638699,  0.3038623 , -0.12869892,  0.7800084 ,  0.5768901 , -1.0877036 , -0.7946388 , -0.38121703, -0.38984403],\n",
       "              [ 0.374261  ,  0.24334376, -0.04227831, -0.25231916,  0.34803241,  0.9141584 ,  0.05031163, -0.29306245,  0.131548  , -0.05075309],\n",
       "              [-0.6140513 ,  0.05466658,  0.02913119,  0.10320958, -0.94363356,  0.60672456,  0.24040386,  0.237728  , -0.05438365, -0.28856188]], dtype=float32),\n",
       " 'bias': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Dense(10)\n",
    "a.setup(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a6e920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb5b16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit #- this fails because model is not a pytree\n",
    "def loss(model, X, y):\n",
    "    # Need to one-bot encode input labels, recall that MNIST has 10 classes, 0-9\n",
    "    y_one_hot = jax.nn.one_hot(y, 10).astype(jnp.float32)\n",
    "    logits = jax.vmap(model)(X) \n",
    "    # calculate loss for each sample in minibatch\n",
    "    #loss = optax.softmax_cross_entropy(logits, y_one_hot)\n",
    "    # average the losses for each sample in minibatch\n",
    "    #loss = loss.mean()\n",
    "    loss = logits.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "instructional-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters pulled from model, this seems really clunky ...\n",
    "def cross_entropy_loss(params, forward, X, y):\n",
    "    # Need to one-bot encode input labels, recall that MNIST has 10 classes, 0-9\n",
    "    y_one_hot = jax.nn.one_hot(y, 10).astype(jnp.float32)\n",
    "    logits = jax.vmap(forward, in_axes=(None, 0))(params, X) \n",
    "    # calculate loss for each sample in minibatch\n",
    "    loss = optax.softmax_cross_entropy(logits, y_one_hot)\n",
    "    # average the losses for each sample in minibatch\n",
    "    loss = loss.mean() \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994d05b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.70658493, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Dense(10)\n",
    "model.setup(10)\n",
    "\n",
    "model.w\n",
    "loss(model, np.random.randn(5, 10), [1,0,2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6bfca9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument '<__main__.Sequential object at 0x7f1eabc21eb0>' of type <class '__main__.Sequential'> is not a valid JAX type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_83295/1912331193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_check_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2973\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_valid_jaxtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2974\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Argument '{arg}' of type {type(arg)} is not a valid JAX type.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2976\u001b[0m \u001b[0;31m# TODO(mattjj,necula): this duplicates code in core.valid_jaxtype, but one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument '<__main__.Sequential object at 0x7f1eabc21eb0>' of type <class '__main__.Sequential'> is not a valid JAX type."
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation=jax.nn.relu),\n",
    "    Dense(10, activation = jax.nn.softmax)   \n",
    ")\n",
    "\n",
    "\n",
    "init_params = model.setup(input_shape=(28, 28))\n",
    "\n",
    "loss(model, np.random.randn(5, 28, 28), [1,0,2,3,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "b6da26a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.02157694, 0.09008522, 0.01330612, 0.14697005, 0.03000481, 0.31934047, 0.08668316, 0.07532562, 0.19995402, 0.01675361], dtype=float32)"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(init_params, np.random.rand(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "91803aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.388558, dtype=float32)"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(init_params, model.forward, np.random.randn(2, 28, 28), [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "bc526387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 2.3042030334472656\n",
      "step 100, loss: 1.7702372074127197\n",
      "step 200, loss: 1.8559722900390625\n",
      "step 300, loss: 1.8455866575241089\n",
      "step 400, loss: 1.6030317544937134\n",
      "step 500, loss: 1.7808022499084473\n",
      "step 600, loss: 1.7045600414276123\n",
      "step 700, loss: 1.6650400161743164\n",
      "step 800, loss: 1.6782312393188477\n",
      "step 900, loss: 1.759960651397705\n",
      "step 1000, loss: 1.7251852750778198\n",
      "step 1100, loss: 1.7144274711608887\n",
      "step 1200, loss: 1.5780982971191406\n",
      "step 1300, loss: 1.5931434631347656\n",
      "step 1400, loss: 1.7018799781799316\n",
      "step 1500, loss: 1.7146620750427246\n",
      "step 1600, loss: 1.6924097537994385\n",
      "step 1700, loss: 1.626793622970581\n",
      "step 1800, loss: 1.6654064655303955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss=cross_entropy_loss, optimizer = optax.adam(learning_rate=1e-3), metrics = ['accuracy'], params=init_params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-knock",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training loop tunes the model parameters for a specified number of epochs.  For now the parameters are adjusted once per epoch, after the entire dataset is consumed by the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-finder",
   "metadata": {},
   "source": [
    "## Performance Curve\n",
    "\n",
    "Let's see the trend in the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-hudson",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66e6e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(10, 3)\n",
    "w = np.random.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5f2b063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95615652, -0.60910943,  0.47719404,  0.50628421, -0.57886369],\n",
       "       [ 3.09498684,  1.70024379, -1.01957485,  2.25813896, -0.37952626],\n",
       "       [-3.67871561, -0.1351104 ,  0.04294664, -1.84479421,  0.98852387],\n",
       "       [-1.00596024,  0.46315551,  0.67104569,  3.13116358, -1.09329311],\n",
       "       [ 2.07878921,  1.88358723, -1.42939824,  0.84352964,  0.39912581],\n",
       "       [ 2.91327168, -1.07647233,  0.73089571,  1.0445742 , -1.1687821 ],\n",
       "       [ 1.03321149,  0.51671102,  0.70734573,  4.30933842, -1.72017219],\n",
       "       [-2.73799462, -1.54710401,  1.83484434,  1.17423297, -1.08464385],\n",
       "       [-1.6715637 , -2.74092988,  1.91974295, -1.18807747, -0.68628707],\n",
       "       [ 1.21170605,  0.15311168, -0.1815407 ,  0.30359954, -0.13915325]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, np.transpose(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9b25ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9561565  -0.6091094   0.47719404  0.50628424 -0.5788637 ]\n",
      " [ 3.0949867   1.7002438  -1.0195749   2.258139   -0.37952614]\n",
      " [-3.6787155  -0.13511032  0.04294658 -1.8447943   0.9885239 ]\n",
      " [-1.0059603   0.46315545  0.67104566  3.1311636  -1.093293  ]\n",
      " [ 2.0787892   1.8835871  -1.4293982   0.8435297   0.39912578]\n",
      " [ 2.9132717  -1.0764723   0.7308957   1.044574   -1.1687821 ]\n",
      " [ 1.0332114   0.516711    0.7073457   4.309338   -1.7201722 ]\n",
      " [-2.7379947  -1.547104    1.8348444   1.174233   -1.0846438 ]\n",
      " [-1.6715636  -2.7409298   1.919743   -1.1880776  -0.686287  ]\n",
      " [ 1.2117062   0.15311167 -0.1815407   0.3035995  -0.13915324]]\n"
     ]
    }
   ],
   "source": [
    "def lin(x): return jnp.dot(w, x)\n",
    "\n",
    "yy = jax.vmap(lin)(X)\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb8757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
