{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capable-visit",
   "metadata": {},
   "source": [
    "# Fashion MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-cornwall",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post, I'm going to implement a basic Fashion-MNIST classifier using JAX.  JAX is an array-processing library that uses Google's XLA (Accelerated Linear Algebra) compiler to generate high-performance code that can run on a variety of hardware platforms.  It feels a lot like numpy, with a number of advantages including built in automatic differentiation, vectorization and parallelization, and just-in-time compilation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-forth",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "\n",
    "To be a little more specific, here's what we're going to work toward:\n",
    "\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: fashion_mnist_loss(model(X), y)))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=None,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1874/1875  [===============================] - 3s  1.49ms/batch  -  loss: 0.4958   -  accuracy: 0.8245      \n",
    "Epoch 2/5\n",
    "1874/1875  [===============================] - 2s  1.40ms/batch  -  loss: 0.3725   -  accuracy: 0.8654    \n",
    "Epoch 3/5\n",
    "1874/1875  [===============================] - 2s  1.53ms/batch  -  loss: 0.3339   -  accuracy: 0.8778    \n",
    "Epoch 4/5\n",
    "1874/1875  [===============================] - 2s  1.55ms/batch  -  loss: 0.3080   -  accuracy: 0.8869    \n",
    "Epoch 5/5\n",
    "1874/1875  [===============================] - 2s  1.46ms/batch  -  loss: 0.2900   -  accuracy: 0.8934\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-mediterranean",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "compound-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable\n",
    "\n",
    "eps = jnp.finfo(jnp.float64).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-conviction",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "I really like Pytorch's approach to handling data, and could have just imported it's builtin `Dataset` and `Dataloader` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sonic-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return jnp.shape(self.X)[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handmade-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, dataset: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entire-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), _ = tf.keras.datasets.fashion_mnist.load_data()\n",
    "dataset = Dataset(X_train, y_train)\n",
    "dataloader = Dataloader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-slope",
   "metadata": {},
   "source": [
    "## The Sequential Model\n",
    "\n",
    "A sequential model is a list of `Callable` objects that is evaluated by calling each members `__call__` method in order.  Each list element is a registered pytree, but I also wanted to have the flexibility to pass ordinary functions to the `Sequential` constructor and have everything just work.  As you'll soon see this feature was implemented by modifying the `__init__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-intranet",
   "metadata": {},
   "source": [
    "### `Linear` Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-portuguese",
   "metadata": {},
   "source": [
    "The `Linear` layer defined below is very similar to implementations (but less general) you'd find in other non-JAX neural network libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supreme-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Linear:\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    ni: int \n",
    "    no: int \n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, build=True, seed=1234):\n",
    "        self.ni = num_inputs \n",
    "        self.no = num_outputs \n",
    "        # want to add seed as internal object\n",
    "        if build:\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs)\n",
    "            self.b = jnp.zeros(num_outputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(num_inputs={self.ni}, num_outputs={self.no})'\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "    def params(self):\n",
    "        return {'w': self.w, 'b': self.b}\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return (self.w, self.b), (self.ni, self.no)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layer = cls(*aux_data, build=False)\n",
    "        layer.w, layer.b = children\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-northwest",
   "metadata": {},
   "source": [
    "The one glaring difference is the decorator and the two *tree* methods.  As the JAX documentation explains, the `register_pytree_node_class`, `tree_flatten` method, and `tree_unflatten` class methods are required to add a user-defined class to the JAX pytree registry.  Once added, JAX will know how to transform back and forth between objects that the rest of JAX system can efficiently operate on, and objects that are specific to your application.    \n",
    "\n",
    "The `tree_flatten` method returns a two-element tuple consisting of the parameters you want to expose to JAX, and any meta-data that can help reconstruct the object.  Because JAX seems to really embrace the functional paradigm of immutable data structures, I thought it might be better to express the parameters as a tuple.  For `Linear`, the parameters are the weights and biases of the neural network.  For now, the only meta-data that seems helpful are the number of inputs and outputs (although this could be derived from the shape of the weights.  \n",
    "\n",
    "Another thing to notice about `Linear` is the `build` attribute.  Most of the time, you want to initialize the weights and biases at creation time.  However, you don't want to do this when JAX reconstructs the object from it's flattened representation.  You probably just want to plop the parameters right into a freshly constructed object. The `build` attribute gives you some flexibility in that regard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-vancouver",
   "metadata": {},
   "source": [
    "### `Function` Layer\n",
    "\n",
    "The `Function` class fills the same need that `Lambda` layers do in Keras: being able to conveniently plug functions into models.  As the `tree_flatten` method shows, classes registered as pytrees can be parameter-free. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "signal-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Function:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Function({self.fn.__name__})'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [], self.fn\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(aux_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-candle",
   "metadata": {},
   "source": [
    "### Helpful Functions\n",
    "\n",
    "Here are a few functions that will be *lifted* to `Function` layer in the `Sequential` model.  One common approach for improving classification accuracy is to normalize your input data.  When working with gray-scale images, this typically means rescaling the pixels from $[0,255]$ to $[0,1]$.  This is what `rescale_image` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "annual-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_image(x): return x / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-machinery",
   "metadata": {},
   "source": [
    "The model built in this post operates on batches of two-dimensional gray-scale images.  Each batch is a three-dimensional array and can be interpreted as a vertical stack of 2D images, where the height of the vertical stack is the number of images.  The `flatten` goes through each slice of the vertical stack and transforms the 2D array into a one-dimensional array.  In the process, the 3D input becomes a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forced-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    shape = jnp.shape(x)\n",
    "    assert len(shape) == 3, 'x must represent a batch of two-dimensional gray-scale images' \n",
    "    batch_size = shape[0]\n",
    "    return jnp.reshape(x, (batch_size, -1)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-socket",
   "metadata": {},
   "source": [
    "The last two functions we'll implement in this section are `relu` and `softmax`.  A couple of tests are also provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accurate-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.clip(x, a_min=0)    \n",
    "\n",
    "x = np.random.randn(10,10)\n",
    "assert np.all(np.isclose(relu(x), tf.nn.relu(x))), 'test failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "serious-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = jnp.exp(x)\n",
    "    return ex / jnp.sum(ex, axis=-1, keepdims=True)    \n",
    "\n",
    "x = np.random.randn(5, 10)\n",
    "assert np.all(np.isclose(softmax(x), tf.nn.softmax(x))), 'test failed'\n",
    "assert np.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-embassy",
   "metadata": {},
   "source": [
    "### Sequential Model\n",
    "\n",
    "With the supporting pieces implemented, we can define `Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "meaning-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'tree_flatten'):\n",
    "                self.layers.append(layer)\n",
    "            elif callable(layer):\n",
    "                self.layers.append(Function(layer))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for layer in self.layers:\n",
    "            string += (repr(layer) + '\\n')\n",
    "        return string\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        aux_data, children = [], []\n",
    "        for layer in self.layers:\n",
    "            params, extra_stuff = jax.tree_flatten(layer)\n",
    "            aux_data.append(extra_stuff)\n",
    "            children.append(params)\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layers = []\n",
    "        for params, spec in zip(children, aux_data):\n",
    "            layers.append(jax.tree_unflatten(spec, params))\n",
    "        return Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-fight",
   "metadata": {},
   "source": [
    "Based on the previous high-level description, it should not be surprising that `Sequential` is essentially a wrapper around a list of layers.  The `__init__` method loops through the list of input objects and does one of the following things:\n",
    "\n",
    "1. if it is a pytree, adds the layer to the list of layers\n",
    "2. if it is a function, but not a pytree, wraps the function in a `Function` object and adds the pytree to the list of layers.\n",
    "\n",
    "Either way, at the end of construction, each `Sequential` instance is a list of registered pytrees.  At this point, `Sequential` is added to the pytree registry by implementing `tree_flatten` and `tree_unflatten` by looping over the layers, calling each layer's flatten of unflatten method, and collecting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-journalist",
   "metadata": {},
   "source": [
    "### Some Testing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "every-valley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(rescale_image)\n",
      "Function(flatten)\n",
      "Linear(num_inputs=784, num_outputs=128)\n",
      "Function(relu)\n",
      "Linear(num_inputs=128, num_outputs=10)\n",
      "Function(softmax)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fashion_mnist_mlp():\n",
    "    return Sequential(\n",
    "        rescale_image,\n",
    "        flatten,\n",
    "        Linear(784, 128),\n",
    "        relu,\n",
    "        Linear(128, 10),\n",
    "        softmax  \n",
    "    )\n",
    "\n",
    "model = fashion_mnist_mlp()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "conscious-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.zeros((5,5))\n",
    "x[jnp.arange(5),[0,1,2,1,5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-eight",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "Cross Entropy is one of the most common loss functions for classification problems.  I'm going to spare you the long-winded mathematical justification of why it's a useful function.  Let me just say that it measures how close two probability distributions are. \n",
    "\n",
    "Here are two different versions of cross-entropy.  The first version (`cross_entropy`) assumes that `y_true` is one-hot encoded while the second version (`sparse_cross_entropy`) assumes that `y_true` is an array of indices.  I like the sparse version because it seems more efficient and less dependent on knowing the number of categories in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "center-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def cross_entropy(y_true, probs):\n",
    "    batch_size = jnp.shape(probs)[0]\n",
    "    return -jnp.sum(jnp.log(probs + eps) * y_true) / batch_size\n",
    "\n",
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "assert np.all(np.isclose(cross_entropy(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecological-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sparse_cross_entropy(y_true, probs):\n",
    "    batch_size = len(y_true)\n",
    "    X = jnp.log(probs + eps)[jnp.arange(batch_size), y_true]\n",
    "    return -jnp.sum(X) / batch_size\n",
    "\n",
    "y_true = jnp.array([1, 2])\n",
    "y_pred = jnp.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "assert np.all(np.isclose(sparse_cross_entropy(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-sphere",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Optimizers update model parameters at each minibatch.  The simplest optimizer, stochastic gradient descent (aka SGD), is shown below and updates the parameters ($W$) as follows: \n",
    "\n",
    "$$\n",
    "    W \\leftarrow W - \\alpha \\frac{\\partial \\ell}{\\partial W}\n",
    "$$\n",
    "\n",
    "where $\\ell$ represents the loss-function used in model training and $\\alpha$ is the learning rate.  \n",
    "\n",
    "While certainly simple and fast, SGD is not the *go-to* optmizer these days.  That title seems to go to the Adam optimizer, as it tends to be the optimizer used in most examples I've seen.  From their original paper, Kingma and Ba describe Adam as \n",
    "\n",
    "> \"an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\"\n",
    "\n",
    "and from the same paper:\n",
    "\n",
    "> \"The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of \n",
    "the gradients; the name Adam is derived from adaptive moment estimation.\"\n",
    "\n",
    "For now, an optimizer has an `__init__` and `step` method.  The `__init__` method initializes the parameters used to calculate the step direction or step size, while the `step` method updates the model and any optimizer parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "polar-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accurate-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.lr = lr \n",
    "    def step(self, model, grads):\n",
    "        return jax.tree_map(lambda p, g: p - self.lr*g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "million-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7):\n",
    "        self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps\n",
    "        self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) \n",
    "        self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model)\n",
    "        self.k = 0 \n",
    "    def step(self, model, grads):\n",
    "        lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps\n",
    "        v, s = self.v, self.s\n",
    "        k = self.k = self.k+1\n",
    "        self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grads)\n",
    "        self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grads)\n",
    "        v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v)\n",
    "        s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s)\n",
    "        new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat)\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-antique",
   "metadata": {},
   "source": [
    "The `step` implementations illustrate why registering models as pytrees is so important.  In functional programming, you often *map* a function operating on a single item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "flush-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m             \u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"Maps a multi-input function over pytree args to produce a new pytree.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Args:\u001b[0m\n",
       "\u001b[0;34m    f: function that takes ``1 + len(rest)`` arguments, to be applied at the\u001b[0m\n",
       "\u001b[0;34m      corresponding leaves of the pytrees.\u001b[0m\n",
       "\u001b[0;34m    tree: a pytree to be mapped over, with each leaf providing the first\u001b[0m\n",
       "\u001b[0;34m      positional argument to ``f``.\u001b[0m\n",
       "\u001b[0;34m    rest: a tuple of pytrees, each of which has the same structure as ``tree``\u001b[0m\n",
       "\u001b[0;34m      or has ``tree`` as a prefix.\u001b[0m\n",
       "\u001b[0;34m    is_leaf: an optionally specified function that will be called at each\u001b[0m\n",
       "\u001b[0;34m      flattening step. It should return a boolean, which indicates whether\u001b[0m\n",
       "\u001b[0;34m      the flattening should traverse the current object, or if it should be\u001b[0m\n",
       "\u001b[0;34m      stopped immediately, with the whole subtree being treated as a leaf.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Returns:\u001b[0m\n",
       "\u001b[0;34m    A new pytree with the same structure as ``tree`` but with the value at each\u001b[0m\n",
       "\u001b[0;34m    leaf given by ``f(x, *xs)`` where ``x`` is the value at the corresponding\u001b[0m\n",
       "\u001b[0;34m    leaf in ``tree`` and ``xs`` is the tuple of values at corresponding nodes in\u001b[0m\n",
       "\u001b[0;34m    ``rest``.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Examples:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    >>> import jax.tree_util\u001b[0m\n",
       "\u001b[0;34m    >>> jax.tree_util.tree_map(lambda x: x + 1, {\"x\": 7, \"y\": 42})\u001b[0m\n",
       "\u001b[0;34m    {'x': 8, 'y': 43}\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    If multiple inputs are passed, the structure of the tree is taken from the\u001b[0m\n",
       "\u001b[0;34m    first input; subsequent inputs need only have ``tree`` as a prefix:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    >>> jax.tree_util.tree_map(lambda x, y: [x] + y, [5, 6], [[7, 9], [1, 2]])\u001b[0m\n",
       "\u001b[0;34m    [[5, 7, 9], [6, 1, 2]]\u001b[0m\n",
       "\u001b[0;34m  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.8/site-packages/jax/_src/tree_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.tree_map??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-plaza",
   "metadata": {},
   "source": [
    "## Progress Bar\n",
    "\n",
    "I really like how Keras logs information to the screen during model training, and decided to mimic the style.  Here's my version of the progress bar.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "seven-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(percentage, total=30):\n",
    "    x = int(percentage*total)\n",
    "    if x < total:\n",
    "        r = '[' + ''.join(['=']*x) + '>' + ''.join(['.']*(total-x)) + ']' \n",
    "    else:\n",
    "        r = '[' + ''.join(['=']*(total+1)) + ']' \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-bacon",
   "metadata": {},
   "source": [
    "It has space for 31 characters sandwiched between an opening and closing bracket.  Examples at various completion percentages are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-juvenile",
   "metadata": {},
   "source": [
    "1. 0% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "young-tiffany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>..............................]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-wheat",
   "metadata": {},
   "source": [
    "2. 10% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "subject-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===>...........................]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-consensus",
   "metadata": {},
   "source": [
    "3. 100% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "graduate-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-tourist",
   "metadata": {},
   "source": [
    "## The Training Loop  \n",
    "\n",
    "The training loop is pretty basic, but looks cluttered because it's arranged as a single function.  Most frameworks split the training loop into several pieces, and incorporates a callback system that allows a user to customize it's functionality.  Otherwise, you'd likely have to resort to writing a new training loop for each problem.  Besides that, a callback system would lead to less cluttered (and therefore less buggy) code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "emerging-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_datasource, valid_datasource, optimizer, loss_fn, grad_fn, model):\n",
    "    history = {'loss':[], 'accuracy':[]}\n",
    "    \n",
    "    if valid_datasource is not None:\n",
    "        history = {**history, 'valid_loss': [], 'valid_accuracy': []}\n",
    "\n",
    "    train_num_batches = len(train_datasource)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # TRAINING PHASE\n",
    "        train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0\n",
    "        \n",
    "        num_steps = 0 \n",
    "        \n",
    "        epoch_duration = 0.0\n",
    "        \n",
    "        # we know how many batches there are ... keep track\n",
    "        for i, (X_train, y_train) in enumerate(train_datasource):\n",
    "            \n",
    "            # logging\n",
    "            batch_start = time.time()\n",
    "\n",
    "            num_steps += 1\n",
    "            \n",
    "            # training loss and gradients for this particular batch\n",
    "            probs = model(X_train)\n",
    "            loss = loss_fn(y_train, probs)\n",
    "            grad = grad_fn(model, X_train, y_train)\n",
    "            model = optimizer.step(model, grad)\n",
    "            \n",
    "            \n",
    "            # Results aggregation\n",
    "            num_correct = jnp.sum(jnp.argmax(probs, axis=-1) == y_train)\n",
    "            train_loss_accum += loss \n",
    "            train_batch_size += len(y_train)\n",
    "            train_accuracy_accum += num_correct\n",
    "            train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "\n",
    "            # Logging ....\n",
    "            batch_duration = time.time() - batch_start\n",
    "            epoch_duration += batch_duration \n",
    "            log_batch_count = f'{i}/{train_num_batches}'\n",
    "            log_epoch_time = f'{int(epoch_duration)}s'\n",
    "            log_batch_time = f'{1_000*batch_duration:.2f}ms/batch'\n",
    "            log_batch_loss = f'loss: {train_loss:.4f}'\n",
    "            log_batch_accuracy = f'accuracy: {train_accuracy:.4f}'\n",
    "            log_string =  f'{log_batch_count:<10s} {progress_bar((i+1)/train_num_batches)} - {log_epoch_time:<3s} {log_batch_time:<5s} - {log_batch_loss:<13s} - {log_batch_accuracy:<20s}'\n",
    "            print(log_string, end='\\r') \n",
    "\n",
    "        # \n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_accuracy)      \n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        if valid_datasource is not None:\n",
    "            valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 \n",
    "\n",
    "            # Run validation step ...\n",
    "            for i, (X_valid, y_valid) in enumerate(valid_datasource):\n",
    "                num_steps += 1\n",
    "                probs = model(X_valid)\n",
    "                loss = loss_fn(y_valid, probs)\n",
    "                \n",
    "                valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid)\n",
    "\n",
    "                valid_loss_accum += loss\n",
    "                valid_batch_size += len(y_valid)\n",
    "\n",
    "            epoch_valid_loss = valid_loss_accum / valid_batch_size \n",
    "            epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size\n",
    "\n",
    "            history['loss'].append(epoch_valid_loss)\n",
    "            history['accuracy'].append(epoch_valid_accuracy)\n",
    "        \n",
    "        # this log_string should include validation results\n",
    "        print(log_string, end='\\n')\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-queensland",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "blank-today",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1874/1875  [===============================] - 3s  1.60ms/batch - loss: 0.4956  - accuracy: 0.8250      \n",
      "Epoch 2/5\n",
      "1874/1875  [===============================] - 2s  1.35ms/batch - loss: 0.3716  - accuracy: 0.8654    \n",
      "Epoch 3/5\n",
      "1874/1875  [===============================] - 3s  2.13ms/batch - loss: 0.3330  - accuracy: 0.8791    \n",
      "Epoch 4/5\n",
      "1874/1875  [===============================] - 2s  1.34ms/batch - loss: 0.3079  - accuracy: 0.8871    \n",
      "Epoch 5/5\n",
      "1874/1875  [===============================] - 2s  1.44ms/batch - loss: 0.2891  - accuracy: 0.8935    \n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=32)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X))))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=valid_datasource,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=sparse_cross_entropy, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-modem",
   "metadata": {},
   "source": [
    "## Keras Execution\n",
    "\n",
    "For comparison, here's the training results for Keras on the same dataset.  While their are a few differences in the output, I'm pretty pleased with how well they match. I'm guessing that the reason there isn't an even better match comes from the fact that parameter initialization is slightly different.  I'm currently limited to Kaiming initialization, whereas Dense layers in Keras use Kaiming uniform by default.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "veterinary-campaign",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 793us/step - loss: 0.4952 - accuracy: 0.8253\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 803us/step - loss: 0.3735 - accuracy: 0.8652\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 807us/step - loss: 0.3321 - accuracy: 0.8799\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 920us/step - loss: 0.3088 - accuracy: 0.8855\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 943us/step - loss: 0.2916 - accuracy: 0.8928\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1/255.0),\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-viewer",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. Implement convolutional layers and train a convolutional neural network\n",
    "2. Add a callback system to simplify the training loop\n",
    "3. Show validation results\n",
    "4. Evaluate performance on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dangerous-quebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001, 0.0009)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3, 900e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-mortgage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
