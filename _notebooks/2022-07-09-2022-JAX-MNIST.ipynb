{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southern-pocket",
   "metadata": {},
   "source": [
    "# Fashion MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png\n",
    "- hide: true\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-memory",
   "metadata": {},
   "source": [
    "In this article, I'm going to start implementing a JAX-based neural network library, and use it to synthesize a Fashion-MNIST classifier. Features will be added over the coming weeks and months as I tackle more advanced topics and architectures. I don't intend to write a full-featured library/framework like Tensorflow or Pytorch. Instead, my goal is compile a set of loosely coupled components that I can mix-and-match to solve problems that interest me. When there's some educational benefit, funtionality will be written from scratch and validated against more mature libraries (emphasis on the Tensorflow/Keras ecosystem).  Yes, this will be a slow process, but I'm certain to learn a lot if I stick to this plan.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-mercy",
   "metadata": {},
   "source": [
    "# Why JAX\n",
    "\n",
    "JAX is a python-based numerical-computing library that uses Google's XLA (Accelerated Linear Algebra) compiler to generate high performance code that can run on a variety of hardware platforms.  It feels a lot like numpy, with a few quirks.  One of it's major selling points, particularly for machine learning and scientific computing, is it's ability to calculate derivatives.  It also heavily embraces the functional-programming-paradigm, which takes some getting used to if you're accustomed to an imperative style where arrays can be mutated at will.\n",
    "\n",
    "There are several JAX-based neural network libraries in the open-source space but none are as mature and full-featured as Pytorch and Tensorflow.  And you wouldn't expect them to be; JAX hasn't been around for nearly as long.  And this gets to the reason I wanted to focus on JAX; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-match",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28,28)),\n",
    "  keras.layers.Dense(128, activation=keras.activations.relu),\n",
    "  keras.layers.Dense(10, activation=keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4955 - accuracy: 0.8250\n",
    "Epoch 2/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3726 - accuracy: 0.8643\n",
    "Epoch 3/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3354 - accuracy: 0.8781\n",
    "Epoch 4/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3136 - accuracy: 0.8845\n",
    "Epoch 5/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2937 - accuracy: 0.8927\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-highlight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "extreme-bracket",
   "metadata": {},
   "source": [
    "## The Fashion MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-privacy",
   "metadata": {},
   "source": [
    "## Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "religious-score",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 21:08:16.001764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/bin:/usr/local/lib:\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-dimension",
   "metadata": {},
   "source": [
    "## Datasource API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inner-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return jnp.shape(self.X)[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "divine-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, dataset: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasource:\n",
    "    def __init__(self, path: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "civic-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "experimental-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "animal-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Dataloader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electrical-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28) (32,)\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataloader:\n",
    "    print(X.shape, y.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-advocate",
   "metadata": {},
   "source": [
    "## `Parametric` Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "indie-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parametric: pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-helping",
   "metadata": {},
   "source": [
    "### Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "elder-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Linear(Parametric):\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    ni: int \n",
    "    no: int \n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, build=True, seed=1234):\n",
    "        self.ni = num_inputs \n",
    "        self.no = num_outputs \n",
    "        # want to add seed as internal object\n",
    "        if build:\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs)\n",
    "            self.b = jnp.zeros(num_outputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(num_inputs={self.ni}, num_outputs={self.no})'\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "    def tree_flatten(self):\n",
    "        return (self.w, self.b), (self.ni, self.no)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layer = cls(*aux_data, build=False)\n",
    "        layer.w, layer.b = children\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-death",
   "metadata": {},
   "source": [
    "### `Function` Layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "prescription-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Function:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Function({self.fn.__name__})'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [None], self.fn\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(aux_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-contest",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "intimate-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    shape = jnp.shape(x)\n",
    "    if len(shape) == 2:\n",
    "        # flatten a single 2D image\n",
    "        return jnp.reshape(x, -1)\n",
    "    elif len(shape) == 3:\n",
    "        # x is a batch of 2D images, flatten each image \n",
    "        batch_size = jnp.shape(x)[0]\n",
    "        return jnp.reshape(x, (batch_size, -1)) \n",
    "    else:\n",
    "        raise Exception(\n",
    "            f'At the moment you can only pass 2D or 3D arrays to flatten, you passed a {len(shape)}D array' \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-hardwood",
   "metadata": {},
   "source": [
    "### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "warming-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.clip(x, a_min=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "chinese-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10,10)\n",
    "assert np.all(np.isclose(relu(x), tf.nn.relu(x))), 'test failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-wesley",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "pending-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = jnp.exp(x)\n",
    "    return ex / jnp.sum(ex, axis=-1, keepdims=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "environmental-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(5, 10)\n",
    "assert np.all(np.isclose(softmax(x), tf.nn.softmax(x))), 'test failed'\n",
    "assert np.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-cooper",
   "metadata": {},
   "source": [
    "### Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "adequate-uzbekistan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable(Linear(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "matched-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_image(x): return x / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-amino",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "A `Sequential` model is a list of function-like each objects.  Looking at the `__init__` method below, each item in the list of layers must be a subclass of `Parametric` or `Function`, or be `callable`.  Note that the order is important here because any object that implements a `__call__` method is by definition `callable`.  The other methods defined in the `Sequential` class definition are pretty straight-forward and consist of simply looping over the list of layers and calling that particular method (and possibly appending results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "basic-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Sequential(Parametric):\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, Parametric) or isinstance(layer, Function):\n",
    "                self.layers.append(layer)\n",
    "            elif callable(layer):\n",
    "                self.layers.append(Function(layer))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for layer in self.layers:\n",
    "            string += (repr(layer) + '\\n')\n",
    "        return string\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        aux_data, children = [], []\n",
    "        for layer in self.layers:\n",
    "            params, extra_stuff = jax.tree_flatten(layer)\n",
    "            aux_data.append(extra_stuff)\n",
    "            children.append(params)\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layers = []\n",
    "        for params, spec in zip(children, aux_data):\n",
    "            layers.append(jax.tree_unflatten(spec, params))\n",
    "        return Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "expired-saudi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(rescale_image)\n",
      "Function(flatten)\n",
      "Linear(num_inputs=784, num_outputs=128)\n",
      "Function(relu)\n",
      "Linear(num_inputs=128, num_outputs=10)\n",
      "Function(softmax)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fashion_mnist_mlp():\n",
    "    return Sequential(\n",
    "        rescale_image,\n",
    "        flatten,\n",
    "        Linear(784, 128),\n",
    "        relu,\n",
    "        Linear(128, 10),\n",
    "        softmax  \n",
    "    )\n",
    "\n",
    "model = fashion_mnist_mlp()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-literature",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "binary-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, probs):\n",
    "    batch_size, _ = jnp.shape(probs)\n",
    "    return -jnp.sum(jnp.log(probs + 1.0e-16) * y_true) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "green-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "assert np.all(np.isclose(cross_entropy_loss(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-heading",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Do we really need to pass the model to the optimizer each time?  Or should we get the step and update the model outside of the optimizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "impressed-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "upper-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.lr = lr \n",
    "    def step(self, model, grads):\n",
    "        return jax.tree_map(lambda p, g: p - self.lr*g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "daily-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7):\n",
    "        self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps\n",
    "        self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) \n",
    "        self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model)\n",
    "        self.k = 0 \n",
    "    def step(self, model, grads):\n",
    "        lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps\n",
    "        v, s = self.v, self.s\n",
    "        k = self.k = self.k+1\n",
    "        self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grads)\n",
    "        self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grads)\n",
    "        v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v)\n",
    "        s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s)\n",
    "        new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat)\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-secondary",
   "metadata": {},
   "source": [
    "## Training Loop  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "accessory-corporation",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (3849411444.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_159067/3849411444.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def train(num_epochs, train_datasource, valid_datasource=None, optimizer, loss_fn, grad_fn, model):\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def train(num_epochs, train_datasource, valid_datasource=None, optimizer, loss_fn, grad_fn, model):\n",
    "    history = {'loss':[], 'accuracy':[]}\n",
    "    \n",
    "    if valid_datasource is not None:\n",
    "        history = {**history, 'valid_loss': [], 'valid_accuracy': []}\n",
    "\n",
    "    train_num_batches = len(train_datasource)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # TRAINING PHASE\n",
    "        train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0\n",
    "        \n",
    "        num_steps = 0 \n",
    "        \n",
    "        epoch_duration = 0.0\n",
    "        \n",
    "        # we know how many batches there are ... keep track\n",
    "        #train_loss_per_batch = []\n",
    "        #train_accuracy_per_batch = []\n",
    "        for i, (X_train, y_train) in enumerate(train_datasource):\n",
    "            \n",
    "            # logging\n",
    "            batch_start = time.time()\n",
    "\n",
    "            num_steps += 1\n",
    "            \n",
    "            # training loss and gradients for this particular batch\n",
    "            probs = model(X_train)\n",
    "            loss = loss_fn(probs, y_train)\n",
    "            \n",
    "            grads = grad_fn(model, X_train, y_train)\n",
    "            model = optimizer.step(model, grads)\n",
    "            \n",
    "            num_correct = jnp.sum(jnp.argmax(probs, axis=-1) == y_train)\n",
    "            \n",
    "            # update for metrics\n",
    "            #train_loss_per_batch.append(loss)\n",
    "            #train_accuracy_per_batch.append(accuracy)\n",
    "\n",
    "            #train_loss_accum = jnp.sum(jnp.array(train_loss_per_batch)) \n",
    "            #train_batch_size += len(y_train)\n",
    "            #train_accuracy_accum = jnp.sum(jnp.array(train_accuracy_per_batch))\n",
    "            #train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            #train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "            \n",
    "            train_loss_accum += loss \n",
    "            train_batch_size += len(y_train)\n",
    "            train_accuracy_accum += num_correct\n",
    "            train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "\n",
    "            # Logging ....\n",
    "            batch_duration = time.time() - batch_start\n",
    "            epoch_duration += batch_duration \n",
    "            log_batch_count = f'{i}/{train_num_batches}'\n",
    "            log_epoch_time = f'{epoch_duration:>5.2f}s'\n",
    "            log_batch_time = f'{1_000*batch_duration:>5.2f}ms/batch'\n",
    "            log_batch_loss = f'train_loss:  {train_loss:>5.4f}'\n",
    "            log_batch_accuracy = f'train_accuracy:  {train_accuracy:<5.4f}'\n",
    "            log_string =  f'{log_batch_count:<10s}  {progress_bar(i/train_num_batches)}  {log_epoch_time} {log_batch_time}  ,  {log_batch_loss}  ,  {log_batch_accuracy} '\n",
    "            print(log_string, end='\\r') \n",
    "\n",
    "        # \n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_accuracy)      \n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        if valid_datasource is not None:\n",
    "            valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 \n",
    "\n",
    "            # Run validation step ...\n",
    "            for i, (X_valid, y_valid) in enumerate(valid_datasource):\n",
    "                num_steps += 1\n",
    "                probs = model(X_valid)\n",
    "                loss = loss_fn(probs, y_valid)\n",
    "                \n",
    "                valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid)\n",
    "\n",
    "                valid_loss_accum += loss\n",
    "                valid_batch_size += len(y_valid)\n",
    "\n",
    "            epoch_valid_loss = valid_loss_accum / valid_batch_size \n",
    "            epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size\n",
    "\n",
    "            history['loss'].append(epoch_valid_loss)\n",
    "            history['accuracy'].append(epoch_valid_accuracy)\n",
    "        \n",
    "        # this log_string should include validation results\n",
    "        print(log_string, end='\\n')\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "hybrid-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnist_loss(probs, y_true, num_classes=10):\n",
    "    # average cross entropy, batch\n",
    "    y_one_hot = jax.nn.one_hot(y_true, num_classes)\n",
    "    return -jnp.sum(jnp.log(probs) * y_one_hot) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "searching-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "937/937     [===============================]   7.22s  7.25ms/batch  ,  train_loss:  0.5235  ,  train_accuracy:  0.8172 \n",
      "Epoch 2/5\n",
      "937/937     [===============================]   7.40s  7.11ms/batch  ,  train_loss:  0.3882  ,  train_accuracy:  0.8618 \n",
      "Epoch 3/5\n",
      "937/937     [===============================]   7.42s  6.99ms/batch  ,  train_loss:  0.3469  ,  train_accuracy:  0.8743 \n",
      "Epoch 4/5\n",
      "937/937     [===============================]   7.33s 10.95ms/batch  ,  train_loss:  0.3202  ,  train_accuracy:  0.8837 \n",
      "Epoch 5/5\n",
      "937/937     [===============================]   7.35s  7.69ms/batch  ,  train_loss:  0.2999  ,  train_accuracy:  0.8908 \n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "# train_datasource = Datasource(MNIST, batchsize=64)\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "\n",
    "train_datasource = Dataloader(train_dataset, batchsize=64)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "model = fashion_mnist_mlp()\n",
    "\n",
    "\n",
    "grad_fn = jax.grad(lambda model, X, y: fashion_mnist_loss(model(X), y), )\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=None,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=model,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-knowing",
   "metadata": {},
   "source": [
    "## Performance Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-wrong",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "adjustable-latter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.34\n"
     ]
    }
   ],
   "source": [
    "print('98.56', end='')\n",
    "time.sleep(1)\n",
    "print('\\r64.34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "numerous-violence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello       10.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 10\n",
    "f'Hello {x:>10.1f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "formal-murray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return value.__format__(format_spec)\n",
      "\n",
      "format_spec defaults to the empty string.\n",
      "See the Format Specification Mini-Language section of help('FORMATTING') for\n",
      "details.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "??format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "naked-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"Reconstructs a pytree from the treedef and the leaves.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  The inverse of :func:`tree_flatten`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Args:\u001b[0m\n",
       "\u001b[0;34m    treedef: the treedef to reconstruct\u001b[0m\n",
       "\u001b[0;34m    leaves: the list of leaves to use for reconstruction. The list must match\u001b[0m\n",
       "\u001b[0;34m      the leaves of the treedef.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Returns:\u001b[0m\n",
       "\u001b[0;34m    The reconstructed pytree, containing the ``leaves`` placed in the structure\u001b[0m\n",
       "\u001b[0;34m    described by ``treedef``.\u001b[0m\n",
       "\u001b[0;34m  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.8/site-packages/jax/_src/tree_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?jax.tree_unflatten??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "?jax.tree_unflatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "descending-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x7f7f78ee1f70>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "established-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = model.tree_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "closing-willow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x7f7ef447b760>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sequential.tree_unflatten(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "speaking-community",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(6., dtype=float32)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sum(jnp.array([1.0,2.0,3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "northern-trinity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello               '"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'hello'\n",
    "f'{x:<20s}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "formal-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(percentage, total=30):\n",
    "    x = int(percentage*total)\n",
    "    if x < total:\n",
    "        r = '[' + ''.join(['=']*x) + '>' + ''.join([' ']*(total-x)) + ']' \n",
    "    else:\n",
    "        r = '[' + ''.join(['=']*(total+1)) + ']' \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "sustained-flush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================>] 1.00000000000000014\r"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0,1.1,0.1):\n",
    "    print(f'{progress_bar(i)} {i}', end='\\r')\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "modular-argentina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'====='"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(['=']*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "beneficial-macintosh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0.1*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "peripheral-kenya",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "range(stop) -> range object\n",
       "range(start, stop[, step]) -> range object\n",
       "\n",
       "Return an object that produces a sequence of integers from start (inclusive)\n",
       "to stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\n",
       "start defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\n",
       "These are exactly the valid indices for a list of 4 elements.\n",
       "When step is given, it specifies the increment (or decrement).\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "satisfied-employer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 10:25:17.632440: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 855us/step - loss: 0.5025 - accuracy: 0.8221\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 813us/step - loss: 0.3739 - accuracy: 0.8648\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 824us/step - loss: 0.3380 - accuracy: 0.8779\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 849us/step - loss: 0.3148 - accuracy: 0.8843\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 903us/step - loss: 0.2954 - accuracy: 0.8902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e58a3ba00>"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-reservoir",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
