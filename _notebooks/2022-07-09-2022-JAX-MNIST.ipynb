{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flexible-secretary",
   "metadata": {},
   "source": [
    "# Fashion MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png\n",
    "- hide: true\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-perception",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this article, I'm going to start implementing a JAX-based neural network library, and use it to synthesize a Fashion-MNIST classifier. Features will be added over the coming weeks and months as I tackle more advanced topics and architectures. I don't intend to write a full-featured library/framework like Tensorflow or Pytorch. Instead, my goal is compile a set of loosely coupled components that I can mix-and-match to solve problems that interest me. When there's some educational benefit, funtionality will be written from scratch and validated against more mature libraries (emphasis on the Tensorflow/Keras ecosystem).  Yes, this will be a slow process, but I'm certain to learn a lot about good   \n",
    "\n",
    "## Why JAX\n",
    "\n",
    "JAX is an array-processing library that uses Google's XLA (Accelerated Linear Algebra) compiler to generate highly performance code that can run on a variety of hardware platforms.  It feels a lot like numpy, with a number of advantages including built in automatic differentiation, parallelization, and just-in-time compilation.  It also heavily embraces the functional-programming-paradigm, which can seem a little strange if you're used to working in an imperative style where arrays can be mutated at will.\n",
    "\n",
    "There are several JAX-based neural network libraries in the open-source space but none are as mature and full-featured as Pytorch and Tensorflow ... yet.  And you wouldn't expect them to be; JAX hasn't been around for nearly as long.  And this gets to the reason I wanted to focus on JAX, it seems like there's some room to contribute without having to learn a massive ecosystem.   \n",
    "\n",
    "**SAY SOMETHING ABOUT BEING MORE MATH FOCUSED**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-disaster",
   "metadata": {},
   "source": [
    "Here's what the process looks like in Keras\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1/255),\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4955 - accuracy: 0.8250\n",
    "Epoch 2/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3726 - accuracy: 0.8643\n",
    "Epoch 3/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3354 - accuracy: 0.8781\n",
    "Epoch 4/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3136 - accuracy: 0.8845\n",
    "Epoch 5/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2937 - accuracy: 0.8927\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-drive",
   "metadata": {},
   "source": [
    "and here's what we'll have by the end of this writeup.\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "\n",
    "\n",
    "mlp = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(784, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: cross_entropy_loss(model(X), y)))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=None,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=mlp,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1874/1875   [=============================> ]   3.05s  1.48ms/batch  ,  train_loss:  0.4958  ,  train_accuracy:  0.8245  \n",
    "Epoch 2/5\n",
    "1874/1875   [=============================> ]   2.97s  1.38ms/batch  ,  train_loss:  0.3725  ,  train_accuracy:  0.8654 \n",
    "Epoch 3/5\n",
    "1874/1875   [=============================> ]   2.97s  1.42ms/batch  ,  train_loss:  0.3339  ,  train_accuracy:  0.8778 \n",
    "Epoch 4/5\n",
    "1874/1875   [=============================> ]   2.97s  1.58ms/batch  ,  train_loss:  0.3080  ,  train_accuracy:  0.8869 \n",
    "Epoch 5/5\n",
    "1874/1875   [=============================> ]   2.95s  1.36ms/batch  ,  train_loss:  0.2900  ,  train_accuracy:  0.8934 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-sharp",
   "metadata": {},
   "source": [
    "## The Fashion MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-ontario",
   "metadata": {},
   "source": [
    "## Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minute-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 21:08:16.001764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/bin:/usr/local/lib:\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-railway",
   "metadata": {},
   "source": [
    "## Datasource API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legendary-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return jnp.shape(self.X)[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "graphic-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, dataset: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "meaning-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "present-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "yellow-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Dataloader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-burning",
   "metadata": {},
   "source": [
    "## Sequential Model\n",
    "\n",
    "```python\n",
    "model = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(784, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-accountability",
   "metadata": {},
   "source": [
    "### Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "industrial-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parametric: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "latter-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Linear(Parametric):\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    ni: int \n",
    "    no: int \n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, build=True, seed=1234):\n",
    "        self.ni = num_inputs \n",
    "        self.no = num_outputs \n",
    "        # want to add seed as internal object\n",
    "        if build:\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs)\n",
    "            self.b = jnp.zeros(num_outputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(num_inputs={self.ni}, num_outputs={self.no})'\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "    def tree_flatten(self):\n",
    "        return (self.w, self.b), (self.ni, self.no)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layer = cls(*aux_data, build=False)\n",
    "        layer.w, layer.b = children\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-stationery",
   "metadata": {},
   "source": [
    "### Functional Layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "inner-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Function:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Function({self.fn.__name__})'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [None], self.fn\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(aux_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-sleeve",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "formal-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    shape = jnp.shape(x)\n",
    "    if len(shape) == 2:\n",
    "        # flatten a single 2D image\n",
    "        return jnp.reshape(x, -1)\n",
    "    elif len(shape) == 3:\n",
    "        # x is a batch of 2D images, flatten each image \n",
    "        batch_size = jnp.shape(x)[0]\n",
    "        return jnp.reshape(x, (batch_size, -1)) \n",
    "    else:\n",
    "        raise Exception(\n",
    "            f'At the moment you can only pass 2D or 3D arrays to flatten, you passed a {len(shape)}D array' \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-eligibility",
   "metadata": {},
   "source": [
    "### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "anticipated-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.clip(x, a_min=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "tested-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10,10)\n",
    "assert np.all(np.isclose(relu(x), tf.nn.relu(x))), 'test failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-cooperation",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "central-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = jnp.exp(x)\n",
    "    return ex / jnp.sum(ex, axis=-1, keepdims=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "seven-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(5, 10)\n",
    "assert np.all(np.isclose(softmax(x), tf.nn.softmax(x))), 'test failed'\n",
    "assert np.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-assignment",
   "metadata": {},
   "source": [
    "### Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "prime-cinema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable(Linear(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "final-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_image(x): return x / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-dallas",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "A `Sequential` model is a list of function-like each objects.  Looking at the `__init__` method below, each item in the list of layers must be a subclass of `Parametric` or `Function`, or be `callable`.  Note that the order is important here because any object that implements a `__call__` method is by definition `callable`.  The other methods defined in the `Sequential` class definition are pretty straight-forward and consist of simply looping over the list of layers and calling that particular method (and possibly appending results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "north-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Sequential(Parametric):\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, Parametric) or isinstance(layer, Function):\n",
    "                self.layers.append(layer)\n",
    "            elif callable(layer):\n",
    "                self.layers.append(Function(layer))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for layer in self.layers:\n",
    "            string += (repr(layer) + '\\n')\n",
    "        return string\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        aux_data, children = [], []\n",
    "        for layer in self.layers:\n",
    "            params, extra_stuff = jax.tree_flatten(layer)\n",
    "            aux_data.append(extra_stuff)\n",
    "            children.append(params)\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layers = []\n",
    "        for params, spec in zip(children, aux_data):\n",
    "            layers.append(jax.tree_unflatten(spec, params))\n",
    "        return Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "approved-smart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(rescale_image)\n",
      "Function(flatten)\n",
      "Linear(num_inputs=784, num_outputs=128)\n",
      "Function(relu)\n",
      "Linear(num_inputs=128, num_outputs=10)\n",
      "Function(softmax)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fashion_mnist_mlp():\n",
    "    return Sequential(\n",
    "        rescale_image,\n",
    "        flatten,\n",
    "        Linear(784, 128),\n",
    "        relu,\n",
    "        Linear(128, 10),\n",
    "        softmax  \n",
    "    )\n",
    "\n",
    "model = fashion_mnist_mlp()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-raise",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "senior-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, probs):\n",
    "    batch_size, _ = jnp.shape(probs)\n",
    "    return -jnp.sum(jnp.log(probs + 1.0e-16) * y_true) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "conditional-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "assert np.all(np.isclose(cross_entropy_loss(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-angle",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Do we really need to pass the model to the optimizer each time?  Or should we get the step and update the model outside of the optimizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "chronic-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "convinced-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.lr = lr \n",
    "    def step(self, model, grads):\n",
    "        return jax.tree_map(lambda p, g: p - self.lr*g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cutting-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7):\n",
    "        self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps\n",
    "        self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) \n",
    "        self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model)\n",
    "        self.k = 0 \n",
    "    def step(self, model, grads):\n",
    "        lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps\n",
    "        v, s = self.v, self.s\n",
    "        k = self.k = self.k+1\n",
    "        self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grads)\n",
    "        self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grads)\n",
    "        v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v)\n",
    "        s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s)\n",
    "        new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat)\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-ridge",
   "metadata": {},
   "source": [
    "## Training Loop  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "answering-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_datasource, valid_datasource, optimizer, loss_fn, grad_fn, model):\n",
    "    history = {'loss':[], 'accuracy':[]}\n",
    "    \n",
    "    if valid_datasource is not None:\n",
    "        history = {**history, 'valid_loss': [], 'valid_accuracy': []}\n",
    "\n",
    "    train_num_batches = len(train_datasource)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # TRAINING PHASE\n",
    "        train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0\n",
    "        \n",
    "        num_steps = 0 \n",
    "        \n",
    "        epoch_duration = 0.0\n",
    "        \n",
    "        # we know how many batches there are ... keep track\n",
    "        #train_loss_per_batch = []\n",
    "        #train_accuracy_per_batch = []\n",
    "        for i, (X_train, y_train) in enumerate(train_datasource):\n",
    "            \n",
    "            # logging\n",
    "            batch_start = time.time()\n",
    "\n",
    "            num_steps += 1\n",
    "            \n",
    "            # training loss and gradients for this particular batch\n",
    "            probs = model(X_train)\n",
    "            loss = loss_fn(probs, y_train)\n",
    "            \n",
    "            grads = grad_fn(model, X_train, y_train)\n",
    "            model = optimizer.step(model, grads)\n",
    "            \n",
    "            \n",
    "            # Results aggregation\n",
    "            num_correct = jnp.sum(jnp.argmax(probs, axis=-1) == y_train)\n",
    "            train_loss_accum += loss \n",
    "            train_batch_size += len(y_train)\n",
    "            train_accuracy_accum += num_correct\n",
    "            train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "\n",
    "            # Logging ....\n",
    "            batch_duration = time.time() - batch_start\n",
    "            epoch_duration += batch_duration \n",
    "            log_batch_count = f'{i}/{train_num_batches}'\n",
    "            log_epoch_time = f'{epoch_duration:>5.2f}s'\n",
    "            log_batch_time = f'{1_000*batch_duration:>5.2f}ms/batch'\n",
    "            log_batch_loss = f'train_loss:  {train_loss:>5.4f}'\n",
    "            log_batch_accuracy = f'train_accuracy:  {train_accuracy:<5.4f}'\n",
    "            log_string =  f'{log_batch_count:<10s}  {progress_bar(i/train_num_batches)}  {log_epoch_time} {log_batch_time}  ,  {log_batch_loss}  ,  {log_batch_accuracy} '\n",
    "            print(log_string, end='\\r') \n",
    "\n",
    "        # \n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_accuracy)      \n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        if valid_datasource is not None:\n",
    "            valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 \n",
    "\n",
    "            # Run validation step ...\n",
    "            for i, (X_valid, y_valid) in enumerate(valid_datasource):\n",
    "                num_steps += 1\n",
    "                probs = model(X_valid)\n",
    "                loss = loss_fn(probs, y_valid)\n",
    "                \n",
    "                valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid)\n",
    "\n",
    "                valid_loss_accum += loss\n",
    "                valid_batch_size += len(y_valid)\n",
    "\n",
    "            epoch_valid_loss = valid_loss_accum / valid_batch_size \n",
    "            epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size\n",
    "\n",
    "            history['loss'].append(epoch_valid_loss)\n",
    "            history['accuracy'].append(epoch_valid_accuracy)\n",
    "        \n",
    "        # this log_string should include validation results\n",
    "        print(log_string, end='\\n')\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "recovered-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def fashion_mnist_loss(probs, y_true, num_classes=10):\n",
    "    # average cross entropy, batch\n",
    "    y_one_hot = jax.nn.one_hot(y_true, num_classes)\n",
    "    return -jnp.sum(jnp.log(probs) * y_one_hot) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "correct-reset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1874/1875   [=============================> ]   3.05s  1.48ms/batch  ,  train_loss:  0.4958  ,  train_accuracy:  0.8245  \n",
      "Epoch 2/5\n",
      "1874/1875   [=============================> ]   2.97s  1.38ms/batch  ,  train_loss:  0.3725  ,  train_accuracy:  0.8654 \n",
      "Epoch 3/5\n",
      "1874/1875   [=============================> ]   2.97s  1.42ms/batch  ,  train_loss:  0.3339  ,  train_accuracy:  0.8778 \n",
      "Epoch 4/5\n",
      "1874/1875   [=============================> ]   2.97s  1.58ms/batch  ,  train_loss:  0.3080  ,  train_accuracy:  0.8869 \n",
      "Epoch 5/5\n",
      "1874/1875   [=============================> ]   2.95s  1.36ms/batch  ,  train_loss:  0.2900  ,  train_accuracy:  0.8934 \n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "# train_datasource = Datasource(MNIST, batchsize=64)\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "model = fashion_mnist_mlp()\n",
    "\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: fashion_mnist_loss(model(X), y)))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=None,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=model,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-greeting",
   "metadata": {},
   "source": [
    "## Performance Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-count",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "careful-culture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.34\n"
     ]
    }
   ],
   "source": [
    "print('98.56', end='')\n",
    "time.sleep(1)\n",
    "print('\\r64.34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cardiac-hudson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello       10.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 10\n",
    "f'Hello {x:>10.1f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "little-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return value.__format__(format_spec)\n",
      "\n",
      "format_spec defaults to the empty string.\n",
      "See the Format Specification Mini-Language section of help('FORMATTING') for\n",
      "details.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "??format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "artificial-magic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"Reconstructs a pytree from the treedef and the leaves.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  The inverse of :func:`tree_flatten`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Args:\u001b[0m\n",
       "\u001b[0;34m    treedef: the treedef to reconstruct\u001b[0m\n",
       "\u001b[0;34m    leaves: the list of leaves to use for reconstruction. The list must match\u001b[0m\n",
       "\u001b[0;34m      the leaves of the treedef.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Returns:\u001b[0m\n",
       "\u001b[0;34m    The reconstructed pytree, containing the ``leaves`` placed in the structure\u001b[0m\n",
       "\u001b[0;34m    described by ``treedef``.\u001b[0m\n",
       "\u001b[0;34m  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.8/site-packages/jax/_src/tree_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?jax.tree_unflatten??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "?jax.tree_unflatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "experienced-supplier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x7f7f78ee1f70>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "juvenile-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = model.tree_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "local-sport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x7f7ef447b760>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sequential.tree_unflatten(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "basic-entry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(6., dtype=float32)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sum(jnp.array([1.0,2.0,3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "improved-remains",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello               '"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'hello'\n",
    "f'{x:<20s}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "paperback-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(percentage, total=30):\n",
    "    x = int(percentage*total)\n",
    "    if x < total:\n",
    "        r = '[' + ''.join(['=']*x) + '>' + ''.join([' ']*(total-x)) + ']' \n",
    "    else:\n",
    "        r = '[' + ''.join(['=']*(total+1)) + ']' \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "guided-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================>] 1.00000000000000014\r"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0,1.1,0.1):\n",
    "    print(f'{progress_bar(i)} {i}', end='\\r')\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "velvet-defendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 797us/step - loss: 3.0031 - accuracy: 0.6674\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 796us/step - loss: 0.6990 - accuracy: 0.7307\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 871us/step - loss: 0.5967 - accuracy: 0.7813\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 811us/step - loss: 0.5475 - accuracy: 0.8084\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 800us/step - loss: 0.5315 - accuracy: 0.8153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e6236c490>"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1/255.0),\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-discrimination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
