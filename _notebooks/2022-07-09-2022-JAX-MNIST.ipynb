{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "located-carnival",
   "metadata": {},
   "source": [
    "# Fashion MNIST with Vanilla JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [deep learning, JAX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-stations",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post, I'm going to implement a basic Fashion-MNIST classifier using JAX.  JAX is an array-processing library that uses Google's XLA (Accelerated Linear Algebra) compiler to generate high-performance code that can run on a variety of hardware platforms.  It feels a lot like numpy, with a number of advantages including built in automatic differentiation, vectorization and parallelization, and just-in-time compilation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-tolerance",
   "metadata": {},
   "source": [
    "By the end of this write up, I'm going to end up with this:\n",
    "\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=32)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X))))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=valid_datasource,\n",
    "    optimizer=Adam(network, lr=1e-3), \n",
    "    loss_fn=sparse_cross_entropy, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1874/1875  [===============================] - 2s  1.65ms/batch - loss: 0.4956  - accuracy: 0.8250     - val_loss: 0.4446 - val_accuracy: 0.8396\n",
    "Epoch 2/5\n",
    "1874/1875  [===============================] - 2s  1.40ms/batch - loss: 0.3716  - accuracy: 0.8654     - val_loss: 0.4271 - val_accuracy: 0.8446\n",
    "Epoch 3/5\n",
    "1874/1875  [===============================] - 2s  1.39ms/batch - loss: 0.3330  - accuracy: 0.8791     - val_loss: 0.4034 - val_accuracy: 0.8528\n",
    "Epoch 4/5\n",
    "1874/1875  [===============================] - 2s  1.46ms/batch - loss: 0.3079  - accuracy: 0.8871     - val_loss: 0.3797 - val_accuracy: 0.8616\n",
    "Epoch 5/5\n",
    "1874/1875  [===============================] - 2s  1.97ms/batch - loss: 0.2891  - accuracy: 0.8935     - val_loss: 0.3599 - val_accuracy: 0.8710\n",
    "```\n",
    "\n",
    "Besides using the Keras datasets library, everything will be written from scratch.  My goal is to demonstrate how simple it is to build the begininnings of a deep-learning library when you're working with the right set of tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-imagination",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "hidden-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable\n",
    "\n",
    "eps = jnp.finfo(jnp.float64).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-advancement",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The `Dataset` and `Dataloader` classes shown below are stripped down versions of their Pytorch versions.  For now, a `Dataset` is a container for JAX feature and label arrays.  In fact, besides having a length ( because it has a `__len__` method), it's pretty useless.  The `Dataloader` gives you the ability to iterate over batches of data from a `Dataset`, which is very important for effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "emerging-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return jnp.shape(self.X)[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "searching-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, dataset: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-sucking",
   "metadata": {},
   "source": [
    "Here's an example of the `Dataset` and `Dataloader` in action.  First we grab the training data using the Keras library and build a dataloader that grabs `8` images at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "outstanding-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), _ = tf.keras.datasets.fashion_mnist.load_data() \n",
    "dataloader = Dataloader(\n",
    "    Dataset(X_train, y_train),\n",
    "    batchsize=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-exercise",
   "metadata": {},
   "source": [
    "To take a look at the first set of images, I'll convert `dataloader` to an iterator and use `next`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "contemporary-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_0 = (8, 28, 28), shape of y_0 = (8,)\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(dataloader))\n",
    "X_0, y_0 = first_batch\n",
    "print(f'shape of X_0 = {X_0.shape}, shape of y_0 = {y_0.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-assault",
   "metadata": {},
   "source": [
    "Indeed, the first batch contains 8, $28 x 28$ images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-athens",
   "metadata": {},
   "source": [
    "## The Sequential Model\n",
    "\n",
    "A sequential model is a list of `Callable` objects that is evaluated by calling each members `__call__` method in order.  Each list element is a registered pytree, but I also wanted to have the flexibility to pass ordinary functions to the `Sequential` constructor and have everything just work.  As you'll soon see this feature was implemented by modifying the `__init__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-stock",
   "metadata": {},
   "source": [
    "### `Linear` Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-edinburgh",
   "metadata": {},
   "source": [
    "The `Linear` layer defined below is very similar to implementations (but less general) you'd find in other non-JAX neural network libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "vertical-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Linear:\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    ni: int \n",
    "    no: int \n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, build=True, seed=1234):\n",
    "        self.ni = num_inputs \n",
    "        self.no = num_outputs \n",
    "        # want to add seed as internal object\n",
    "        if build:\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs)\n",
    "            self.b = jnp.zeros(num_outputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(num_inputs={self.ni}, num_outputs={self.no})'\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "    def params(self):\n",
    "        return {'w': self.w, 'b': self.b}\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return (self.w, self.b), (self.ni, self.no)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layer = cls(*aux_data, build=False)\n",
    "        layer.w, layer.b = children\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-railway",
   "metadata": {},
   "source": [
    "The one glaring difference is the decorator and the two *tree* methods.  As the JAX documentation explains, the `register_pytree_node_class`, `tree_flatten` method, and `tree_unflatten` class methods are required to add a user-defined class to the JAX pytree registry.  Once added, JAX will know how to transform back and forth between objects that the rest of JAX system can efficiently operate on, and objects that are specific to your application.    \n",
    "\n",
    "The `tree_flatten` method returns a two-element tuple consisting of the parameters you want to expose to JAX, and any meta-data that can help reconstruct the object.  Because JAX embraces the functional paradigm of immutable data structures, I thought it might be better to express the parameters as a tuple.  For `Linear`, the parameters are the weights and biases of the neural network.  For now, the only meta-data that seems helpful are the number of inputs and outputs (although this could be derived from the shape of the weights.  \n",
    "\n",
    "Another noteworthy thing about `Linear` is the `build` attribute.  Most of the time, you want to initialize the weights and biases at creation time time.  However, you don't want to do this when JAX reconstructs the object from it's flattened representation.  You probably just want to plop the parameters right into a freshly constructed object. The `build` attribute gives you some flexibility in that regard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-brand",
   "metadata": {},
   "source": [
    "### `Function` Layer\n",
    "\n",
    "The `Function` class fills the same need that `Lambda` layers do in Keras: being able to conveniently plug functions into models.  As the `tree_flatten` method shows, classes registered as pytrees can be parameter-free. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "greek-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Function:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Function({self.fn.__name__})'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [], self.fn\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(aux_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-harvest",
   "metadata": {},
   "source": [
    "### Helpful Functions\n",
    "\n",
    "Here are a few functions that will be *lifted* to `Function` layer in the `Sequential` model.  One common approach for improving classification accuracy is to normalize your input data.  When working with gray-scale images, this typically means rescaling the pixels from $[0,255]$ to $[0,1]$.  This is what `rescale_image` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "caroline-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_image(x): return x / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-swimming",
   "metadata": {},
   "source": [
    "The model built in this post operates on batches of two-dimensional gray-scale images.  Each batch is a three-dimensional array and can be interpreted as a vertical stack of 2D images, where the height of the vertical stack is the number of images.  The `flatten` goes through each slice of the vertical stack and transforms the 2D array into a one-dimensional array.  In the process, the 3D input becomes a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "civil-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    shape = jnp.shape(x)\n",
    "    assert len(shape) == 3, 'x must represent a batch of two-dimensional gray-scale images' \n",
    "    batch_size = shape[0]\n",
    "    return jnp.reshape(x, (batch_size, -1)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-marshall",
   "metadata": {},
   "source": [
    "The last two functions we'll implement in this section are `relu` and `softmax`.  A couple of tests are also provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "smart-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.clip(x, a_min=0)    \n",
    "\n",
    "x = np.random.randn(10,10)\n",
    "assert np.all(np.isclose(relu(x), tf.nn.relu(x))), 'test failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "metropolitan-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = jnp.exp(x)\n",
    "    return ex / jnp.sum(ex, axis=-1, keepdims=True)    \n",
    "\n",
    "x = np.random.randn(5, 10)\n",
    "assert np.all(np.isclose(softmax(x), tf.nn.softmax(x))), 'test failed'\n",
    "assert np.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-mileage",
   "metadata": {},
   "source": [
    "### Sequential Model\n",
    "\n",
    "With the supporting pieces implemented, the `Sequential` class can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "geological-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'tree_flatten'):\n",
    "                self.layers.append(layer)\n",
    "            elif callable(layer):\n",
    "                self.layers.append(Function(layer))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for layer in self.layers:\n",
    "            string += (repr(layer) + '\\n')\n",
    "        return string\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        aux_data, children = [], []\n",
    "        for layer in self.layers:\n",
    "            params, extra_stuff = jax.tree_flatten(layer)\n",
    "            aux_data.append(extra_stuff)\n",
    "            children.append(params)\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layers = []\n",
    "        for params, spec in zip(children, aux_data):\n",
    "            layers.append(jax.tree_unflatten(spec, params))\n",
    "        return Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-continent",
   "metadata": {},
   "source": [
    "Based on the previous high-level description, it should not be surprising that `Sequential` is essentially a wrapper around a list of layers.  The `__init__` method loops through the list of input objects and does one of the following things:\n",
    "\n",
    "1. if it is a pytree, adds the layer to the list of layers\n",
    "2. if it is a function, but not a pytree, wraps the function in a `Function` object and adds the pytree to the list of layers.\n",
    "\n",
    "Either way, at the end of construction, each `Sequential` instance is a list of pytrees.  At this point, `Sequential` is made into a pytree in the usual way: by implementing `tree_flatten` and `tree_unflatten` which loop over the layers, calling each layer's flatten of unflatten method, and collecting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-integration",
   "metadata": {},
   "source": [
    "### Some Testing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "broadband-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(rescale_image)\n",
      "Function(flatten)\n",
      "Linear(num_inputs=784, num_outputs=128)\n",
      "Function(relu)\n",
      "Linear(num_inputs=128, num_outputs=10)\n",
      "Function(softmax)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fashion_mnist_mlp():\n",
    "    return Sequential(\n",
    "        rescale_image,\n",
    "        flatten,\n",
    "        Linear(784, 128),\n",
    "        relu,\n",
    "        Linear(128, 10),\n",
    "        softmax  \n",
    "    )\n",
    "\n",
    "model = fashion_mnist_mlp()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-cursor",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "Cross Entropy is one of the most common loss functions for classification problems.  I'm going to spare you the long-winded mathematical justification of why it's a useful function.  Let me just say that it measures how close two probability distributions are. \n",
    "\n",
    "Here are two different versions of cross-entropy.  The first version (`cross_entropy`) assumes that `y_true` is one-hot encoded while the second version (`sparse_cross_entropy`) assumes that `y_true` is an array of indices.  I like the sparse version because it seems more efficient and less dependent on knowing the number of categories in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "significant-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def cross_entropy(y_true, probs):\n",
    "    batch_size = jnp.shape(probs)[0]\n",
    "    return -jnp.sum(jnp.log(probs + eps) * y_true) / batch_size\n",
    "\n",
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "assert np.all(np.isclose(cross_entropy(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "premium-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sparse_cross_entropy(y_true, probs):\n",
    "    batch_size = len(y_true)\n",
    "    X = jnp.log(probs + eps)[jnp.arange(batch_size), y_true]\n",
    "    return -jnp.sum(X) / batch_size\n",
    "\n",
    "y_true = jnp.array([1, 2])\n",
    "y_pred = jnp.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "assert np.all(np.isclose(sparse_cross_entropy(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-dairy",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Optimizers update model parameters at each minibatch.  The simplest optimizer, stochastic gradient descent (aka SGD), is shown below and updates the parameters ($W$) as follows: \n",
    "\n",
    "$$\n",
    "    W \\leftarrow W - \\alpha \\frac{\\partial \\ell}{\\partial W}\n",
    "$$\n",
    "\n",
    "where $\\ell$ represents the loss-function used in model training and $\\alpha$ is the learning rate.  \n",
    "\n",
    "While certainly simple and fast, SGD is not the *go-to* optmizer these days.  That title seems to go to *Adam*, as it tends to be used in most examples I've seen.  From their original paper, Kingma and Ba describe Adam as \n",
    "\n",
    "> \"an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\"\n",
    "\n",
    "For now, an optimizer has an `__init__` and `step` method.  The `__init__` method initializes the parameters and `step` updates the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "reverse-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "useful-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.lr = lr \n",
    "    def step(self, model, grads):\n",
    "        return jax.tree_map(lambda p, g: p - self.lr*g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ranging-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7):\n",
    "        self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps\n",
    "        self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) \n",
    "        self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model)\n",
    "        self.k = 0 \n",
    "    def step(self, model, grad):\n",
    "        lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps\n",
    "        v, s = self.v, self.s\n",
    "        k = self.k = self.k+1\n",
    "        self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grad)\n",
    "        self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grad)\n",
    "        v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v)\n",
    "        s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s)\n",
    "        new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat)\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-radiation",
   "metadata": {},
   "source": [
    "This code illustrate why pytree registration is so important.  Consider the following line taken from `Adam`'s `step` method:\n",
    "\n",
    "```python\n",
    "self.v = jax.tree_map(lambda v, g: v_decay*v + (1-v_decay)*g, v, grad)\n",
    "```\n",
    "\n",
    "The passed in `lambda` is a function of two JAX arrays; `v` and `grads` are pytrees.  Internally, `jax.tree_map` executes 3 steps:\n",
    "\n",
    "1. pytrees are flattened\n",
    "2. function called on each item in the flattened pytree\n",
    "3. results unflattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-cemetery",
   "metadata": {},
   "source": [
    "## Progress Bar\n",
    "\n",
    "I really like how Keras logs information to the screen during model training, and decided to mimic the style.  Here's my version of the progress bar.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "thrown-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(percentage, total=30):\n",
    "    x = int(percentage*total)\n",
    "    if x < total:\n",
    "        r = '[' + ''.join(['=']*x) + '>' + ''.join(['.']*(total-x)) + ']' \n",
    "    else:\n",
    "        r = '[' + ''.join(['=']*(total+1)) + ']' \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-blackjack",
   "metadata": {},
   "source": [
    "It has space for 31 characters sandwiched between an opening and closing bracket.  Examples at various completion percentages are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-surge",
   "metadata": {},
   "source": [
    "1. 0% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "thrown-repository",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>..............................]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-bibliography",
   "metadata": {},
   "source": [
    "2. 10% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "complicated-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===>...........................]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-vatican",
   "metadata": {},
   "source": [
    "3. 100% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "rural-fraud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-coaching",
   "metadata": {},
   "source": [
    "## The Training Loop  \n",
    "\n",
    "The training loop is pretty simple, but I admit that it looks a little cluttered.  Most frameworks and libraries split the it into several separate functions, and incorporate a callback system that allows a user to customize it's functionality.  Without the flexibility that a callback system offers, you'd probably need to implement a new training loop for every project.  Besides that, callbacks lend themselved to less cluttered (and therefore less buggy) code.  I'll likely work on some semi-simple callback system in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "elder-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_datasource, valid_datasource, optimizer, loss_fn, grad_fn, model):\n",
    "    history = {'loss':[], 'accuracy':[]}\n",
    "    \n",
    "    if valid_datasource is not None:\n",
    "        history = {**history, 'valid_loss': [], 'valid_accuracy': []}\n",
    "\n",
    "    train_num_batches = len(train_datasource)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # TRAINING PHASE\n",
    "        train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0\n",
    "        \n",
    "        num_steps = 0 \n",
    "        \n",
    "        epoch_duration = 0.0\n",
    "        \n",
    "        # we know how many batches there are ... keep track\n",
    "        for i, (X_train, y_train) in enumerate(train_datasource):\n",
    "            \n",
    "            # logging\n",
    "            batch_start = time.time()\n",
    "\n",
    "            num_steps += 1\n",
    "            \n",
    "            # training loss and gradients for this particular batch\n",
    "            probs = model(X_train)\n",
    "            loss = loss_fn(y_train, probs)\n",
    "            grad = grad_fn(model, X_train, y_train)\n",
    "            model = optimizer.step(model, grad)\n",
    "            \n",
    "            \n",
    "            # Results aggregation\n",
    "            num_correct = jnp.sum(jnp.argmax(probs, axis=-1) == y_train)\n",
    "            train_loss_accum += loss \n",
    "            train_batch_size += len(y_train)\n",
    "            train_accuracy_accum += num_correct\n",
    "            train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "\n",
    "            # Logging ....\n",
    "            batch_duration = time.time() - batch_start\n",
    "            epoch_duration += batch_duration \n",
    "            log_batch_count = f'{i}/{train_num_batches}'\n",
    "            log_epoch_time = f'{int(epoch_duration)}s'\n",
    "            log_batch_time = f'{1_000*batch_duration:.2f}ms/batch'\n",
    "            log_batch_loss = f'loss: {train_loss:.4f}'\n",
    "            log_batch_accuracy = f'accuracy: {train_accuracy:.4f}'\n",
    "            log_string =  f'{log_batch_count:<10s} {progress_bar((i+1)/train_num_batches)} - {log_epoch_time:<3s} {log_batch_time:<5s} - {log_batch_loss:<13s} - {log_batch_accuracy:<20s}'\n",
    "            print(log_string, end='\\r') \n",
    "\n",
    "        # \n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_accuracy)      \n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        if valid_datasource is not None:\n",
    "            valid_num_batches = len(valid_datasource)\n",
    "            valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 \n",
    "\n",
    "            # Run validation step ...\n",
    "            for i, (X_valid, y_valid) in enumerate(valid_datasource):\n",
    "                num_steps += 1\n",
    "                probs = model(X_valid)\n",
    "                loss = loss_fn(y_valid, probs)\n",
    "                \n",
    "                valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid)\n",
    "\n",
    "                valid_loss_accum += loss\n",
    "                valid_batch_size += len(y_valid)\n",
    "\n",
    "            epoch_valid_loss = valid_loss_accum / valid_num_batches\n",
    "            epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size\n",
    "            log_valid_loss = f'val_loss: {epoch_valid_loss:.4f}'\n",
    "            log_valid_accuracy = f'val_accuracy: {epoch_valid_accuracy:.4f}'\n",
    "            log_string += f' - {log_valid_loss:<13s} - {log_valid_accuracy:<20s}' \n",
    "            \n",
    "            history['loss'].append(epoch_valid_loss)\n",
    "            history['accuracy'].append(epoch_valid_accuracy)\n",
    "        \n",
    "        # this log_string should include validation results\n",
    "        print(log_string, end='\\n')\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-solid",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "criminal-membership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1874/1875  [===============================] - 2s  1.65ms/batch - loss: 0.4956  - accuracy: 0.8250     - val_loss: 0.4446 - val_accuracy: 0.8396\n",
      "Epoch 2/5\n",
      "1874/1875  [===============================] - 2s  1.40ms/batch - loss: 0.3716  - accuracy: 0.8654     - val_loss: 0.4271 - val_accuracy: 0.8446\n",
      "Epoch 3/5\n",
      "1874/1875  [===============================] - 2s  1.39ms/batch - loss: 0.3330  - accuracy: 0.8791     - val_loss: 0.4034 - val_accuracy: 0.8528\n",
      "Epoch 4/5\n",
      "1874/1875  [===============================] - 2s  1.46ms/batch - loss: 0.3079  - accuracy: 0.8871     - val_loss: 0.3797 - val_accuracy: 0.8616\n",
      "Epoch 5/5\n",
      "1874/1875  [===============================] - 2s  1.97ms/batch - loss: 0.2891  - accuracy: 0.8935     - val_loss: 0.3599 - val_accuracy: 0.8710\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=32)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X))))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=valid_datasource,\n",
    "    optimizer=Adam(network, lr=1e-3), \n",
    "    loss_fn=sparse_cross_entropy, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-protocol",
   "metadata": {},
   "source": [
    "## Keras Execution\n",
    "\n",
    "For comparison, here's the training results for Keras on the same dataset.  While their are a few differences in the output, I'm pretty pleased with how well they match. I'm guessing that the reason there isn't an even better match comes from the fact that parameter initialization is slightly different.  I'm currently limited to Kaiming initialization, whereas Dense layers in Keras use Kaiming uniform by default.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "subjective-ebony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 945us/step - loss: 0.5048 - accuracy: 0.8246 - val_loss: 0.4426 - val_accuracy: 0.8448\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 927us/step - loss: 0.3779 - accuracy: 0.8637 - val_loss: 0.3890 - val_accuracy: 0.8589\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 964us/step - loss: 0.3402 - accuracy: 0.8765 - val_loss: 0.3942 - val_accuracy: 0.8593\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3141 - accuracy: 0.8853 - val_loss: 0.3635 - val_accuracy: 0.8683\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 997us/step - loss: 0.2954 - accuracy: 0.8911 - val_loss: 0.3466 - val_accuracy: 0.8777\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1/255.0),\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-pasta",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1. Implement convolutional layers and train a convolutional neural network\n",
    "2. Add a callback system to simplify the training loop\n",
    "3. Show validation results\n",
    "4. Evaluate performance on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "frozen-circus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001, 0.0009)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3, 900e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "collective-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `model.fit` not found.\n"
     ]
    }
   ],
   "source": [
    "??model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-arbor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
