{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minute-rogers",
   "metadata": {},
   "source": [
    "# Fashion MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png\n",
    "- hide: true\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-planning",
   "metadata": {},
   "source": [
    "In this article, I'm going to start implementing a JAX-based neural network library, and use it to synthesize a Fashion-MNIST classifier. Features will be added over the coming weeks and months as I try tackle more advanced topics and architectures. I don't intend to write a full-featured library/framework like Tensorflow or Pytorch. Instead, my goal is compile a set of loosely coupled components that I can mix-and-match to solve problems that interest me. When there's some educational benefit, funtionality will be written from scratch and validated against other well-known libraries.  \n",
    "\n",
    "Why do I want to do this: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-quality",
   "metadata": {},
   "source": [
    "# Why JAX\n",
    "\n",
    "JAX is a python-based numerical-computing library that uses Google's XLA (Accelerated Linear Algebra) compiler to generate high performance code that can run on a variety of hardware platforms.  It feels a lot like numpy, with a few quirks.  One of it's major selling points, particularly for machine learning and scientific computing, is it's ability to calculate derivatives.  It also heavily embraces the functional-programming-paradigm.\n",
    "\n",
    "There are several JAX-based neural network libraries in the open-source space "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-retail",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28,28)),\n",
    "  keras.layers.Dense(128, activation=keras.activations.relu),\n",
    "  keras.layers.Dense(10, activation=keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4955 - accuracy: 0.8250\n",
    "Epoch 2/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3726 - accuracy: 0.8643\n",
    "Epoch 3/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3354 - accuracy: 0.8781\n",
    "Epoch 4/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3136 - accuracy: 0.8845\n",
    "Epoch 5/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2937 - accuracy: 0.8927\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-avenue",
   "metadata": {},
   "source": [
    "## The Fashion MNIST Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "agreed-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 21:08:16.001764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/bin:/usr/local/lib:\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-increase",
   "metadata": {},
   "source": [
    "## Datasource API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "provincial-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return jnp.shape(self.X)[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dramatic-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, dataset: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "major-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "#X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "structural-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "magnetic-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Dataloader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "moral-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28) (32,)\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataloader:\n",
    "    print(X.shape, y.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-strengthening",
   "metadata": {},
   "source": [
    "## Parametric Equations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "personalized-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parametric: pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-colleague",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-branch",
   "metadata": {},
   "source": [
    "To get this to work, the `Linear` class must be registered as a pytree.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "respected-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Linear(Parametric):\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    ni: int \n",
    "    no: int \n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, build=True, seed=1234):\n",
    "        self.ni = num_inputs \n",
    "        self.no = num_outputs \n",
    "        # want to add seed as internal object\n",
    "        if build:\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs)\n",
    "            self.b = jnp.zeros(num_outputs)\n",
    "    \n",
    "    def merge(self, params):\n",
    "        self.w, self.b = params\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(num_inputs={self.ni}, num_outputs={self.no})'\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "    def tree_flatten(self):\n",
    "        return (self.w, self.b), (self.ni, self.no)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layer = cls(*aux_data, build=False)\n",
    "        layer.w, layer.b = children\n",
    "        #layer.merge(params=children)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "anticipated-killing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-diving",
   "metadata": {},
   "source": [
    "### Flatten\n",
    "\n",
    "The `Flatten` layer converts a two dimension array to a single dimensional array or a batch of two-dimensional arrays, to a batch of one-dimensional arrays.  Each two dimensional array is flattened row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "spread-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Function:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Function({self.fn.__name__})'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [None], self.fn\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(aux_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "permanent-bacteria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] PyTreeDef(CustomNode(<class '__main__.Function'>[<function relu at 0x7f7e65272dc0>], [None]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Function(relu)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = jax.tree_flatten(Function(relu))\n",
    "print(a, b)\n",
    "jax.tree_unflatten(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "interim-wilson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<lambda>'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = lambda a : a+1\n",
    "x.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "loving-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    shape = jnp.shape(x)\n",
    "        \n",
    "    if len(shape) == 1:\n",
    "        return x \n",
    "    elif len(shape) == 2:\n",
    "        # flatten a single 2D image\n",
    "        return jnp.reshape(x, -1)\n",
    "    elif len(shape) == 3:\n",
    "        # x is a batch of 2D images, flatten each image \n",
    "        batch_size = jnp.shape(x)[0]\n",
    "        return jnp.reshape(x, (batch_size, -1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "developing-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Flatten(Parametric):\n",
    "    def __call__(self, x):\n",
    "        shape = jnp.shape(x)\n",
    "        \n",
    "        if len(shape) == 1:\n",
    "            return x \n",
    "        elif len(shape) == 2:\n",
    "            # flatten a single 2D image\n",
    "            return jnp.reshape(x, -1)\n",
    "        elif len(shape) == 3:\n",
    "            # x is a batch of 2D images, flatten each image \n",
    "            batch_size = jnp.shape(x)[0]\n",
    "            return jnp.reshape(x, (batch_size, -1))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Flatten()'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [None], self.__class__.__name__\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-obligation",
   "metadata": {},
   "source": [
    "Here's an example of `Flatten` operating on a two-element batch of $3\\times 3$ arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "patent-handbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[1,2,3], [4,5,6]], [[7,8,9], [10,11,12]]])\n",
    "y = Flatten()(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-convertible",
   "metadata": {},
   "source": [
    "### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-internship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "quality-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return jnp.clip(x, a_min=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "numerous-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Relu(Parametric):\n",
    "    def __call__(self, x):\n",
    "        return jnp.clip(x, a_min=0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Relu()'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [None], self.__class__.__name__\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "connected-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10,10)\n",
    "assert jnp.all(jnp.isclose(relu(x), jax.nn.relu(x))), 'test failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-petersburg",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "latin-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = jnp.exp(x)\n",
    "    return ex / jnp.sum(ex, axis=1, keepdims=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "applicable-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Softmax(Parametric):\n",
    "    def __call__(self, x):\n",
    "        ex = jnp.exp(x)\n",
    "        return ex / jnp.sum(ex, axis=1, keepdims=True) \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Softmax()'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [None], self.__class__.__name__\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "recovered-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(5, 10)\n",
    "assert jnp.all(jnp.isclose(softmax(x), jax.nn.softmax(x))), 'test failed'\n",
    "assert jnp.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-bridges",
   "metadata": {},
   "source": [
    "### ScaleImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "political-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(x):\n",
    "    return x / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "smaller-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class NormalizeImage(Parametric):\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x / 255.0 \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'NormalizeImage()'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [None], self.__class__.__name__\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-seating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "quiet-college",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "guided-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Sequential(Parametric):\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, Parametric) or isinstance(layer, Function):\n",
    "                self.layers.append(layer)\n",
    "            elif callable(layer):\n",
    "                self.layers.append(Function(layer))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for layer in self.layers:\n",
    "            string += (repr(layer) + '\\n')\n",
    "        return string\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        aux_data, children = [], []\n",
    "        for layer in self.layers:\n",
    "            params, extra_stuff = jax.tree_flatten(layer)\n",
    "            aux_data.append(extra_stuff)\n",
    "            children.append(params)\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layers = []\n",
    "        for params, spec in zip(children, aux_data):\n",
    "            layers.append(jax.tree_unflatten(spec, params))\n",
    "        return Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "green-cuisine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(normalize_image)\n",
      "Function(flatten)\n",
      "Linear(num_inputs=784, num_outputs=128)\n",
      "Function(relu)\n",
      "Linear(num_inputs=128, num_outputs=10)\n",
      "Function(softmax)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Function(normalize_image)\n",
       "Function(flatten)\n",
       "Linear(num_inputs=784, num_outputs=128)\n",
       "Function(relu)\n",
       "Linear(num_inputs=128, num_outputs=10)\n",
       "Function(softmax)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def fashion_mnist_mlp():\n",
    "    model = Sequential(\n",
    "        normalize_image,\n",
    "        flatten,\n",
    "        Linear(784, 128),\n",
    "        relu,\n",
    "        Linear(128, 10),\n",
    "        softmax  \n",
    "    )\n",
    "\n",
    "    return model\n",
    "model = fashion_mnist_mlp()\n",
    "print(model)\n",
    "a, b = jax.tree_flatten(model)\n",
    "jax.tree_unflatten(b, a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-output",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-providence",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dried-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "published-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.lr = lr \n",
    "    def step(self, model, grads):\n",
    "        return jax.tree_map(lambda p, g: p - self.lr*g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "heavy-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7):\n",
    "        self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps\n",
    "        self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) \n",
    "        self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model)\n",
    "        self.k = 0 \n",
    "    def step(self, model, grads):\n",
    "        lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps\n",
    "        v, s = self.v, self.s\n",
    "        k = self.k = self.k+1\n",
    "        self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grads)\n",
    "        self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grads)\n",
    "        v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v)\n",
    "        s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s)\n",
    "        new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat)\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-theater",
   "metadata": {},
   "source": [
    "## Training Loop  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "mineral-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_datasource, valid_datasource, optimizer, loss_fn, grad_fn, model):\n",
    "    history = {'loss':[], 'accuracy':[]}\n",
    "    \n",
    "    if valid_datasource is not None:\n",
    "        history = {**history, 'valid_loss': [], 'valid_accuracy': []}\n",
    "\n",
    "    train_num_batches = len(train_datasource)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # TRAINING PHASE\n",
    "        train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0\n",
    "        \n",
    "        num_steps = 0 \n",
    "        \n",
    "        epoch_duration = 0.0\n",
    "        for i, (X_train, y_train) in enumerate(train_datasource):\n",
    "            \n",
    "            # logging\n",
    "            batch_start = time.time()\n",
    "\n",
    "            num_steps += 1\n",
    "            \n",
    "            # training loss and gradients for this particular batch\n",
    "            probs = model(X_train)\n",
    "            loss = loss_fn(probs, y_train)\n",
    "            \n",
    "            grads = grad_fn(model, X_train, y_train)\n",
    "            model = optimizer.step(model, grads)\n",
    "                  \n",
    "            # update for metrics\n",
    "            train_loss_accum += loss \n",
    "            train_batch_size += len(y_train)\n",
    "            train_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_train)\n",
    "            train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "\n",
    "            # Logging ....\n",
    "            batch_duration = time.time() - batch_start\n",
    "            epoch_duration += batch_duration \n",
    "            log_batch_count = f'{i}/{train_num_batches}'\n",
    "            log_epoch_time = f'{epoch_duration:>5.2f}s'\n",
    "            log_batch_time = f'{1_000*batch_duration:>5.2f}ms/batch'\n",
    "            log_batch_loss = f'train_loss:  {train_loss:>5.2f}'\n",
    "            log_batch_accuracy = f'train_accuracy:  {100*train_accuracy:<5.2f}'\n",
    "            log_string =  f'{log_batch_count}  [           ]  {log_epoch_time} {log_batch_time}  ,  {log_batch_loss}  ,  {log_batch_accuracy} '\n",
    "            print(log_string, end='\\r') \n",
    "\n",
    "        # \n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_accuracy)      \n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        if valid_datasource is not None:\n",
    "            valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 \n",
    "\n",
    "            # Run validation step ...\n",
    "            for X_valid, y_valid in valid_datasource:\n",
    "                num_steps += 1\n",
    "                probs = model(X_valid)\n",
    "                loss = loss_fn(probs, y_valid)\n",
    "                \n",
    "                valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid)\n",
    "\n",
    "                valid_loss_accum += loss\n",
    "                valid_batch_size += len(y_valid)\n",
    "\n",
    "            epoch_valid_loss = valid_loss_accum / valid_batch_size \n",
    "            epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size\n",
    "\n",
    "            history['loss'].append(epoch_valid_loss)\n",
    "            history['accuracy'].append(epoch_valid_accuracy)\n",
    "        \n",
    "        # this log_string should include validation results\n",
    "        print(log_string, end='\\n')\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "extra-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_mnist_loss(probs, y_true, num_classes=10):\n",
    "    # average cross entropy, batch\n",
    "    y_one_hot = jax.nn.one_hot(y_true, num_classes)\n",
    "    return -jnp.sum(jnp.log(probs) * y_one_hot) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "automotive-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "624/625  [           ]   4.78s  7.43ms/batch  ,  train_loss:   0.56  ,  train_accuracy:  80.31 \n",
      "Epoch 2/5\n",
      "624/625  [           ]   4.79s  7.07ms/batch  ,  train_loss:   0.41  ,  train_accuracy:  85.45 \n",
      "Epoch 3/5\n",
      "624/625  [           ]   4.85s  8.14ms/batch  ,  train_loss:   0.37  ,  train_accuracy:  86.83 \n",
      "Epoch 4/5\n",
      "624/625  [           ]   4.94s  7.94ms/batch  ,  train_loss:   0.34  ,  train_accuracy:  87.76 \n",
      "Epoch 5/5\n",
      "624/625  [           ]   5.61s  8.44ms/batch  ,  train_loss:   0.32  ,  train_accuracy:  88.52 \n"
     ]
    }
   ],
   "source": [
    "X_train_, y_train_ = X_train[:40_000,:,:], y_train[:40_000]\n",
    "X_valid, y_valid = X_train[40_000:,:,:], y_train[40_000:]\n",
    "\n",
    "train_dataset = Dataset(X_train_, y_train_)\n",
    "valid_dataset = Dataset(X_valid, y_valid)\n",
    "\n",
    "train_datasource = Dataloader(train_dataset, batchsize=64)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "model = fashion_mnist_mlp()\n",
    "\n",
    "\n",
    "\n",
    "grad_fn = jax.grad(lambda model, X, y: fashion_mnist_loss(model(X), y))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=valid_datasource, \n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=model,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-matter",
   "metadata": {},
   "source": [
    "## Performance Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-lyric",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "solved-toolbox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.34\n"
     ]
    }
   ],
   "source": [
    "print('98.56', end='')\n",
    "time.sleep(1)\n",
    "print('\\r64.34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "collaborative-prevention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello       10.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 10\n",
    "f'Hello {x:>10.1f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "blond-asthma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return value.__format__(format_spec)\n",
      "\n",
      "format_spec defaults to the empty string.\n",
      "See the Format Specification Mini-Language section of help('FORMATTING') for\n",
      "details.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "??format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "medical-pocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"Reconstructs a pytree from the treedef and the leaves.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  The inverse of :func:`tree_flatten`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Args:\u001b[0m\n",
       "\u001b[0;34m    treedef: the treedef to reconstruct\u001b[0m\n",
       "\u001b[0;34m    leaves: the list of leaves to use for reconstruction. The list must match\u001b[0m\n",
       "\u001b[0;34m      the leaves of the treedef.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Returns:\u001b[0m\n",
       "\u001b[0;34m    The reconstructed pytree, containing the ``leaves`` placed in the structure\u001b[0m\n",
       "\u001b[0;34m    described by ``treedef``.\u001b[0m\n",
       "\u001b[0;34m  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.8/site-packages/jax/_src/tree_util.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?jax.tree_unflatten??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "?jax.tree_unflatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "tropical-corruption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x7f7f78ee1f70>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "positive-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = model.tree_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "radio-survival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x7f7ef447b760>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sequential.tree_unflatten(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-portugal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
