{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-series",
   "metadata": {},
   "source": [
    "# Fashion MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723eea1",
   "metadata": {},
   "source": [
    "My main objective here is to develop a Keras like API for JAX, and use it on the so-called Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "burning-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optax \n",
    "\n",
    "from typing import Callable, List, NamedTuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c7223",
   "metadata": {},
   "source": [
    "## Grabbing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "f98152f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "4e6f0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "4884b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 60000\n",
      "Number of test samples = 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Number of training samples = {len(y_train)}') \n",
    "print(f'Number of test samples = {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "905a2971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = X_train[0]\n",
    "np.set_printoptions(linewidth=200)\n",
    "plt.imshow(X, cmap='gray')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "f925bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-language",
   "metadata": {},
   "source": [
    "## Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "a5526e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function: \n",
    "    pass\n",
    "class Model: \n",
    "    def forward(self, params, X, y):\n",
    "        raise NotImplementedError\n",
    "    def compile(self, loss, optimizer, metrics, params):\n",
    "        self.loss = loss \n",
    "        self.optimizer = optimizer \n",
    "        self.metrics = metrics \n",
    "        self.params = params\n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        train_dataset = train_dataset.shuffle(100).batch(32)\n",
    "        \n",
    "        loss = self.loss \n",
    "        optimizer = self.optimizer \n",
    "        params = self.params\n",
    "        # Get the parameters from the model ...\n",
    "\n",
    "        # Taken from optax tutorial code ...\n",
    "        opt_state = self.optimizer.init(params)\n",
    "        \n",
    "        def step(params, opt_state, batch, labels):\n",
    "            loss_value, grads = jax.value_and_grad(loss)(params, self.forward, batch, labels)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return params, opt_state, loss_value\n",
    "\n",
    "        # one epoch\n",
    "        for i, (batch, labels) in enumerate(train_dataset.as_numpy_iterator()):\n",
    "            loss_value, _ = jax.value_and_grad(loss)(params, self.forward, batch, labels)\n",
    "            params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
    "            if i % 100 == 0:\n",
    "                print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "cb0c53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Model):\n",
    "    def __init__(self, num_features, activation=jax.nn.relu):\n",
    "        self.num_features = num_features \n",
    "        self.activation = activation\n",
    "    \n",
    "    def setup(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        shape = (self.num_features, self.input_shape)\n",
    "        key = jax.random.PRNGKey(1234)\n",
    "        _params = {\n",
    "            #'input_shape': input_shape,\n",
    "            #'num_features': self.num_features,\n",
    "            'weights': jax.random.normal(key, shape) * jnp.sqrt(2.0 / input_shape),\n",
    "            'bias': jnp.zeros(self.num_features)\n",
    "        }\n",
    "        return _params \n",
    "\n",
    "    def forward(self, params, x):\n",
    "        w, b = params['weights'], params['bias']\n",
    "        return self.activation(jnp.dot(w, x) + b) \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.activation(jnp.dot(self.w, x) + self.b) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.        , 0.        , 0.        , 3.9159968 , 4.3758116 , 0.2836208 , 0.10902514, 0.        , 0.44189295, 1.9883591 ], dtype=float32)"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = Dense(10)\n",
    "p = l.setup(input_shape=3)\n",
    "l.forward(p, np.random.randn(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7786d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff41fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "a1482a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Function):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_features= np.prod(input_shape) \n",
    "    def setup(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_features = np.prod(input_shape)\n",
    "    def __call__(self, x):\n",
    "        assert jnp.shape(x) == self.input_shape, f'Shapes do not match'\n",
    "        return jnp.reshape(x, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "05462254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Model):\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "    def setup(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        params = []\n",
    "        input_shape = self.input_shape\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Model):\n",
    "                params.append(layer.setup(input_shape))\n",
    "                input_shape = layer.num_features\n",
    "            elif isinstance(layer, Function): \n",
    "                #params.append({'input_dim': layer.input_shape, 'num_features': layer.num_features})\n",
    "                input_shape = layer.num_features\n",
    "        return params    \n",
    "    def __call__(self, x):\n",
    "        y = x\n",
    "        for layer in self.layers:\n",
    "            y = layer(x)\n",
    "        return y\n",
    "        \n",
    "    def forward(self, params, x):\n",
    "        y = x\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Model):\n",
    "                y = layer.forward(params[i], y)\n",
    "                i += 1 \n",
    "            else:\n",
    "                y = layer(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f2c8d",
   "metadata": {},
   "source": [
    "The type of model we're looking to build can be represented in Keras, as follows:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28,28)),\n",
    "  keras.layers.Dense(128, activation=keras.activations.relu),\n",
    "  keras.layers.Dense(10, activation=keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd1b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49876033",
   "metadata": {},
   "source": [
    "# How can we convert the following simple module into something usable, this is really just a specification ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "6a7218ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(x):\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation=jax.nn.relu)(x)\n",
    "    x = Dense(10)\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "43542351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    x = Flatten()(x)',\n",
       " '    x = Dense(128, activation=jax.nn.relu)(x)',\n",
       " '    x = Dense(10)',\n",
       " '    return x    ']"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import inspect\n",
    "inspect.getsource(MLP).split('\\n')[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-spirit",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b16f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "instructional-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters pulled from model, this seems really clunky ...\n",
    "def cross_entropy_loss(params, forward, X, y):\n",
    "    # Need to one-bot encode input labels, recall that MNIST has 10 classes, 0-9\n",
    "    y_one_hot = jax.nn.one_hot(y, 10).astype(jnp.float32)\n",
    "    logits = jax.vmap(forward, in_axes=(None, 0))(params, X) \n",
    "    # calculate loss for each sample in minibatch\n",
    "    loss = optax.softmax_cross_entropy(logits, y_one_hot)\n",
    "    # average the losses for each sample in minibatch\n",
    "    loss = loss.mean() \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "c6bfca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation=jax.nn.relu),\n",
    "    Dense(10, activation = jax.nn.softmax)   \n",
    ")\n",
    "\n",
    "init_params = model.setup(input_shape=(28, 28))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17fad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "b6da26a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.02157694, 0.09008522, 0.01330612, 0.14697005, 0.03000481, 0.31934047, 0.08668316, 0.07532562, 0.19995402, 0.01675361], dtype=float32)"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(init_params, np.random.rand(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "91803aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.388558, dtype=float32)"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(init_params, model.forward, np.random.randn(2, 28, 28), [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "bc526387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 2.3042030334472656\n",
      "step 100, loss: 1.7702372074127197\n",
      "step 200, loss: 1.8559722900390625\n",
      "step 300, loss: 1.8455866575241089\n",
      "step 400, loss: 1.6030317544937134\n",
      "step 500, loss: 1.7808022499084473\n",
      "step 600, loss: 1.7045600414276123\n",
      "step 700, loss: 1.6650400161743164\n",
      "step 800, loss: 1.6782312393188477\n",
      "step 900, loss: 1.759960651397705\n",
      "step 1000, loss: 1.7251852750778198\n",
      "step 1100, loss: 1.7144274711608887\n",
      "step 1200, loss: 1.5780982971191406\n",
      "step 1300, loss: 1.5931434631347656\n",
      "step 1400, loss: 1.7018799781799316\n",
      "step 1500, loss: 1.7146620750427246\n",
      "step 1600, loss: 1.6924097537994385\n",
      "step 1700, loss: 1.626793622970581\n",
      "step 1800, loss: 1.6654064655303955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss=cross_entropy_loss, optimizer = optax.adam(learning_rate=1e-3), metrics = ['accuracy'], params=init_params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-knock",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training loop tunes the model parameters for a specified number of epochs.  For now the parameters are adjusted once per epoch, after the entire dataset is consumed by the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-finder",
   "metadata": {},
   "source": [
    "## Performance Curve\n",
    "\n",
    "Let's see the trend in the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-hudson",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66e6e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(10, 3)\n",
    "w = np.random.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5f2b063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95615652, -0.60910943,  0.47719404,  0.50628421, -0.57886369],\n",
       "       [ 3.09498684,  1.70024379, -1.01957485,  2.25813896, -0.37952626],\n",
       "       [-3.67871561, -0.1351104 ,  0.04294664, -1.84479421,  0.98852387],\n",
       "       [-1.00596024,  0.46315551,  0.67104569,  3.13116358, -1.09329311],\n",
       "       [ 2.07878921,  1.88358723, -1.42939824,  0.84352964,  0.39912581],\n",
       "       [ 2.91327168, -1.07647233,  0.73089571,  1.0445742 , -1.1687821 ],\n",
       "       [ 1.03321149,  0.51671102,  0.70734573,  4.30933842, -1.72017219],\n",
       "       [-2.73799462, -1.54710401,  1.83484434,  1.17423297, -1.08464385],\n",
       "       [-1.6715637 , -2.74092988,  1.91974295, -1.18807747, -0.68628707],\n",
       "       [ 1.21170605,  0.15311168, -0.1815407 ,  0.30359954, -0.13915325]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, np.transpose(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9b25ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9561565  -0.6091094   0.47719404  0.50628424 -0.5788637 ]\n",
      " [ 3.0949867   1.7002438  -1.0195749   2.258139   -0.37952614]\n",
      " [-3.6787155  -0.13511032  0.04294658 -1.8447943   0.9885239 ]\n",
      " [-1.0059603   0.46315545  0.67104566  3.1311636  -1.093293  ]\n",
      " [ 2.0787892   1.8835871  -1.4293982   0.8435297   0.39912578]\n",
      " [ 2.9132717  -1.0764723   0.7308957   1.044574   -1.1687821 ]\n",
      " [ 1.0332114   0.516711    0.7073457   4.309338   -1.7201722 ]\n",
      " [-2.7379947  -1.547104    1.8348444   1.174233   -1.0846438 ]\n",
      " [-1.6715636  -2.7409298   1.919743   -1.1880776  -0.686287  ]\n",
      " [ 1.2117062   0.15311167 -0.1815407   0.3035995  -0.13915324]]\n"
     ]
    }
   ],
   "source": [
    "def lin(x): return jnp.dot(w, x)\n",
    "\n",
    "yy = jax.vmap(lin)(X)\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb8757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
