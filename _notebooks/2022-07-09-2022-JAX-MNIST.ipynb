{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binding-robert",
   "metadata": {},
   "source": [
    "# Fashion MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-charter",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post, I'm going to implement a basic Fashion-MNIST classifier using JAX.  JAX is an array-processing library that uses Google's XLA (Accelerated Linear Algebra) compiler to generate high-performance code that can run on a variety of hardware platforms.  It feels a lot like numpy, with a number of advantages including built in automatic differentiation, parallelization, and just-in-time compilation.  It's built in array type is called a `DeviceArray`.  Unlike numpy's `ndarray` type, elements of `DeviceArray`s cannot be directly mutated.  The other fundamental datatype in JAX is the *pytree*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-access",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "\n",
    "Here's one of the Fashion MNIST classifiers you e .  The API design is very elegant and easy to \n",
    "understand.  I even like the logging information provided by the call to `model.fit`, so much in fact that I'm going to replicate the style in\n",
    "my training loop.\n",
    "\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1/255),\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4955 - accuracy: 0.8250\n",
    "Epoch 2/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3726 - accuracy: 0.8643\n",
    "Epoch 3/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3354 - accuracy: 0.8781\n",
    "Epoch 4/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3136 - accuracy: 0.8845\n",
    "Epoch 5/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2937 - accuracy: 0.8927\n",
    "```\n",
    "\n",
    "\n",
    "Here's what I'll be able to do by the end of this post.  Okay, it's nowhere near as nice\n",
    "as the Keras API at this point.  Part of that comes from the fact that Keras tries to hide\n",
    "some of the details from you, while I'm definitely not.  \n",
    "\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: fashion_mnist_loss(model(X), y)))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=None,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1874/1875  [===============================] - 3s  1.49ms/batch  -  loss: 0.4958   -  accuracy: 0.8245      \n",
    "Epoch 2/5\n",
    "1874/1875  [===============================] - 2s  1.40ms/batch  -  loss: 0.3725   -  accuracy: 0.8654    \n",
    "Epoch 3/5\n",
    "1874/1875  [===============================] - 2s  1.53ms/batch  -  loss: 0.3339   -  accuracy: 0.8778    \n",
    "Epoch 4/5\n",
    "1874/1875  [===============================] - 2s  1.55ms/batch  -  loss: 0.3080   -  accuracy: 0.8869    \n",
    "Epoch 5/5\n",
    "1874/1875  [===============================] - 2s  1.46ms/batch  -  loss: 0.2900   -  accuracy: 0.8934\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-batch",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "lightweight-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-philippines",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "I really like Pytorch's approach to handling data, and could have just imported it's `Dataset` and `Dataloader` classes.  My little project is simple enough that I don't need the extra features their implementation provides.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "electric-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return jnp.shape(self.X)[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "acoustic-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, dataset: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "alternate-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), _ = tf.keras.datasets.fashion_mnist.load_data()\n",
    "dataset = Dataset(X_train, y_train)\n",
    "dataloader = Dataloader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-reunion",
   "metadata": {},
   "source": [
    "## The Sequential Model\n",
    "\n",
    "A sequential model is simply a list of `Callable` objects that is evaluated by evaluating each member of the list in sequence.  In my implementation, each element of the list must be a registered pytree, but I also wanted to have the flexibility to pass ordinary functions to the `Sequential` constructor and have everything just work.  As you'll soon seem this feature was implemented by modifying the `__init__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-danger",
   "metadata": {},
   "source": [
    "### `Linear` Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-earth",
   "metadata": {},
   "source": [
    "The `Linear` layer defined below is very similar to implementations (but less general) you'd find in other non-JAX neural network libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "stock-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Linear:\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    ni: int \n",
    "    no: int \n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, build=True, seed=1234):\n",
    "        self.ni = num_inputs \n",
    "        self.no = num_outputs \n",
    "        # want to add seed as internal object\n",
    "        if build:\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs)\n",
    "            self.b = jnp.zeros(num_outputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(num_inputs={self.ni}, num_outputs={self.no})'\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "    def params(self):\n",
    "        return {'w': self.w, 'b': self.b}\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return (self.w, self.b), (self.ni, self.no)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layer = cls(*aux_data, build=False)\n",
    "        layer.w, layer.b = children\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-hepatitis",
   "metadata": {},
   "source": [
    "The one glaring difference is the decorator and the two *tree* methods.  As the JAX documentation explains, the `register_pytree_node_class`, `tree_flatten` method, and `tree_unflatten` class methods are used to add a user-defined class to the JAX pytree registry.  Once added, JAX will know how to transform back and forth between objects that the rest of JAX system (including the just-in-time compiler, vectorization/parallelization, and automatic differentiation) can efficiently operate on, and objects that are specific to your application.    \n",
    "\n",
    "The `tree_flatten` method returns a two-element tuple consisting of the parameters you want to expose to JAX, and any meta-data that can help reconstruct the object.  Because JAX seems to really embrace the functional paradigm of immutable datastructures, I thought it might be better to express the parameters as a tuple.  For `Linear`, the parameters are the weights and biases of the neural network.  For now, the only meta-data that seems helpful are the number of inputs and outputs (although this could be derived from the shape of the weights.  \n",
    "\n",
    "Another thing to notice about `Linear` is the `build` attribute.  Most of the time, you want to initialize the weights and biases at creation time.  However, you don't want to do this when JAX reconstructs the object from it's flattened representation.  You probably just want to plop the parameters right into a freshly constructed object. The `build` attribute gives you some control over this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-speech",
   "metadata": {},
   "source": [
    "### `Function` Layer\n",
    "\n",
    "The `Function` class fills the same need that `Lambda` layers do in Keras: being able to conveniently plug functions into models.  As the `tree_flatten` method shows, classes registered as pytrees can be parameter-free. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "educational-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Function:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Function({self.fn.__name__})'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [], self.fn\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(aux_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-bunny",
   "metadata": {},
   "source": [
    "### Helpful Functions\n",
    "\n",
    "Here are a few functions that will be *lifted* to `Function` layer in the `Sequential` model.  One common approach for improving classification accuracy is to normalize your input data.  When working with gray-scale images, this typically means rescaling the pixels from $[0,255]$ to $[0,1]$.  This is what `rescale_image` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "rolled-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_image(x): return x / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-recall",
   "metadata": {},
   "source": [
    "The model built in this post operates on batches of two-dimensional gray-scale images.  Each batch is a three-dimensional array and can be interpreted as a vertical stack of 2D images, where the height of the vertical stack is the number of images.  The `flatten` goes through each slice of the vertical stack and transforms the 2D array into a one-dimensional array.  In the process, the 3D input becomes a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "obvious-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    shape = jnp.shape(x)\n",
    "    assert len(shape) == 3, 'x must represent a batch of two-dimensional gray-scale images' \n",
    "    batch_size = shape[0]\n",
    "    return jnp.reshape(x, (batch_size, -1)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "sharp-worst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "identified-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.clip(x, a_min=0)    \n",
    "\n",
    "x = np.random.randn(10,10)\n",
    "assert np.all(np.isclose(relu(x), tf.nn.relu(x))), 'test failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-relationship",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "anticipated-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = jnp.exp(x)\n",
    "    return ex / jnp.sum(ex, axis=-1, keepdims=True)    \n",
    "\n",
    "x = np.random.randn(5, 10)\n",
    "assert np.all(np.isclose(softmax(x), tf.nn.softmax(x))), 'test failed'\n",
    "assert np.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-despite",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "raising-clerk",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "A `Sequential` model is a list of function-like each objects.  Looking at the `__init__` method below, each item in the list of layers must be a subclass of `Parametric` or `Function`, or be `callable`.  Note that the order is important here because any object that implements a `__call__` method is by definition `callable`.  The other methods defined in the `Sequential` class definition are pretty straight-forward and consist of simply looping over the list of layers and calling that particular method (and possibly appending results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "neural-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Sequential(Parametric):\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'tree_flatten'):\n",
    "            #if isinstance(layer, Parametric) or isinstance(layer, Function):\n",
    "                self.layers.append(layer)\n",
    "            elif callable(layer):\n",
    "                self.layers.append(Function(layer))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for layer in self.layers:\n",
    "            string += (repr(layer) + '\\n')\n",
    "        return string\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        aux_data, children = [], []\n",
    "        for layer in self.layers:\n",
    "            params, extra_stuff = jax.tree_flatten(layer)\n",
    "            aux_data.append(extra_stuff)\n",
    "            children.append(params)\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layers = []\n",
    "        for params, spec in zip(children, aux_data):\n",
    "            layers.append(jax.tree_unflatten(spec, params))\n",
    "        return Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "welcome-yield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(rescale_image)\n",
      "Function(flatten)\n",
      "Linear(num_inputs=784, num_outputs=128)\n",
      "Function(relu)\n",
      "Linear(num_inputs=128, num_outputs=10)\n",
      "Function(softmax)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fashion_mnist_mlp():\n",
    "    return Sequential(\n",
    "        rescale_image,\n",
    "        flatten,\n",
    "        Linear(784, 128),\n",
    "        relu,\n",
    "        Linear(128, 10),\n",
    "        softmax  \n",
    "    )\n",
    "\n",
    "model = fashion_mnist_mlp()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-camera",
   "metadata": {},
   "source": [
    "## The Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "identical-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, probs):\n",
    "    batch_size, _ = jnp.shape(probs)\n",
    "    return -jnp.sum(jnp.log(probs + 1.0e-16) * y_true) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "disabled-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "assert np.all(np.isclose(cross_entropy_loss(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-shadow",
   "metadata": {},
   "source": [
    "## The Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "dominant-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "elder-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.lr = lr \n",
    "    def step(self, model, grads):\n",
    "        return jax.tree_map(lambda p, g: p - self.lr*g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "chemical-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7):\n",
    "        self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps\n",
    "        self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) \n",
    "        self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model)\n",
    "        self.k = 0 \n",
    "    def step(self, model, grads):\n",
    "        lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps\n",
    "        v, s = self.v, self.s\n",
    "        k = self.k = self.k+1\n",
    "        self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grads)\n",
    "        self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grads)\n",
    "        v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v)\n",
    "        s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s)\n",
    "        new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat)\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-internship",
   "metadata": {},
   "source": [
    "## The Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "herbal-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(percentage, total=30):\n",
    "    x = int(percentage*total)\n",
    "    if x < total:\n",
    "        r = '[' + ''.join(['=']*x) + '>' + ''.join(['.']*(total-x)) + ']' \n",
    "    else:\n",
    "        r = '[' + ''.join(['=']*(total+1)) + ']' \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-profile",
   "metadata": {},
   "source": [
    "## The Training Loop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "indian-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_datasource, valid_datasource, optimizer, loss_fn, grad_fn, model):\n",
    "    history = {'loss':[], 'accuracy':[]}\n",
    "    \n",
    "    if valid_datasource is not None:\n",
    "        history = {**history, 'valid_loss': [], 'valid_accuracy': []}\n",
    "\n",
    "    train_num_batches = len(train_datasource)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # TRAINING PHASE\n",
    "        train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0\n",
    "        \n",
    "        num_steps = 0 \n",
    "        \n",
    "        epoch_duration = 0.0\n",
    "        \n",
    "        # we know how many batches there are ... keep track\n",
    "        for i, (X_train, y_train) in enumerate(train_datasource):\n",
    "            \n",
    "            # logging\n",
    "            batch_start = time.time()\n",
    "\n",
    "            num_steps += 1\n",
    "            \n",
    "            # training loss and gradients for this particular batch\n",
    "            probs = model(X_train)\n",
    "            loss = loss_fn(probs, y_train)\n",
    "            \n",
    "            grads = grad_fn(model, X_train, y_train)\n",
    "            model = optimizer.step(model, grads)\n",
    "            \n",
    "            \n",
    "            # Results aggregation\n",
    "            num_correct = jnp.sum(jnp.argmax(probs, axis=-1) == y_train)\n",
    "            train_loss_accum += loss \n",
    "            train_batch_size += len(y_train)\n",
    "            train_accuracy_accum += num_correct\n",
    "            train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "\n",
    "            # Logging ....\n",
    "            batch_duration = time.time() - batch_start\n",
    "            epoch_duration += batch_duration \n",
    "            log_batch_count = f'{i}/{train_num_batches}'\n",
    "            log_epoch_time = f'{int(epoch_duration)}s'\n",
    "            log_batch_time = f'{1_000*batch_duration:.2f}ms/batch'\n",
    "            log_batch_loss = f'loss: {train_loss:.4f}'\n",
    "            log_batch_accuracy = f'accuracy: {train_accuracy:.4f}'\n",
    "            log_string =  f'{log_batch_count:<10s} {progress_bar((i+1)/train_num_batches)} - {log_epoch_time:<3s} {log_batch_time:<5s} - {log_batch_loss:<13s} - {log_batch_accuracy:<20s}'\n",
    "            print(log_string, end='\\r') \n",
    "\n",
    "        # \n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_accuracy)      \n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        if valid_datasource is not None:\n",
    "            valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 \n",
    "\n",
    "            # Run validation step ...\n",
    "            for i, (X_valid, y_valid) in enumerate(valid_datasource):\n",
    "                num_steps += 1\n",
    "                probs = model(X_valid)\n",
    "                loss = loss_fn(probs, y_valid)\n",
    "                \n",
    "                valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid)\n",
    "\n",
    "                valid_loss_accum += loss\n",
    "                valid_batch_size += len(y_valid)\n",
    "\n",
    "            epoch_valid_loss = valid_loss_accum / valid_batch_size \n",
    "            epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size\n",
    "\n",
    "            history['loss'].append(epoch_valid_loss)\n",
    "            history['accuracy'].append(epoch_valid_accuracy)\n",
    "        \n",
    "        # this log_string should include validation results\n",
    "        print(log_string, end='\\n')\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "transsexual-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def fashion_mnist_loss(probs, y_true, num_classes=10):\n",
    "    # average cross entropy, batch\n",
    "    y_one_hot = jax.nn.one_hot(y_true, num_classes)\n",
    "    return -jnp.sum(jnp.log(probs) * y_one_hot) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "eight-italian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1874/1875  [===============================] - 3s  1.57ms/batch - loss: 0.4958  - accuracy: 0.8245      \n",
      "Epoch 2/5\n",
      "1874/1875  [===============================] - 2s  1.50ms/batch - loss: 0.3725  - accuracy: 0.8654    \n",
      "Epoch 3/5\n",
      "1874/1875  [===============================] - 3s  1.67ms/batch - loss: 0.3339  - accuracy: 0.8778    \n",
      "Epoch 4/5\n",
      "1874/1875  [===============================] - 3s  1.85ms/batch - loss: 0.3080  - accuracy: 0.8869    \n",
      "Epoch 5/5\n",
      "1874/1875  [===============================] - 3s  1.95ms/batch - loss: 0.2900  - accuracy: 0.8934    \n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: fashion_mnist_loss(model(X), y)))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=None,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-algeria",
   "metadata": {},
   "source": [
    "## Keras Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "unlike-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 793us/step - loss: 0.4952 - accuracy: 0.8253\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 803us/step - loss: 0.3735 - accuracy: 0.8652\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 807us/step - loss: 0.3321 - accuracy: 0.8799\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 920us/step - loss: 0.3088 - accuracy: 0.8855\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 943us/step - loss: 0.2916 - accuracy: 0.8928\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1/255.0),\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-playing",
   "metadata": {},
   "source": [
    "## Next Time\n",
    "\n",
    "1. Implement convolutional layers and train a convolutional neural network\n",
    "2. Add a callback system to simplify the training loop\n",
    "3. Show validation results\n",
    "4. Evaluate performance on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-victorian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
