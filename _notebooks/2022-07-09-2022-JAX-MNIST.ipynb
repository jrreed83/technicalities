{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "meaning-johnson",
   "metadata": {},
   "source": [
    "# Fashion MNIST using Linear Layers with JAX\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-capital",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this article, I'm going to start implementing a JAX-based neural network library, and use it to synthesize a Fashion-MNIST classifier. Features will be added over the coming weeks and months as I tackle more advanced topics and architectures. I don't intend to write a full-featured framework. My goal is to engineer some components that I can mix-and-match (and easily customize) to solve problems that interest me.  Except for basic linear algebra and automatic differentiation, I'll try to implement everything from scratch and use Tensorflow/Keras to validate my implementation.  The reason I'm choosing to test against Tensorflow is that I have more experience working with it (plus it's very easy to throw something together in Keras).  I'll be sure to give credit to any design ideas and techniques I use in the process of building out functionality.  \n",
    "\n",
    "\n",
    "## JAX \n",
    "\n",
    "JAX is an array-processing library that uses Google's XLA (Accelerated Linear Algebra) compiler to generate highly performance code that can run on a variety of hardware platforms.  It feels a lot like numpy, with a number of advantages including built in automatic differentiation, parallelization, and just-in-time compilation.  It's built in array type is called a `DeviceArray`.  Unlike numpy's `ndarray` type though, elements of `DeviceArray`s cannot be directly mutated.  \n",
    "\n",
    "The other fundamental datatype in JAX is the *pytree*, which (not surprisingly given the name) is a tree of python objects.  A lot of the magic and simplicity of JAX comes from working with pytrees.  More on this later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-calculation",
   "metadata": {},
   "source": [
    "## The Goal\n",
    "\n",
    "Here is an example of how you could go about building a basic Fashion MNIST classifier in Keras.  The API design is very elegant and easy to \n",
    "understand.  I even like the logging information provided by the call to `model.fit`, so much in fact that I'm going to replicate the style in\n",
    "my training loop.\n",
    "\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1/255),\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5)\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4955 - accuracy: 0.8250\n",
    "Epoch 2/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3726 - accuracy: 0.8643\n",
    "Epoch 3/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3354 - accuracy: 0.8781\n",
    "Epoch 4/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3136 - accuracy: 0.8845\n",
    "Epoch 5/5\n",
    "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2937 - accuracy: 0.8927\n",
    "```\n",
    "\n",
    "\n",
    "Here's what I'll be able to do by the end of this post.  Okay, it's nowhere near as nice\n",
    "as the Keras API at this point.  Part of that comes from the fact that Keras tries to hide\n",
    "some of the details from you, while I'm definitely not.  \n",
    "\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: fashion_mnist_loss(model(X), y)))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=None,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1874/1875  [===============================] - 3s  1.49ms/batch  -  loss: 0.4958   -  accuracy: 0.8245      \n",
    "Epoch 2/5\n",
    "1874/1875  [===============================] - 2s  1.40ms/batch  -  loss: 0.3725   -  accuracy: 0.8654    \n",
    "Epoch 3/5\n",
    "1874/1875  [===============================] - 2s  1.53ms/batch  -  loss: 0.3339   -  accuracy: 0.8778    \n",
    "Epoch 4/5\n",
    "1874/1875  [===============================] - 2s  1.55ms/batch  -  loss: 0.3080   -  accuracy: 0.8869    \n",
    "Epoch 5/5\n",
    "1874/1875  [===============================] - 2s  1.46ms/batch  -  loss: 0.2900   -  accuracy: 0.8934\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-forward",
   "metadata": {},
   "source": [
    "## Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "friendly-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-enlargement",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "I've looked at enough Pytorch and fast.ai code to realize that the `Dataset` and `Dataloader` approach to batching data are really convenient to work with.  In Pytorch, a `Dataset` is much more general that what I have here.  In fact, because the data returned by `tf.keras.datasets.fashion_mnist.load_data()` is just a few numpy arrays, the `Dataset` abstraction is pretty useless right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "flexible-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return jnp.shape(self.X)[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-religion",
   "metadata": {},
   "source": [
    "The `Dataloader` is responsible for slicing the passed in dataset into a collection of batches that can be used for training, validating, testing, or anything else you can think of.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "eligible-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, dataset: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-notification",
   "metadata": {},
   "source": [
    "By design, `Dataloader`s are lazy, meaning that batches are only constructed and returned when you ask for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "chemical-preference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       " array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9, 1, 0, 6, 4, 3, 1,\n",
       "        4, 8, 4, 3, 0, 2, 4, 4, 5, 3], dtype=uint8))"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), _ = tf.keras.datasets.fashion_mnist.load_data()\n",
    "dataset = Dataset(X_train, y_train)\n",
    "dataloader = Dataloader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "announced-andorra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "fuzzy-familiar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "foreign-opposition",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "In this section, I'll describe each component of the `Sequential` model being \n",
    "\n",
    "```python\n",
    "model = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(784, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-employment",
   "metadata": {},
   "source": [
    "### Linear Layers\n",
    "\n",
    "Like the `Treex` and `Equinox` neural network libraries, I interpret each layer as a parametric function.  A parametric function is just like an ordinary function, except that it's behavior is governed by a set of parameters.  In the case of a linear layer in a neural network, these parameters correspond to the weights and biases that are updated in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "colored-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parametric: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-white",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "monthly-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Linear(Parametric):\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    ni: int \n",
    "    no: int \n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, build=True, seed=1234):\n",
    "        self.ni = num_inputs \n",
    "        self.no = num_outputs \n",
    "        # want to add seed as internal object\n",
    "        if build:\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs)\n",
    "            self.b = jnp.zeros(num_outputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(num_inputs={self.ni}, num_outputs={self.no})'\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "    def tree_flatten(self):\n",
    "        return (self.w, self.b), (self.ni, self.no)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layer = cls(*aux_data, build=False)\n",
    "        layer.w, layer.b = children\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-fishing",
   "metadata": {},
   "source": [
    "### Functional Layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "impaired-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Function:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Function({self.fn.__name__})'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [None], self.fn\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(aux_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-priest",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "fancy-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    shape = jnp.shape(x)\n",
    "    if len(shape) == 2:\n",
    "        # flatten a single 2D image\n",
    "        return jnp.reshape(x, -1)\n",
    "    elif len(shape) == 3:\n",
    "        # x is a batch of 2D images, flatten each image \n",
    "        batch_size = jnp.shape(x)[0]\n",
    "        return jnp.reshape(x, (batch_size, -1)) \n",
    "    else:\n",
    "        raise Exception(\n",
    "            f'At the moment you can only pass 2D or 3D arrays to flatten, you passed a {len(shape)}D array' \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-pension",
   "metadata": {},
   "source": [
    "### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "clean-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.clip(x, a_min=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "periodic-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10,10)\n",
    "assert np.all(np.isclose(relu(x), tf.nn.relu(x))), 'test failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-location",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "variable-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = jnp.exp(x)\n",
    "    return ex / jnp.sum(ex, axis=-1, keepdims=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "surrounded-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(5, 10)\n",
    "assert np.all(np.isclose(softmax(x), tf.nn.softmax(x))), 'test failed'\n",
    "assert np.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-notification",
   "metadata": {},
   "source": [
    "### Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "aggressive-robin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable(Linear(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "average-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_image(x): return x / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-fraction",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "A `Sequential` model is a list of function-like each objects.  Looking at the `__init__` method below, each item in the list of layers must be a subclass of `Parametric` or `Function`, or be `callable`.  Note that the order is important here because any object that implements a `__call__` method is by definition `callable`.  The other methods defined in the `Sequential` class definition are pretty straight-forward and consist of simply looping over the list of layers and calling that particular method (and possibly appending results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "scenic-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Sequential(Parametric):\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, Parametric) or isinstance(layer, Function):\n",
    "                self.layers.append(layer)\n",
    "            elif callable(layer):\n",
    "                self.layers.append(Function(layer))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for layer in self.layers:\n",
    "            string += (repr(layer) + '\\n')\n",
    "        return string\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        aux_data, children = [], []\n",
    "        for layer in self.layers:\n",
    "            params, extra_stuff = jax.tree_flatten(layer)\n",
    "            aux_data.append(extra_stuff)\n",
    "            children.append(params)\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layers = []\n",
    "        for params, spec in zip(children, aux_data):\n",
    "            layers.append(jax.tree_unflatten(spec, params))\n",
    "        return Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "chemical-resident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(rescale_image)\n",
      "Function(flatten)\n",
      "Linear(num_inputs=784, num_outputs=128)\n",
      "Function(relu)\n",
      "Linear(num_inputs=128, num_outputs=10)\n",
      "Function(softmax)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fashion_mnist_mlp():\n",
    "    return Sequential(\n",
    "        rescale_image,\n",
    "        flatten,\n",
    "        Linear(784, 128),\n",
    "        relu,\n",
    "        Linear(128, 10),\n",
    "        softmax  \n",
    "    )\n",
    "\n",
    "model = fashion_mnist_mlp()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-monaco",
   "metadata": {},
   "source": [
    "## The Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "southern-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, probs):\n",
    "    batch_size, _ = jnp.shape(probs)\n",
    "    return -jnp.sum(jnp.log(probs + 1.0e-16) * y_true) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "tamil-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "assert np.all(np.isclose(cross_entropy_loss(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-warning",
   "metadata": {},
   "source": [
    "## The Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "recent-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "single-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.lr = lr \n",
    "    def step(self, model, grads):\n",
    "        return jax.tree_map(lambda p, g: p - self.lr*g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "french-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7):\n",
    "        self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps\n",
    "        self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) \n",
    "        self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model)\n",
    "        self.k = 0 \n",
    "    def step(self, model, grads):\n",
    "        lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps\n",
    "        v, s = self.v, self.s\n",
    "        k = self.k = self.k+1\n",
    "        self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grads)\n",
    "        self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grads)\n",
    "        v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v)\n",
    "        s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s)\n",
    "        new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat)\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-stadium",
   "metadata": {},
   "source": [
    "## The Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "raising-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(percentage, total=30):\n",
    "    x = int(percentage*total)\n",
    "    if x < total:\n",
    "        r = '[' + ''.join(['=']*x) + '>' + ''.join(['.']*(total-x)) + ']' \n",
    "    else:\n",
    "        r = '[' + ''.join(['=']*(total+1)) + ']' \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-count",
   "metadata": {},
   "source": [
    "## The Training Loop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "matched-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_datasource, valid_datasource, optimizer, loss_fn, grad_fn, model):\n",
    "    history = {'loss':[], 'accuracy':[]}\n",
    "    \n",
    "    if valid_datasource is not None:\n",
    "        history = {**history, 'valid_loss': [], 'valid_accuracy': []}\n",
    "\n",
    "    train_num_batches = len(train_datasource)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # TRAINING PHASE\n",
    "        train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0\n",
    "        \n",
    "        num_steps = 0 \n",
    "        \n",
    "        epoch_duration = 0.0\n",
    "        \n",
    "        # we know how many batches there are ... keep track\n",
    "        #train_loss_per_batch = []\n",
    "        #train_accuracy_per_batch = []\n",
    "        for i, (X_train, y_train) in enumerate(train_datasource):\n",
    "            \n",
    "            # logging\n",
    "            batch_start = time.time()\n",
    "\n",
    "            num_steps += 1\n",
    "            \n",
    "            # training loss and gradients for this particular batch\n",
    "            probs = model(X_train)\n",
    "            loss = loss_fn(probs, y_train)\n",
    "            \n",
    "            grads = grad_fn(model, X_train, y_train)\n",
    "            model = optimizer.step(model, grads)\n",
    "            \n",
    "            \n",
    "            # Results aggregation\n",
    "            num_correct = jnp.sum(jnp.argmax(probs, axis=-1) == y_train)\n",
    "            train_loss_accum += loss \n",
    "            train_batch_size += len(y_train)\n",
    "            train_accuracy_accum += num_correct\n",
    "            train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "\n",
    "            # Logging ....\n",
    "            batch_duration = time.time() - batch_start\n",
    "            epoch_duration += batch_duration \n",
    "            log_batch_count = f'{i}/{train_num_batches}'\n",
    "            log_epoch_time = f'{int(epoch_duration)}s'\n",
    "            log_batch_time = f'{1_000*batch_duration:.2f}ms/batch'\n",
    "            log_batch_loss = f'loss: {train_loss:.4f}'\n",
    "            log_batch_accuracy = f'accuracy: {train_accuracy:.4f}'\n",
    "            log_string =  f'{log_batch_count:<10s} {progress_bar((i+1)/train_num_batches)} - {log_epoch_time:<3s} {log_batch_time:<5s} - {log_batch_loss:<13s} - {log_batch_accuracy:<20s}'\n",
    "            print(log_string, end='\\r') \n",
    "\n",
    "        # \n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_accuracy)      \n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        if valid_datasource is not None:\n",
    "            valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 \n",
    "\n",
    "            # Run validation step ...\n",
    "            for i, (X_valid, y_valid) in enumerate(valid_datasource):\n",
    "                num_steps += 1\n",
    "                probs = model(X_valid)\n",
    "                loss = loss_fn(probs, y_valid)\n",
    "                \n",
    "                valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid)\n",
    "\n",
    "                valid_loss_accum += loss\n",
    "                valid_batch_size += len(y_valid)\n",
    "\n",
    "            epoch_valid_loss = valid_loss_accum / valid_batch_size \n",
    "            epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size\n",
    "\n",
    "            history['loss'].append(epoch_valid_loss)\n",
    "            history['accuracy'].append(epoch_valid_accuracy)\n",
    "        \n",
    "        # this log_string should include validation results\n",
    "        print(log_string, end='\\n')\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "sudden-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def fashion_mnist_loss(probs, y_true, num_classes=10):\n",
    "    # average cross entropy, batch\n",
    "    y_one_hot = jax.nn.one_hot(y_true, num_classes)\n",
    "    return -jnp.sum(jnp.log(probs) * y_one_hot) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "smooth-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1874/1875  [===============================] - 3s  1.49ms/batch  -  loss: 0.4958   -  accuracy: 0.8245      \n",
      "Epoch 2/5\n",
      "1874/1875  [===============================] - 2s  1.40ms/batch  -  loss: 0.3725   -  accuracy: 0.8654    \n",
      "Epoch 3/5\n",
      "1874/1875  [===============================] - 2s  1.53ms/batch  -  loss: 0.3339   -  accuracy: 0.8778    \n",
      "Epoch 4/5\n",
      "1874/1875  [===============================] - 2s  1.55ms/batch  -  loss: 0.3080   -  accuracy: 0.8869    \n",
      "Epoch 5/5\n",
      "1874/1875  [===============================] - 2s  1.46ms/batch  -  loss: 0.2900   -  accuracy: 0.8934    \n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=64)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: fashion_mnist_loss(model(X), y)))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=None,\n",
    "    optimizer=Adam(model, lr=1e-3), \n",
    "    loss_fn=fashion_mnist_loss, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-scholar",
   "metadata": {},
   "source": [
    "## Keras Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "lesbian-volume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 793us/step - loss: 0.4952 - accuracy: 0.8253\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 803us/step - loss: 0.3735 - accuracy: 0.8652\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 807us/step - loss: 0.3321 - accuracy: 0.8799\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 920us/step - loss: 0.3088 - accuracy: 0.8855\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 943us/step - loss: 0.2916 - accuracy: 0.8928\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1/255.0),\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-drama",
   "metadata": {},
   "source": [
    "## Next Time\n",
    "\n",
    "1. Implement convolutional layers and train a convolutional neural network\n",
    "2. Add a callback system to simplify the training loop\n",
    "3. Show validation results\n",
    "4. Evaluate performance on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-oxygen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
