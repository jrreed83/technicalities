{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "given-render",
   "metadata": {},
   "source": [
    "# Two Dimensional Convolutional Layers\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [jax, convolution, pooling]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-traffic",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post, I'll start by implementing a basic convolutional layer using numpy and validate it against Keras.  After this, I'll write a more efficient one using JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-christian",
   "metadata": {},
   "source": [
    "## How Convolutional Layers work\n",
    "\n",
    "A two dimensional convolutional layer consists of several randomly initialized filters, biases, and a rule for moving the filter across the input array.  Typically, the input array is rank 4, meaning that the shape has 4 components.  One of these components represents the number of individual images in the input,  two of them tell you the size of each of the images, and the fourth tells you the number of *input channels*.  Interpret the number of channels as the number of components describing a pixel.  For instance, a pixel in a gray-scale image is a number between 0 and 255 and therefore has one channel.  However, an RGB color image has 3 channels because it has a red component, green component, and blue component.\n",
    "\n",
    "The number of filters and biases in the layer tell you the number of channels each image will have after application.  When definining a layer, you generally specify how many output channels, or filters, you want.  In Keras, it's the number of filters; in Pytorch it's the number of output channels.  Either way, the layer eventually generates one rank 3 filter and 1 scalar bias per output channel.  The   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-crash",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "For now, I only need numpy and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "double-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-folder",
   "metadata": {},
   "source": [
    "## Implementation from First Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-thriller",
   "metadata": {},
   "source": [
    "This function filters a single image with every output filter, resulting in a rank 3 array.  The first two levels of the nested loop extract a rank 3 chunk from `image`, while the third level of the nested loop applies each output filter to the chunk.  After a chunk is processed and the results placed in the output array `y`, the filter shape and stride step to the next chunk position.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "related-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image(image, filters, strides):\n",
    "    \n",
    "    xm, xn, _  = image.shape \n",
    "    \n",
    "    km, kn, ni, no = filters.shape \n",
    "    \n",
    "    \n",
    "    sm, sn = strides\n",
    "    #ym, yn = 1 + ((xm - km + 1)//sm), 1 + ((xn - kn + 1)//sn)\n",
    "    ym, yn = 1 + ((xm - km)//sm), 1 + ((xn - kn)//sn) \n",
    "    y = np.zeros((ym, yn, no))\n",
    "\n",
    "    for iy, ix in enumerate(range(0, xm-km+1, sm)):\n",
    "        for jy, jx in enumerate(range(0, xn-kn+1, sn)):\n",
    "            # Apply each output filter and bias term to this chunk\n",
    "            chunk = image[ix:ix+km,jx:jx+kn,:]\n",
    "            for channel in range(no):\n",
    "                y[iy,jy,channel] = np.sum(filters[:,:,:,channel] * chunk)\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-faculty",
   "metadata": {},
   "source": [
    "Once we have an algorithm to filter a single image, it can easily be extended to a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "italic-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image_batch(batch, filters, biases, strides):\n",
    "    outputs = [filter_image(image, filters, strides) for image in batch]\n",
    "    outputs = np.array(outputs)\n",
    "    outputs = outputs + biases\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-kansas",
   "metadata": {},
   "source": [
    "First, a list comprehension applies the `filter_image` function defined above, to each image in the batch.  Next, the list returned by the \n",
    "list comprehension, is converted to a rank 4 array with the `np.array` function.  The line preceding the `return` statement, \n",
    "```python\n",
    "outputs = outputs + biases\n",
    "```\n",
    "seems like it shouldn't work, because the ranks don't match.  Fortunately,  numpy's broadcasting rules come to the rescue and does what we want.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-jurisdiction",
   "metadata": {},
   "source": [
    "## Compare to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-stocks",
   "metadata": {},
   "source": [
    "To compare the numpy version to Keras, I'm going to create a `Conv2D` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "super-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_keras = tf.keras.layers.Conv2D(\n",
    "    filters=4, \n",
    "    kernel_size=(4, 4), \n",
    "    strides=(1,1), \n",
    "    bias_initializer='he_uniform', \n",
    "    padding='valid'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-quilt",
   "metadata": {},
   "source": [
    "initialize a random batch of fakey images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "sudden-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = np.random.randn(2,28,28,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-cruise",
   "metadata": {},
   "source": [
    "and filter the batch with the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "sensitive-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_batch_keras = layer_keras(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-mandate",
   "metadata": {},
   "source": [
    "Next, the filters, biases and strides are extracted from the layer.  Note that `strides` doesn't really need to be accessed from the layer, it's in the `Conv2D` constructor after all.  The way I did it here is just less error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "architectural-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, biases = layer_keras.get_weights()\n",
    "strides = layer_keras.strides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-decade",
   "metadata": {},
   "source": [
    "Now all the inputs can be passed to the `filter_image_batch` implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "finished-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_batch_numpy = filter_image_batch(input_batch, filters, biases, strides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-emphasis",
   "metadata": {},
   "source": [
    "To check that the Keras output and numpy outputs are approximately equal, I make sure that the absolute error is below a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "medical-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.max(np.abs(output_batch_keras - output_batch_numpy)) < 1e-6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-cruise",
   "metadata": {},
   "source": [
    "Although the outputs are about the same, the Keras version runs much faster, as the following benchmarks show.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "sunset-mechanism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 ms ± 888 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "filter_image_batch(input_batch, filters, biases, strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "alpha-joint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 µs ± 683 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "layer_keras(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-denver",
   "metadata": {},
   "source": [
    "I can't say that these timing results are a surprise.  Remember how my numpy version of convolution has a three-level nested loop?  Well, this leads to very poor performance.  Unfortunately, to write fast python programs, a lot of its syntax and functionality (like loops) must be avoided in favor of calling wrappers for optimized C code.  This is precisely what Keras does.  Later on, we'll see that the JAX version is competitive with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-frame",
   "metadata": {},
   "source": [
    "### Convolutional Layer in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "cardiac-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Tuple, List\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-reproduction",
   "metadata": {},
   "source": [
    "In addition to the standard set of imports, I'm also importing `patch` from [fastcore](https://fastcore.fast.ai/).  It's selling point is that it contains\n",
    "\n",
    "> Python goodies to make your coding faster, easier, and more maintainable.\n",
    "\n",
    "The nice thing about `patch`, is that it allows you to write methods outside of class definitions.  This is particularly useful if you're interested in incrementally developing, and explaining, class functionality in notebooks.  Without `patch`, you'd have to either have an entire class implementation in a single cell, or abandon classes altogether and use functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "afraid-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D: \n",
    "    filters: jnp.ndarray \n",
    "    biases: jnp.ndarray\n",
    "    input_channels: int \n",
    "    output_channels: int \n",
    "    filter_shape: Tuple[int,int]\n",
    "    strides: Tuple[int,int]\n",
    "    padding: str\n",
    "    seed: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-helping",
   "metadata": {},
   "source": [
    "#### Constructor\n",
    "\n",
    "The constructor is pretty self explanatory.  Like the layers implemented in the previous post, the `build` parameter determines whether or not the filters and biases get initialized.  For now, the filters and biases follow a kaiming-uniform distribution.  This is the default initializer in Pytorch, so I figured it would be effective.  Can't say I know why at this point, but Keras has a different default initialization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "fancy-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def __init__(\n",
    "    self: Conv2D, \n",
    "    input_channels, \n",
    "    output_channels, \n",
    "    filter_shape=(2,2), \n",
    "    strides=(1,1), \n",
    "    padding='valid', \n",
    "    seed=1234, \n",
    "    build=True):\n",
    "    \n",
    "    self.input_channels = input_channels\n",
    "    self.output_channels = output_channels\n",
    "    self.filter_shape = filter_shape\n",
    "    self.strides = strides \n",
    "    self.padding = padding\n",
    "    self.seed = seed\n",
    "        \n",
    "    if build:\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        fkey, bkey = jax.random.split(key)\n",
    "            \n",
    "        # kaiming/he uniform, using Pytorch documentation\n",
    "        K = 1 / (input_channels * filter_shape[0] * filter_shape[1])\n",
    "        sqrtK = jnp.sqrt(K)\n",
    "        self.filters = jax.random.uniform(\n",
    "            fkey, \n",
    "            (*filter_shape, input_channels, output_channels), \n",
    "            minval=-sqrtK, maxval=+sqrtK\n",
    "        )\n",
    "        \n",
    "        self.biases = jax.random.uniform(\n",
    "            bkey, \n",
    "            (output_channels,), \n",
    "            minval=-sqrtK, maxval=+sqrtK\n",
    "        )   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-harvey",
   "metadata": {},
   "source": [
    "#### `__call__` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-notebook",
   "metadata": {},
   "source": [
    "To implement `__call__` we use the JAX builtin function `conv_general_dilated`.  Except for the `dimension_numbers` argument, it's pretty easy to figure out what it's doing (but I'm still not clear on how it works - maybe save that for another post).  Like the Keras `Conv2D` layer, it has additional input arguments that give you further control over the convolution.  I'm not including these additional arguments here because I'm trying to keep things as simple as possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "specific-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def __call__(self: Conv2D, batch: jnp.ndarray):\n",
    "    outputs = jax.lax.conv_general_dilated(\n",
    "        lhs=batch,\n",
    "        rhs=self.filters,\n",
    "        window_strides=self.strides,\n",
    "        padding=self.padding,\n",
    "        dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    "    )   \n",
    "    \n",
    "    # This uses the broadcasting rules.\n",
    "    outputs +=  biases\n",
    "        \n",
    "    # Need to add biases...\n",
    "    return outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-hurricane",
   "metadata": {},
   "source": [
    "The `dimension_numbers` field is a three element tuple describing the shape of the input batch, the filters, and the output batch respectively.  We've adopted the default Keras layout, which means that for an input batch of images, the batch dimension is listed first, the image height second, the image width third, and the number of input channels fourth.  The dimension number for this is represented as `'NHWC'`.  \n",
    "\n",
    "By default, the filters are arranged in a similar way although there is no batch dimension: the filter height comes first, the filter width second, the input channel count third, and the output channel count last.  The description number for this is `'HWIO'`.\n",
    "\n",
    "Because `conv_general_dilatated` does not work with the biases, they must be added in separately.  Like numpy, JAX has broadcasting rules that make this mixed-rank addition work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-video",
   "metadata": {},
   "source": [
    "#### Adding to pytree Registry\n",
    "\n",
    "Like the `Linear` and `Function` layers defined in my last post, `tree_flatten` and `tree_unflatten` methods must be defined.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "configured-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def tree_flatten(self: Conv2D) -> Tuple[List[jnp.ndarray], dict]:\n",
    "    params = (self.filters, self.biases)\n",
    "    metadata = {k: v for k, v in self.__dict__.items() if k not in {'biases', 'filters'}}\n",
    "    return params, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "useful-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(cls_method=True)\n",
    "def tree_unflatten(cls: Conv2D, metadata: dict, params:List[jnp.ndarray]):\n",
    "    # This assumes that each key-value pair in the metadata dict corresponds to a constructor argument.\n",
    "    layer = cls(**metadata, build=False)\n",
    "    layer.filters, layer.biases = params\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-dream",
   "metadata": {},
   "source": [
    "You'll notice that I'm trying to be a little more generic here.  Rather than use a tuple to store the layer's metadata, I'm using a dictionary that contains every data attribute, except the biases and filters.  Because these attributes correspond to arguments to the constructor, `metadata` is passed to `Conv2D`'s constructor in `tree_unflatten`.  \n",
    "\n",
    "Now that JAX knows how to flatten and unflatten a `Conv2D` layer, it can formally be can be added to the pytree registry: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "contrary-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = jax.tree_util.register_pytree_node_class(Conv2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-shark",
   "metadata": {},
   "source": [
    "Last time, `register_pytree_node_class` was used as a class decorator.  We can get away with using it as a function because that's all decorators are: special types of functions.  The reason the decorator approarch could not be used here is that the cell with the class definition didn't include the `tree_flatten` and `tree_unflatten` methods.  If you add the decorator and run the cell, JAX will complain and point out that these methods are not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "robust-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_batch_jax = layer_jax(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-spell",
   "metadata": {},
   "source": [
    "## Compare JAX and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-liquid",
   "metadata": {},
   "source": [
    "To compare the JAX and Keras convolutional layer implementations, the JAX `Conv2D` layer is initialized with data from the Keras layer, and applied to the same input batch that was used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "furnished-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_jax = Conv2D(\n",
    "    input_channels=layer_keras.input_spec.axes[-1],\n",
    "    output_channels=layer_keras.filters,\n",
    "    filter_shape=layer_keras.kernel_size,\n",
    "    strides=layer_keras.strides,\n",
    "    padding=layer_keras.padding\n",
    ")\n",
    "\n",
    "layer_jax.filters, layer_jax.biases = layer_keras.get_weights()\n",
    "\n",
    "output_batch_jax = layer_jax(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-democracy",
   "metadata": {},
   "source": [
    "Because the following assertion passes, we can be reassured that both layers are calculating the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "precise-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.max(np.abs(output_batch_keras - output_batch_jax)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-massachusetts",
   "metadata": {},
   "source": [
    "What about the calculation time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "intense-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 µs ± 8.95 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "layer_keras(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "delayed-victim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 µs ± 5.55 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "layer_jax(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-orange",
   "metadata": {},
   "source": [
    "Not too bad, the JAX version only about 100 microseconds slower than Keras.  Fortunately, the JAX version can go even faster after applying the `jax.jit` transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "criminal-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_jax_jitted = jax.jit(layer_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "focused-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 µs ± 1.73 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "layer_jax_jitted(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-parallel",
   "metadata": {},
   "source": [
    "Now it's faster than Keras.  I realize you can't trust micro-benchmarks, but it's good to know that with respect to performance, JAX and Keras seem to be in the same ballpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-idaho",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-gates",
   "metadata": {},
   "source": [
    "In the post, I implemented the mechanics of a 2D convolutional layer in numpy, from first principles.  After this was proven to be very inefficient compare to Keras, I built a simple layer in JAX and showed that it performed as well as the Keras version on a sample input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-guest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
