{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broken-forwarding",
   "metadata": {},
   "source": [
    "# Convolutional Layers\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [jax, convolution, pooling]\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-young",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post, I'll start by implementing a basic convolutional layer using numpy and validate it against Keras.  After this, I'll write a more efficient one using JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-carnival",
   "metadata": {},
   "source": [
    "## How Convolutional Layers work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-retirement",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "For now, I only need numpy and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "romance-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-salvation",
   "metadata": {},
   "source": [
    "## Implementation from First Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-wagner",
   "metadata": {},
   "source": [
    "This function filters a single image with every output filter and adds the bias term, resulting in a rank 3 array.  The first two levels of the nested loop extract a rank 3 chunk from `image`, while the third level of the nested loop performs the filtering and biasing.  After a chunk is processed and the results placed in the output array `y`, the filter shape and stride is used to calculate the next chunk position.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "former-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image(image, filters, strides):\n",
    "    \n",
    "    xm, xn, _  = image.shape \n",
    "    \n",
    "    km, kn, ni, no = filters.shape \n",
    "    \n",
    "    \n",
    "    sm, sn = strides\n",
    "    #ym, yn = 1 + ((xm - km + 1)//sm), 1 + ((xn - kn + 1)//sn)\n",
    "    ym, yn = 1 + ((xm - km)//sm), 1 + ((xn - kn)//sn) \n",
    "    y = np.zeros((ym, yn, no))\n",
    "\n",
    "    for iy, ix in enumerate(range(0, xm-km+1, sm)):\n",
    "        for jy, jx in enumerate(range(0, xn-kn+1, sn)):\n",
    "            # Apply each output filter and bias term to this chunk\n",
    "            chunk = image[ix:ix+km,jx:jx+kn,:]\n",
    "            for channel in range(no):\n",
    "                y[iy,jy,channel] = np.sum(filters[:,:,:,channel] * chunk)# + biases[channel]\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-timeline",
   "metadata": {},
   "source": [
    "Once we have an algorithm to filter a single image, it can easily be extended to a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "compatible-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image_batch(batch, filters, biases, strides):\n",
    "    outputs = [filter_image(image, filters, strides) for image in batch]\n",
    "    outputs = np.array(outputs)\n",
    "    outputs = outputs + biases\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-venice",
   "metadata": {},
   "source": [
    "First, a list comprehension applies the `filter_image` function defined above, to each image in the batch.  Next, the list returned by the \n",
    "list comprehension, is converted to a rank 4 array with the `np.array` function.  The line preceding the `return` statement, \n",
    "```python\n",
    "outputs = outputs + biases\n",
    "```\n",
    "seems like it shouldn't work, because the ranks don't match.  Fortunately,  numpy's broadcasting rules come to the rescue and does what we want.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-watch",
   "metadata": {},
   "source": [
    "## Compare to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-motel",
   "metadata": {},
   "source": [
    "To compare the numpy version to Keras, I'm going to create a `Conv2D` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "postal-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_keras = tf.keras.layers.Conv2D(filters=4, kernel_size=(4, 4), strides=(1,1), bias_initializer='he_uniform', padding='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-harvest",
   "metadata": {},
   "source": [
    "initialize a random batch of fakey images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "enclosed-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = np.random.randn(2,28,28,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-funeral",
   "metadata": {},
   "source": [
    "and filter the batch with the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "computational-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_batch_keras = layer_keras(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-cassette",
   "metadata": {},
   "source": [
    "Next, the filters, biases and strides are extracted from the layer.  Note that `strides` doesn't really need to be accessed from the layer, it's in the `Conv2D` constructor after all.  The way I did it here is just less error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "israeli-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, biases = layer_keras.get_weights()\n",
    "strides = layer_keras.strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-catch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "hungarian-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_batch_numpy = filter_image_batch(input_batch, filters, biases, strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "royal-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.max(np.abs(output_batch_keras - output_batch_numpy)) < 1e-6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-preview",
   "metadata": {},
   "source": [
    "### Convolutional Layer in JAX\n",
    "In addition to the standard set of imports, I decided to import a function from fast.ai's fastcore library, called `patch`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-savage",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "fixed-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Tuple, List\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "existing-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D: \n",
    "    filters: jnp.ndarray \n",
    "    biases: jnp.ndarray\n",
    "    input_channels: int \n",
    "    output_channels: int \n",
    "    filter_shape: Tuple[int,int]\n",
    "    strides: Tuple[int,int]\n",
    "    padding: str\n",
    "    seed: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-robertson",
   "metadata": {},
   "source": [
    "#### Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "improved-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def __init__(self: Conv2D, input_channels, output_channels, filter_shape=(2,2), strides=(1,1), padding='valid', seed=1234, build=True):\n",
    "    \n",
    "    self.input_channels = input_channels\n",
    "    self.output_channels = output_channels\n",
    "    self.filter_shape = filter_shape\n",
    "    self.strides = strides \n",
    "    self.padding = padding\n",
    "    self.seed = seed\n",
    "        \n",
    "    if build:\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        fkey, bkey = jax.random.split(key)\n",
    "            \n",
    "        # kaiming/he uniform, using Pytorch documentation\n",
    "        K = input_channels * filter_shape[0] * filter_shape[1]\n",
    "        sqrtK = jnp.sqrt(K)\n",
    "        self.filters = jax.random.uniform(fkey, (*filter_shape, input_channels, output_channels), minval=-sqrtK, maxval=+sqrtK)\n",
    "        self.biases = jax.random.uniform(bkey, (output_channels,), minval=-sqrtK, maxval=+sqrtK)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-cross",
   "metadata": {},
   "source": [
    "#### The `__call__` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-honolulu",
   "metadata": {},
   "source": [
    "To implement `__call__` we use the JAX builtin function `conv_general_dilated`.  Except for the `dimension_numbers` argument, it's pretty easy to figure out what it's doing (but I'm still not clear on how it works - maybe save that for another post).  Like the Keras `Conv2D` layer, it has additional input arguments that give you further control over the convolution.  I'm not including these additional arguments here because I'm trying to keep things as simple as possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "exciting-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def __call__(self: Conv2D, batch: jnp.ndarray):\n",
    "    outputs = jax.lax.conv_general_dilated(\n",
    "        lhs=batch,\n",
    "        rhs=self.filters,\n",
    "        window_strides=self.strides,\n",
    "        padding=self.padding,\n",
    "        dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    "    )   \n",
    "    \n",
    "    # This uses the broadcasting rules.\n",
    "    outputs +=  biases\n",
    "        \n",
    "    # Need to add biases...\n",
    "    return outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-platform",
   "metadata": {},
   "source": [
    "The `dimension_numbers` field is a three element tuple that defines the shape layout of the input batch, the filters, and the output batch respectively.  We've adopted the default Keras layout, which means that for an input batch of images, the batch dimension is listed first, the image height second, the image width third, and the number of input channels fourth.  The dimension number for this is represented as `'NHWC'`.  By default, the filters are arranged in a similar way although there is no batch dimension: the filter height comes first, the filter width second, the input channel count third, and the output channel count last.  As you can see, the description number for this is `'HWIO'`.\n",
    "\n",
    "Because `conv_general_dilatated` does not work with the biases, they must be added to the convolution outputs.  Like numpy, JAX has broadcasting rules that make this mixed-rank addition work as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-pepper",
   "metadata": {},
   "source": [
    "#### Adding to pytree Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "heard-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def tree_flatten(self: Conv2D) -> Tuple[List[jnp.ndarray], dict]:\n",
    "    params = (self.filters, self.biases)\n",
    "    metadata = {k: v for k, v in self.__dict__.items() if k not in {'biases', 'filters'}}\n",
    "    return params, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "ecological-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(cls_method=True)\n",
    "def tree_unflatten(cls: Conv2D, metadata: dict, params:List[jnp.ndarray]):\n",
    "    # This assumes that each key-value pair in the metadata dict corresponds to a constructor argument.\n",
    "    layer = cls(**metadata, build=False)\n",
    "    layer.filters, layer.biases = params\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-trustee",
   "metadata": {},
   "source": [
    "Finally, `Conv2D` can be added the pytree registry with the following line of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "cooked-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = jax.tree_util.register_pytree_node_class(Conv2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-shelter",
   "metadata": {},
   "source": [
    "You'll get an exception complaining about duplicate registration if you run this cell twice.  Until I figure out how to remove from the pytree registry (if it's even possible), my work around is to re-run the cells (in this order) containing the class declaration, the methods, and finally the class registration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "corresponding-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = jax.tree_flatten(Conv2D(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "integrated-influence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "assisted-berry",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "opening-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_channels\n",
      "<class 'str'>\n",
      "output_channels\n",
      "<class 'str'>\n",
      "filter_shape\n",
      "<class 'str'>\n",
      "strides\n",
      "<class 'str'>\n",
      "padding\n",
      "<class 'str'>\n",
      "seed\n",
      "<class 'str'>\n",
      "build\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "args_name = inspect.signature(Conv2D)\n",
    "for a in args_name.parameters:\n",
    "    print(a), print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-gamma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
