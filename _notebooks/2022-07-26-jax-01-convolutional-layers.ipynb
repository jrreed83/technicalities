{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enclosed-vessel",
   "metadata": {},
   "source": [
    "# Convolutional Layers\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [jax, convolution, pooling]\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-induction",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post, I'll start by implementing a basic convolutional layer using numpy and validate it against Keras.  After this, I'll write a more efficient one using JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-teens",
   "metadata": {},
   "source": [
    "## How Convolutional Layers work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-indian",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "For now, I only need numpy and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daily-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-london",
   "metadata": {},
   "source": [
    "## Implementation from First Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-chinese",
   "metadata": {},
   "source": [
    "This function filters a single image with every output filter and adds the bias term, resulting in a rank 3 array.  The first two levels of the nested loop extract a rank 3 chunk from `image`, while the third level of the nested loop performs the filtering and biasing.  After a chunk is processed and the results placed in the output array `y`, the filter shape and stride is used to calculate the next chunk position.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "immediate-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image(image, filters, strides):\n",
    "    \n",
    "    xm, xn, _  = image.shape \n",
    "    \n",
    "    km, kn, ni, no = filters.shape \n",
    "    \n",
    "    \n",
    "    sm, sn = strides\n",
    "    #ym, yn = 1 + ((xm - km + 1)//sm), 1 + ((xn - kn + 1)//sn)\n",
    "    ym, yn = 1 + ((xm - km)//sm), 1 + ((xn - kn)//sn) \n",
    "    y = np.zeros((ym, yn, no))\n",
    "\n",
    "    for iy, ix in enumerate(range(0, xm-km+1, sm)):\n",
    "        for jy, jx in enumerate(range(0, xn-kn+1, sn)):\n",
    "            # Apply each output filter and bias term to this chunk\n",
    "            chunk = image[ix:ix+km,jx:jx+kn,:]\n",
    "            for channel in range(no):\n",
    "                y[iy,jy,channel] = np.sum(filters[:,:,:,channel] * chunk)# + biases[channel]\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-drama",
   "metadata": {},
   "source": [
    "Once we have an algorithm to filter a single image, it's very simple to filter a batch of images.  Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "lovely-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image_batch(batch, filters, biases, strides):\n",
    "    outputs = [filter_image(image, filters, strides) for image in batch]\n",
    "    outputs = np.array(outputs)\n",
    "    outputs = outputs + biases\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-sandwich",
   "metadata": {},
   "source": [
    "First, a list comprehension applies the `filter_image` function defined above, to each image in the batch.  Next, the list returned by the \n",
    "list comprehension, is converted to a rank 4 array with the `np.array` function.  The line preceding the `return` statement, \n",
    "```python\n",
    "outputs = outputs + biases\n",
    "```\n",
    "seems like it shouldn't work, because the ranks don't match.  Fortunately,  numpy's broadcasting rules come to the rescue and does what we want.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-diploma",
   "metadata": {},
   "source": [
    "## Compare to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "middle-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_keras = tf.keras.layers.Conv2D(filters=4, kernel_size=(4, 4), strides=(1,1), bias_initializer='he_uniform', padding='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "ranging-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = np.random.randn(2,28,28,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "obvious-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_batch_keras = layer_keras(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "unlike-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, biases = layer_keras.get_weights()\n",
    "strides = layer_keras.strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "declared-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_batch_numpy = filter_image_batch(input_batch, filters, biases, strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "radical-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.max(np.abs(output_batch_keras - output_batch_numpy)) < 1e-6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-eclipse",
   "metadata": {},
   "source": [
    "### Convolutional Layer in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "grand-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from fastcore.basics import patch, store_attr\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "fiscal-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D: \n",
    "    filters: jnp.ndarray \n",
    "    biases: jnp.ndarray\n",
    "    input_channels: int \n",
    "    output_channels: int \n",
    "    filter_shape: Tuple[int,int]\n",
    "    strides: Tuple[int,int]\n",
    "    padding: str\n",
    "    seed: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "confidential-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def __init__(self: Conv2D, input_channels, output_channels, filter_shape=(2,2), strides=(1,1), padding='valid', seed=1234, build=True):\n",
    "    \n",
    "    self.input_channels = input_channels\n",
    "    self.output_channels = output_channels\n",
    "    self.filter_shape = filter_shape\n",
    "    self.strides = strides \n",
    "    self.padding = padding\n",
    "    self.seed = seed\n",
    "        \n",
    "    if build:\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        fkey, bkey = jax.random.split(key)\n",
    "            \n",
    "        # kaiming/he uniform, using Pytorch documentation\n",
    "        K = input_channels * filter_shape[0] * filter_shape[1]\n",
    "        sqrtK = jnp.sqrt(K)\n",
    "        self.filters = jax.random.uniform(fkey, (*filter_shape, input_channels, output_channels), minval=-sqrtK, maxval=+sqrtK)\n",
    "        self.biases = jax.random.uniform(bkey, (output_channels,), minval=-sqrtK, maxval=+sqrtK)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-platinum",
   "metadata": {},
   "source": [
    "The `__call__` method uses a JAX function called `conv_general_dilated`.  Except for the `dimension_numbers` argument, it's pretty easy to figure out what it's doing.  Like the Keras `Conv2D` layer, it has additional input arguments that give you further control over the convolution.  I'm not including these additional arguments here because I'm trying to keep things as simple as possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "marine-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def __call__(self: Conv2D, batch: jnp.ndarray):\n",
    "    outputs = jax.lax.conv_general_dilated(\n",
    "        lhs=batch,\n",
    "        rhs=self.filters,\n",
    "        window_strides=self.strides,\n",
    "        padding=self.padding,\n",
    "        dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    "    )   \n",
    "    \n",
    "    # This uses the broadcasting rules.\n",
    "    outputs +=  biases\n",
    "        \n",
    "    # Need to add biases...\n",
    "    return outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-reason",
   "metadata": {},
   "source": [
    "The `dimension_numbers` field is a three element tuple that defines the shape layout of the input batch, the filters, and the output batch respectively.  We've adopted the default Keras layout, which means that for an input batch of images, the batch dimension first, the image height second, the image width third, and the number of input channels fourth.  The dimension number for this is represented as `'NHWC'`.  By default, the filters are arranged in a similar way although there is no batch dimension: the filter height comes first, the filter width second, the input channel count third, and the output channel count last.  As you can see, the description number for this is `'HWIO'`.\n",
    "\n",
    "Because `conv_general_dilatated` does not work with the biases, they must be added to the convolution outputs.  Like numpy, JAX has broadcasting rules that make this mixed-rank addition work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "boring-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def tree_flatten(self: Conv2D):\n",
    "    params = (self.filters, self.biases)\n",
    "    metadata = {k: v for k, v in self.__dict__.items() if k not in {'biases', 'filters'}}\n",
    "    return params, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "military-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(cls_method=True)\n",
    "def tree_unflatten(cls: Conv2D, metadata, params):\n",
    "    \n",
    "    layer = cls(**metadata, build=False)\n",
    "    layer.filters, layer.biases = params\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-sussex",
   "metadata": {},
   "source": [
    "Finally, register the `Conv2D` class as a pytree.  The `register_pytree_node_class` checks that the class name you plug in has a `tree_flatten` method and a `tree_unflatten` class method.  If you don't, JAX will raise an exception.  You'll get a different exception if you try to register the same class twice.  I haven't figured out how to remove a class from the pytree registry yet.  When working in Jupyter notebooks, my work around is to re-run the cell that has the class definition.  My guess is that this error doesn't show up when working in a text-editor or IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "??jax.tree_util.register_pytree_node_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "anonymous-water",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Conv2D"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.register_pytree_node_class(Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "fewer-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = Conv2D(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "analyzed-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = jax.tree_flatten(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "failing-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = jax.tree_unflatten(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "organized-papua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_channels': 2,\n",
       " 'output_channels': 3,\n",
       " 'filter_shape': (2, 2),\n",
       " 'strides': (1, 1),\n",
       " 'padding': 'valid',\n",
       " 'seed': 1234,\n",
       " 'filters': DeviceArray([[[[ 2.0073094 ,  1.6691824 , -0.52152306],\n",
       "                [ 0.01462327,  0.3469043 ,  2.7000263 ]],\n",
       " \n",
       "               [[-2.5442414 ,  2.7617264 , -0.4976167 ],\n",
       "                [ 0.39192185, -0.4488464 , -1.8725433 ]]],\n",
       " \n",
       " \n",
       "              [[[ 0.5440059 ,  1.5025641 , -2.4879715 ],\n",
       "                [-1.8504584 , -0.22452806,  2.825495  ]],\n",
       " \n",
       "               [[-0.43859494, -1.9360245 , -1.3612974 ],\n",
       "                [-0.29709414, -1.0776254 ,  0.91766566]]]], dtype=float32),\n",
       " 'biases': DeviceArray([-1.5805513,  1.9187174,  2.2878523], dtype=float32)}"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "maritime-semiconductor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_channels': 2,\n",
       " 'output_channels': 3,\n",
       " 'filter_shape': (2, 2),\n",
       " 'strides': (1, 1),\n",
       " 'padding': 'valid',\n",
       " 'seed': 1234,\n",
       " 'filters': DeviceArray([[[[ 2.0073094 ,  1.6691824 , -0.52152306],\n",
       "                [ 0.01462327,  0.3469043 ,  2.7000263 ]],\n",
       " \n",
       "               [[-2.5442414 ,  2.7617264 , -0.4976167 ],\n",
       "                [ 0.39192185, -0.4488464 , -1.8725433 ]]],\n",
       " \n",
       " \n",
       "              [[[ 0.5440059 ,  1.5025641 , -2.4879715 ],\n",
       "                [-1.8504584 , -0.22452806,  2.825495  ]],\n",
       " \n",
       "               [[-0.43859494, -1.9360245 , -1.3612974 ],\n",
       "                [-0.29709414, -1.0776254 ,  0.91766566]]]], dtype=float32),\n",
       " 'biases': DeviceArray([-1.5805513,  1.9187174,  2.2878523], dtype=float32)}"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "wound-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Conv2D(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "medieval-defensive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((DeviceArray([[[[ 3.4430575 , -0.5913736 ,  1.7550349 ,  2.4153824 ],\n",
       "                 [ 2.8320181 , -0.85370946, -1.1039739 , -0.12423529],\n",
       "                 [ 2.7139978 , -0.70104486, -3.4376502 ,  2.6779363 ]],\n",
       "  \n",
       "                [[-2.0729692 ,  0.3460869 ,  1.6827095 , -1.5708281 ],\n",
       "                 [ 3.45853   , -0.8395171 , -1.9809046 ,  2.2974694 ],\n",
       "                 [ 0.7820349 ,  1.3051779 , -0.6155949 , -1.9725654 ]]],\n",
       "  \n",
       "  \n",
       "               [[[ 0.04273734,  0.6269528 ,  0.33181936, -0.38829234],\n",
       "                 [-0.02845082, -0.33282036, -3.160122  ,  1.999361  ],\n",
       "                 [-0.4157628 ,  1.7358813 ,  2.6174173 ,  3.3147407 ]],\n",
       "  \n",
       "                [[-2.8882236 ,  0.46306247, -2.8370042 ,  3.4490387 ],\n",
       "                 [ 2.8144124 , -0.44645762, -1.2920336 ,  2.0018265 ],\n",
       "                 [ 3.3088658 ,  2.9831839 , -0.35555673,  1.3202416 ]]]],            dtype=float32),\n",
       "  DeviceArray([-1.9357721 ,  0.32821512,  2.8020353 , -3.1447244 ], dtype=float32)),\n",
       " (3, 4, (1, 1), 'valid'))"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.tree_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "polish-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.zeros((2,4,4,2))\n",
    "b = jnp.array([1,2])\n",
    "y = x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "present-filename",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[2., 2., 2., 2.],\n",
       "             [2., 2., 2., 2.],\n",
       "             [2., 2., 2., 2.],\n",
       "             [2., 2., 2., 2.]], dtype=float32)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "placed-sender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"2D convolution layer (e.g. spatial convolution over images).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  This layer creates a convolution kernel that is convolved\u001b[0m\n",
       "\u001b[0;34m  with the layer input to produce a tensor of\u001b[0m\n",
       "\u001b[0;34m  outputs. If `use_bias` is True,\u001b[0m\n",
       "\u001b[0;34m  a bias vector is created and added to the outputs. Finally, if\u001b[0m\n",
       "\u001b[0;34m  `activation` is not `None`, it is applied to the outputs as well.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  When using this layer as the first layer in a model,\u001b[0m\n",
       "\u001b[0;34m  provide the keyword argument `input_shape`\u001b[0m\n",
       "\u001b[0;34m  (tuple of integers or `None`, does not include the sample axis),\u001b[0m\n",
       "\u001b[0;34m  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\u001b[0m\n",
       "\u001b[0;34m  in `data_format=\"channels_last\"`. You can use `None` when\u001b[0m\n",
       "\u001b[0;34m  a dimension has variable size.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Examples:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  >>> # The inputs are 28x28 RGB images with `channels_last` and the batch\u001b[0m\n",
       "\u001b[0;34m  >>> # size is 4.\u001b[0m\n",
       "\u001b[0;34m  >>> input_shape = (4, 28, 28, 3)\u001b[0m\n",
       "\u001b[0;34m  >>> x = tf.random.normal(input_shape)\u001b[0m\n",
       "\u001b[0;34m  >>> y = tf.keras.layers.Conv2D(\u001b[0m\n",
       "\u001b[0;34m  ... 2, 3, activation='relu', input_shape=input_shape[1:])(x)\u001b[0m\n",
       "\u001b[0;34m  >>> print(y.shape)\u001b[0m\n",
       "\u001b[0;34m  (4, 26, 26, 2)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  >>> # With `dilation_rate` as 2.\u001b[0m\n",
       "\u001b[0;34m  >>> input_shape = (4, 28, 28, 3)\u001b[0m\n",
       "\u001b[0;34m  >>> x = tf.random.normal(input_shape)\u001b[0m\n",
       "\u001b[0;34m  >>> y = tf.keras.layers.Conv2D(\u001b[0m\n",
       "\u001b[0;34m  ... 2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)\u001b[0m\n",
       "\u001b[0;34m  >>> print(y.shape)\u001b[0m\n",
       "\u001b[0;34m  (4, 24, 24, 2)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  >>> # With `padding` as \"same\".\u001b[0m\n",
       "\u001b[0;34m  >>> input_shape = (4, 28, 28, 3)\u001b[0m\n",
       "\u001b[0;34m  >>> x = tf.random.normal(input_shape)\u001b[0m\n",
       "\u001b[0;34m  >>> y = tf.keras.layers.Conv2D(\u001b[0m\n",
       "\u001b[0;34m  ... 2, 3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\u001b[0m\n",
       "\u001b[0;34m  >>> print(y.shape)\u001b[0m\n",
       "\u001b[0;34m  (4, 28, 28, 2)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  >>> # With extended batch shape [4, 7]:\u001b[0m\n",
       "\u001b[0;34m  >>> input_shape = (4, 7, 28, 28, 3)\u001b[0m\n",
       "\u001b[0;34m  >>> x = tf.random.normal(input_shape)\u001b[0m\n",
       "\u001b[0;34m  >>> y = tf.keras.layers.Conv2D(\u001b[0m\n",
       "\u001b[0;34m  ... 2, 3, activation='relu', input_shape=input_shape[2:])(x)\u001b[0m\n",
       "\u001b[0;34m  >>> print(y.shape)\u001b[0m\n",
       "\u001b[0;34m  (4, 7, 26, 26, 2)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Args:\u001b[0m\n",
       "\u001b[0;34m    filters: Integer, the dimensionality of the output space (i.e. the number of\u001b[0m\n",
       "\u001b[0;34m      output filters in the convolution).\u001b[0m\n",
       "\u001b[0;34m    kernel_size: An integer or tuple/list of 2 integers, specifying the height\u001b[0m\n",
       "\u001b[0;34m      and width of the 2D convolution window. Can be a single integer to specify\u001b[0m\n",
       "\u001b[0;34m      the same value for all spatial dimensions.\u001b[0m\n",
       "\u001b[0;34m    strides: An integer or tuple/list of 2 integers, specifying the strides of\u001b[0m\n",
       "\u001b[0;34m      the convolution along the height and width. Can be a single integer to\u001b[0m\n",
       "\u001b[0;34m      specify the same value for all spatial dimensions. Specifying any stride\u001b[0m\n",
       "\u001b[0;34m      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\u001b[0m\n",
       "\u001b[0;34m    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\u001b[0m\n",
       "\u001b[0;34m      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\u001b[0m\n",
       "\u001b[0;34m      to the left/right or up/down of the input. When `padding=\"same\"` and\u001b[0m\n",
       "\u001b[0;34m      `strides=1`, the output has the same size as the input.\u001b[0m\n",
       "\u001b[0;34m    data_format: A string, one of `channels_last` (default) or `channels_first`.\u001b[0m\n",
       "\u001b[0;34m      The ordering of the dimensions in the inputs. `channels_last` corresponds\u001b[0m\n",
       "\u001b[0;34m      to inputs with shape `(batch_size, height, width, channels)` while\u001b[0m\n",
       "\u001b[0;34m      `channels_first` corresponds to inputs with shape `(batch_size, channels,\u001b[0m\n",
       "\u001b[0;34m      height, width)`. It defaults to the `image_data_format` value found in\u001b[0m\n",
       "\u001b[0;34m      your Keras config file at `~/.keras/keras.json`. If you never set it, then\u001b[0m\n",
       "\u001b[0;34m      it will be `channels_last`.\u001b[0m\n",
       "\u001b[0;34m    dilation_rate: an integer or tuple/list of 2 integers, specifying the\u001b[0m\n",
       "\u001b[0;34m      dilation rate to use for dilated convolution. Can be a single integer to\u001b[0m\n",
       "\u001b[0;34m      specify the same value for all spatial dimensions. Currently, specifying\u001b[0m\n",
       "\u001b[0;34m      any `dilation_rate` value != 1 is incompatible with specifying any stride\u001b[0m\n",
       "\u001b[0;34m      value != 1.\u001b[0m\n",
       "\u001b[0;34m    groups: A positive integer specifying the number of groups in which the\u001b[0m\n",
       "\u001b[0;34m      input is split along the channel axis. Each group is convolved separately\u001b[0m\n",
       "\u001b[0;34m      with `filters / groups` filters. The output is the concatenation of all\u001b[0m\n",
       "\u001b[0;34m      the `groups` results along the channel axis. Input channels and `filters`\u001b[0m\n",
       "\u001b[0;34m      must both be divisible by `groups`.\u001b[0m\n",
       "\u001b[0;34m    activation: Activation function to use. If you don't specify anything, no\u001b[0m\n",
       "\u001b[0;34m      activation is applied (see `keras.activations`).\u001b[0m\n",
       "\u001b[0;34m    use_bias: Boolean, whether the layer uses a bias vector.\u001b[0m\n",
       "\u001b[0;34m    kernel_initializer: Initializer for the `kernel` weights matrix (see\u001b[0m\n",
       "\u001b[0;34m      `keras.initializers`). Defaults to 'glorot_uniform'.\u001b[0m\n",
       "\u001b[0;34m    bias_initializer: Initializer for the bias vector (see\u001b[0m\n",
       "\u001b[0;34m      `keras.initializers`). Defaults to 'zeros'.\u001b[0m\n",
       "\u001b[0;34m    kernel_regularizer: Regularizer function applied to the `kernel` weights\u001b[0m\n",
       "\u001b[0;34m      matrix (see `keras.regularizers`).\u001b[0m\n",
       "\u001b[0;34m    bias_regularizer: Regularizer function applied to the bias vector (see\u001b[0m\n",
       "\u001b[0;34m      `keras.regularizers`).\u001b[0m\n",
       "\u001b[0;34m    activity_regularizer: Regularizer function applied to the output of the\u001b[0m\n",
       "\u001b[0;34m      layer (its \"activation\") (see `keras.regularizers`).\u001b[0m\n",
       "\u001b[0;34m    kernel_constraint: Constraint function applied to the kernel matrix (see\u001b[0m\n",
       "\u001b[0;34m      `keras.constraints`).\u001b[0m\n",
       "\u001b[0;34m    bias_constraint: Constraint function applied to the bias vector (see\u001b[0m\n",
       "\u001b[0;34m      `keras.constraints`).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Input shape:\u001b[0m\n",
       "\u001b[0;34m    4+D tensor with shape: `batch_shape + (channels, rows, cols)` if\u001b[0m\n",
       "\u001b[0;34m      `data_format='channels_first'`\u001b[0m\n",
       "\u001b[0;34m    or 4+D tensor with shape: `batch_shape + (rows, cols, channels)` if\u001b[0m\n",
       "\u001b[0;34m      `data_format='channels_last'`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Output shape:\u001b[0m\n",
       "\u001b[0;34m    4+D tensor with shape: `batch_shape + (filters, new_rows, new_cols)` if\u001b[0m\n",
       "\u001b[0;34m    `data_format='channels_first'` or 4+D tensor with shape: `batch_shape +\u001b[0m\n",
       "\u001b[0;34m      (new_rows, new_cols, filters)` if `data_format='channels_last'`.  `rows`\u001b[0m\n",
       "\u001b[0;34m      and `cols` values might have changed due to padding.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Returns:\u001b[0m\n",
       "\u001b[0;34m    A tensor of rank 4+ representing\u001b[0m\n",
       "\u001b[0;34m    `activation(conv2d(inputs, kernel) + bias)`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m  Raises:\u001b[0m\n",
       "\u001b[0;34m    ValueError: if `padding` is `\"causal\"`.\u001b[0m\n",
       "\u001b[0;34m    ValueError: when both `strides > 1` and `dilation_rate > 1`.\u001b[0m\n",
       "\u001b[0;34m  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.8/site-packages/keras/layers/convolutional.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Conv2DTranspose, Conv2D\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tf.keras.layers.Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-interim",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
