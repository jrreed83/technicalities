{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "little-safety",
   "metadata": {},
   "source": [
    "# Convolutional Layers\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [jax, convolution, pooling]\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-conspiracy",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post, I'll start by implementing a basic convolutional layer using numpy and validate it against Keras.  After this, I'll write a more efficient one using JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-nickname",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "For now, I only need numpy and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "manual-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-kansas",
   "metadata": {},
   "source": [
    "Here's a small sequential model consisting of a convolutional layer and max-pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "preceding-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Conv2D(filters=4, kernel_size=(2, 2), strides=(2,2), padding='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-funeral",
   "metadata": {},
   "source": [
    "For this experiment, I don't care what the input to the model is.  So I'll just create a 1-element batch consisting of a random 28-by-28 array and apply `model` to it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "minute-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 15:14:21.032488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/bin:/usr/local/lib:\n",
      "2022-07-30 15:14:21.032522: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-30 15:14:21.032810: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2022-07-30 15:14:21.041129: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = np.random.randn(1,28,28,3)\n",
    "outputs = layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-russell",
   "metadata": {},
   "source": [
    "Let's check the shape of `inputs` and `outputs`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "increasing-massage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Mapping:  (1, 28, 28, 3) -> (1, 14, 14, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature Mapping:  {inputs.shape} -> {outputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-nancy",
   "metadata": {},
   "source": [
    "The `outputs` is a 1-element batch consisting of a 7-by-7 array with 4 channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-reminder",
   "metadata": {},
   "source": [
    "Getting the pooling layer output is easy; it's the same as `outputs`.  However, it's not immediately obvious how to get the intermediate convolution output. The recommended way to extract the outputs from all the layers in your model is to use the so-called *Functional* API.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-alberta",
   "metadata": {},
   "source": [
    "## Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-oxford",
   "metadata": {},
   "source": [
    "This function filters a single image with every output filter and adds the bias term, resulting in a rank 3 array.  The first two levels of the nested loop extract a rank 3 chunk from `image`, while the third level of the nested loop performs the filtering and biasing.  After a chunk is processed and the results placed in the output array `y`, the filter shape and stride is used to calculate the next chunk position.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "sealed-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image(image, filters, biases, strides):\n",
    "    \n",
    "    xm, xn, _  = image.shape \n",
    "    km, kn, *_ = filters.shape \n",
    "    \n",
    "    num_output_channels = len(biases)\n",
    "    \n",
    "    sm, sn = strides\n",
    "    ym, yn = 1 + ((xm - km + 1)//sm), 1 + ((xn - kn + 1)//sn)\n",
    "    \n",
    "    y = np.zeros((ym, yn, num_output_channels))\n",
    "\n",
    "    for iy, ix in enumerate(range(0, xm-km+1, sm)):\n",
    "        for jy, jx in enumerate(range(0, xn-kn+1, sn)):\n",
    "            # Apply each output filter and bias term to this chunk\n",
    "            chunk = image[ix:ix+km,jx:jx+kn,:]\n",
    "            for channel in range(num_output_channels):\n",
    "                y[iy,jy,channel] = np.sum(filters[...,channel] * chunk) + biases[channel]\n",
    "            \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-anthropology",
   "metadata": {},
   "source": [
    "Once we have an algorithm to filter a single image, it's very simple to filter a batch of images.  Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "impressive-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_image_batch(images, filters, biases, strides):\n",
    "    return np.array([filter_image(image, filters, biases, strides) for image in images])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-minneapolis",
   "metadata": {},
   "source": [
    "This function requires a little explanation.  First the list comprehension passed to the `np.array` function, i.e.\n",
    "\n",
    "```python\n",
    "[filter_image(image, filters, biases, strides) for image in images]\n",
    "```\n",
    "applies `filter_image` to each image in the batch, resulting in a list of filtered images.  By passing this list to the `np.array` function, it's converted to an `ndarray` with a leading batch dimension.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "american-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.random.randn(2,28,28,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "wanted-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_layer = tf.keras.layers.Conv2D(filters=4, kernel_size=(2, 2), strides=(2,2), padding='valid')\n",
    "#keras_output = keras_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-turtle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "alpha-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "image = np.random.randn(1,28,28,3)\n",
    "keras_output = layer(image)[0,:,:,:]\n",
    "filters, biases = layer.get_weights()\n",
    "strides = (2,2)\n",
    "\n",
    "filter_image(np.squeeze(image), filters, biases, strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "artificial-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "keras_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-flood",
   "metadata": {},
   "source": [
    "Now I can make a little class that contains the filters, biases, and other necessary parameters for a convolutional layer.  The `__call__` method correlates each output filter with the input image and adds the bias.  Each filtered output is added to a list, and converted to a single-element batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "frequent-printing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3, 4)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_layer.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "controlling-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_layer.build(input_shape=(28,28,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "built-atlanta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "changed-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer = MyConv2D.setup(keras_layer)\n",
    "my_output = my_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "emotional-links",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 14, 14, 4)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-preserve",
   "metadata": {},
   "source": [
    "## Compare to Keras\n",
    "\n",
    "To compare my convolutional layer to Keras', I need to get any convolutional-relevant parameters from the Keras model and use them to initialize my layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "announced-bumper",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_280627/4206355748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkeras_conv_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "kernels, biases = model.layers[0].get_weights()\n",
    "strides = model.layers[0].strides\n",
    "keras_conv_features = keras_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "weekly-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer = MyConv2D(kernels, biases, strides=strides)\n",
    "my_conv_features = my_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "hollow-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.isclose(keras_conv_features, my_conv_features, atol=1e-6))\n",
    "assert np.all(np.isclose(jax_result, my_conv_features, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-thanksgiving",
   "metadata": {},
   "source": [
    "### Convolution in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_result = jax.lax.conv_general_dilated(\n",
    "    lhs=inputs,\n",
    "    rhs=kernels,\n",
    "    window_strides=strides,\n",
    "    padding='valid',\n",
    "    dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-reservoir",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
