{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "similar-superior",
   "metadata": {},
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-teaching",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-barrier",
   "metadata": {},
   "source": [
    "## Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "signed-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-psychiatry",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "The first function argument is the number of filters and the second argument is the shape of the kernel.  There are other parameters too, but I'm going to keep things simple for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "western-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=2, kernel_size=(2, 2), use_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-method",
   "metadata": {},
   "source": [
    "Arrays fed to `layer` must be a 4D tensor where the first axis is the batch size, the second axis is the width, the third axis is the height, and thr fourth axis is \n",
    "the number of input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "statistical-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = np.random.randn(4,4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-station",
   "metadata": {},
   "source": [
    "Because Keras layers operate on batches of data, and not individual examples, a batch dimension must be addes to `input_image`.  To do this in numpy, take `input_image` and add a `None` in the axis you want to add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "premium-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_image[None,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "lined-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = conv_layer(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dated-arcade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3, 3, 2])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "checked-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = conv_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "formal-birth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w shape = (2, 2, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'w shape = {kernels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-dictionary",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "other-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(kernel, x):\n",
    "    y = np.zeros((3,3))\n",
    "    \n",
    "    y[0,0] = np.sum(kernel * x[0:2,0:2])\n",
    "    y[0,1] = np.sum(kernel * x[0:2,1:3])\n",
    "    y[0,2] = np.sum(kernel * x[0:2,2:4])\n",
    "    y[1,0] = np.sum(kernel * x[1:3,0:2])\n",
    "    y[1,1] = np.sum(kernel * x[1:3,1:3])\n",
    "    y[1,2] = np.sum(kernel * x[1:3,2:4])\n",
    "    y[2,0] = np.sum(kernel * x[2:4,0:2])\n",
    "    y[2,1] = np.sum(kernel * x[2:4,1:3])\n",
    "    y[2,2] = np.sum(kernel * x[2:4,2:4])    \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "touched-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(range(5), range(5)):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "modular-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_1(kernel, x):\n",
    "    xm, xn = x.shape \n",
    "    km, kn = kernel.shape \n",
    "    \n",
    "    y = np.zeros((xm - km + 1, xn - kn + 1))\n",
    "    \n",
    "    ym, yn = y.shape\n",
    "    for i in range(ym):\n",
    "        for j in range(yn):\n",
    "            y[i, j] = np.sum(kernel * x[i:i+km, j:j+kn]) \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-passion",
   "metadata": {},
   "source": [
    "Here's a faily inefficient way to duplicate the evaluation of the `conv_layer` defined in above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "stylish-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(input_image, kernels):\n",
    "    \n",
    "    width, height, input_chans = input_image.shape \n",
    "    km, kn, _, output_chans = kernels.shape\n",
    "    \n",
    "    width_ = width - km + 1\n",
    "    height_ = height - kn + 1 \n",
    "    \n",
    "    features = np.zeros((width_, height_, output_chans))\n",
    "\n",
    "    for out_chan in range(output_chans):\n",
    "        y = np.zeros((width_, height_))\n",
    "        for in_chan in range(input_chans):\n",
    "            y += conv2d_1(\n",
    "                kernels[:,:,in_chan, out_chan], \n",
    "                input_image[:,:,in_chan]\n",
    "            )\n",
    "        \n",
    "        # set the features\n",
    "        features[:,:,out_chan] = y\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "short-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ = convolve(input_image, kernels)[None,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "accomplished-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.isclose(feature_maps, features_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-brunswick",
   "metadata": {},
   "source": [
    "## Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "framed-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ecological-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 2) -> (1, 2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "yy = pooling_layer(feature_maps)\n",
    "print(f'{feature_maps.shape} -> {yy.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "uniform-collar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 0.89900464 -1.0616233 ]\n",
      "   [-0.10247962  1.2988867 ]\n",
      "   [-0.8515937   0.6712394 ]]\n",
      "\n",
      "  [[ 0.9654248   0.23607667]\n",
      "   [-0.43404913  0.27193436]\n",
      "   [-0.5712364  -0.73882663]]\n",
      "\n",
      "  [[ 0.6847748  -0.8419535 ]\n",
      "   [ 0.46285468  0.7197448 ]\n",
      "   [-1.3353215   0.47066653]]]], shape=(1, 3, 3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "grateful-apple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 0.9654248   1.2988867 ]\n",
      "   [-0.10247962  1.2988867 ]]\n",
      "\n",
      "  [[ 0.9654248   0.7197448 ]\n",
      "   [ 0.46285468  0.7197448 ]]]], shape=(1, 2, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "sudden-waterproof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.0616233   1.2988867   0.6712394 ]\n",
      " [ 0.23607667  0.27193436 -0.73882663]\n",
      " [-0.8419535   0.7197448   0.47066653]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(feature_maps[0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "otherwise-finnish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.2988867, 1.2988867],\n",
       "       [0.7197448, 0.7197448]], dtype=float32)>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy[0,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "swiss-organizer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 4, 3)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "considerable-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2D(x, pool_size=(2,2), strides=(1,1)):\n",
    "    xm, xn = x.shape \n",
    "    pm, pn = pool_size \n",
    "    \n",
    "    y = np.zeros((xm - pm + 1, xn - pn + 1))\n",
    "    \n",
    "    ym, yn = y.shape\n",
    "    for i in range(ym):\n",
    "        for j in range(yn):\n",
    "            y[i, j] = np.max(x[i:i+pm, j:j+pn]) \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "cutting-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(features, pool_size=(2,2), strides=(1,1)):\n",
    "    \n",
    "    px, py = pool_size\n",
    "    width, height, chans = features.shape \n",
    "    \n",
    "    width_ = width - px + 1\n",
    "    height_ = height - py + 1 \n",
    "    \n",
    "    features_ = np.zeros((width_, height_, chans))\n",
    "\n",
    "    for chan in range(chans):\n",
    "        features_[:,:,chan] = pool2D(features[:,:,chan])\n",
    "    \n",
    "    return features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "expired-parent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.96542478,  1.29888666],\n",
       "        [-0.10247962,  1.29888666]],\n",
       "\n",
       "       [[ 0.96542478,  0.7197448 ],\n",
       "        [ 0.46285468,  0.7197448 ]]])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling(feature_maps[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "social-exercise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 2), dtype=float32, numpy=\n",
       "array([[[[ 0.9654248 ,  1.2988867 ],\n",
       "         [-0.10247962,  1.2988867 ]],\n",
       "\n",
       "        [[ 0.9654248 ,  0.7197448 ],\n",
       "         [ 0.46285468,  0.7197448 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-surrey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
