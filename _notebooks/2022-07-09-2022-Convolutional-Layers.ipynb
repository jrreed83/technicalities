{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interested-appeal",
   "metadata": {},
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png\n",
    "- hide: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-hardwood",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-recruitment",
   "metadata": {},
   "source": [
    "## Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "micro-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-filing",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "The first function argument is the number of filters and the second argument is the shape of the kernel.  There are other parameters too, but I'm going to keep things simple for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "current-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=2, kernel_size=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-consumer",
   "metadata": {},
   "source": [
    "Arrays fed to `layer` must be a 4D tensor where the first axis is the batch size, the second axis is the width, the third axis is the height, and thr fourth axis is \n",
    "the number of input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "noted-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = np.random.randn(4,4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-springer",
   "metadata": {},
   "source": [
    "Because Keras layers operate on batches of data, and not individual examples, a batch dimension must be addes to `input_image`.  To do this in numpy, take `input_image` and add a `None` in the axis you want to add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "viral-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_image[None,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "available-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = conv_layer(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "front-diploma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3, 3, 2])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "bigger-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels, biases = conv_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "armed-attention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernels shape = (2, 2, 3, 2), biases shape =(2,)\n"
     ]
    }
   ],
   "source": [
    "print(f'kernels shape = {kernels.shape}, biases shape ={biases.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "likely-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_(x, kernel):\n",
    "    y = np.zeros((3,3))\n",
    "    \n",
    "    y[0,0] = np.sum(kernel * x[0:2,0:2])\n",
    "    y[0,1] = np.sum(kernel * x[0:2,1:3])\n",
    "    y[0,2] = np.sum(kernel * x[0:2,2:4])\n",
    "    y[1,0] = np.sum(kernel * x[1:3,0:2])\n",
    "    y[1,1] = np.sum(kernel * x[1:3,1:3])\n",
    "    y[1,2] = np.sum(kernel * x[1:3,2:4])\n",
    "    y[2,0] = np.sum(kernel * x[2:4,0:2])\n",
    "    y[2,1] = np.sum(kernel * x[2:4,1:3])\n",
    "    y[2,2] = np.sum(kernel * x[2:4,2:4])    \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "satisfactory-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, kernel):\n",
    "    xm, xn = x.shape \n",
    "    km, kn = kernel.shape \n",
    "    \n",
    "    y = np.zeros((xm - km + 1, xn - kn + 1))\n",
    "    \n",
    "    ym, yn = y.shape\n",
    "    for i in range(ym):\n",
    "        for j in range(yn):\n",
    "            y[i, j] = np.sum(kernel * x[i:i+km, j:j+kn]) \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-comparison",
   "metadata": {},
   "source": [
    "Here's a faily inefficient way to duplicate the evaluation of the `conv_layer` defined in above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "intended-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(input_image, kernels, biases):\n",
    "    \n",
    "    width, height, input_chans = input_image.shape \n",
    "    km, kn, _, output_chans = kernels.shape\n",
    "    \n",
    "    w = width - km + 1\n",
    "    h = height - kn + 1 \n",
    "    \n",
    "    features = np.zeros((w, h, output_chans))\n",
    "\n",
    "    for out_chan in range(output_chans):\n",
    "        y = np.zeros((w, h))\n",
    "        for in_chan in range(input_chans):\n",
    "            y += conv2d(\n",
    "                input_image[:,:,in_chan],\n",
    "                kernels[:,:,in_chan, out_chan], \n",
    "            )\n",
    "        \n",
    "        # one bias per output feature\n",
    "        features[:,:,out_chan] = y + biases[out_chan]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "democratic-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ = convolve(input_image, kernels, biases)[None,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "planned-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.isclose(feature_maps, features_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-boundary",
   "metadata": {},
   "source": [
    "## Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "worth-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "italian-variance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 2) -> (1, 1, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "yy = pooling_layer(feature_maps)\n",
    "print(f'{feature_maps.shape} -> {yy.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "covered-farming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 2), dtype=float32, numpy=array([[[[0.9654248, 1.2988867]]]], dtype=float32)>"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "after-phase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 0.89900464 -1.0616233 ]\n",
      "   [-0.10247962  1.2988867 ]\n",
      "   [-0.8515937   0.6712394 ]]\n",
      "\n",
      "  [[ 0.9654248   0.23607667]\n",
      "   [-0.43404913  0.27193436]\n",
      "   [-0.5712364  -0.73882663]]\n",
      "\n",
      "  [[ 0.6847748  -0.8419535 ]\n",
      "   [ 0.46285468  0.7197448 ]\n",
      "   [-1.3353215   0.47066653]]]], shape=(1, 3, 3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ecological-point",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 0.9654248   1.2988867 ]\n",
      "   [-0.10247962  1.2988867 ]]\n",
      "\n",
      "  [[ 0.9654248   0.7197448 ]\n",
      "   [ 0.46285468  0.7197448 ]]]], shape=(1, 2, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "partial-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.0616233   1.2988867   0.6712394 ]\n",
      " [ 0.23607667  0.27193436 -0.73882663]\n",
      " [-0.8419535   0.7197448   0.47066653]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(feature_maps[0,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "golden-responsibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.2988867, 1.2988867],\n",
       "       [0.7197448, 0.7197448]], dtype=float32)>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy[0,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "hairy-authentication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 4, 3)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "amino-designer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "secondary-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2D(x, pool_size=(2,2), strides=(1,1), fn=np.max):\n",
    "    xm, xn = x.shape \n",
    "    pm, pn = pool_size \n",
    "    sm, sn = strides\n",
    "    \n",
    "    ym, yn = (xm-pm+1) // sm, (xn-pn+1) // sn\n",
    "\n",
    "    y = np.zeros((ym, yn))\n",
    "    \n",
    "    ii = 0\n",
    "    for i in range(0, xm-pm+1, sm):\n",
    "        jj = 0\n",
    "        for j in range(0, xn-pn+1, sn):\n",
    "            y[ii,jj] = fn(x[i:i+pm,j:j+pn])\n",
    "            jj += 1\n",
    "        ii += 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "sufficient-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def pool2D_(x, pool_size=(2,2), strides=(1,1)):\n",
    "##    xm, xn = x.shape \n",
    "#    pm, pn = pool_size \n",
    "#    sm, sn = strides\n",
    "#    \n",
    "#    y = np.zeros((xm - pm + 1, xn - pn + 1))\n",
    "#    \n",
    "#    ym, yn = y.shape\n",
    "#    \n",
    "#    for i in range(0, ym):\n",
    "#        for j in range(0, yn):\n",
    "#            y[i,j] = np.max(x[i:i+pm,j:j+pn])\n",
    "#    \n",
    "#    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "hawaiian-information",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36715236  0.09317793]\n",
      " [-0.36715236 -0.06301992]]\n",
      "[[0.98810509]]\n",
      "[[-0.92430295 -0.55469247 -0.59627284]\n",
      " [ 0.98810509 -0.64934654  0.29590853]\n",
      " [ 1.21270553  0.98248372 -0.38071894]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3,3)\n",
    "b = pool2D(x, strides=(2,2))\n",
    "print(a)\n",
    "print(b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "earlier-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(features, pool_size=(2,2), strides=(2,2)):\n",
    "    \n",
    "    px, py = pool_size\n",
    "    sm, sn = strides\n",
    "    width, height, chans = features.shape \n",
    "    \n",
    "    m, n = (width - px + 1) // sm, (height - py + 1) // sn\n",
    "    \n",
    "    features_ = np.zeros((m, n, chans))\n",
    "\n",
    "    # Note that we're not changing the number of features\n",
    "    for chan in range(chans):\n",
    "        features_[:,:,chan] = pool2D(features[:,:,chan], pool_size, strides)\n",
    "    \n",
    "    return features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "behavioral-genesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.3011235 , -0.24245605]]])"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling(feature_maps[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-freeze",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-cancer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
