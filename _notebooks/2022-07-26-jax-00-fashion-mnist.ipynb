{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acceptable-insulin",
   "metadata": {},
   "source": [
    "# Fashion MNIST with Vanilla JAX\n",
    "\n",
    "- toc: false\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [deep learning, JAX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-street",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this post, I'm going to implement a basic Fashion-MNIST classifier using JAX.  JAX is an array-processing library that uses Google's XLA (Accelerated Linear Algebra) compiler to generate high-performance code that can run on a variety of hardware platforms.  It feels a lot like numpy, with a number of advantages including built in automatic differentiation, vectorization and parallelization, and just-in-time compilation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-acting",
   "metadata": {},
   "source": [
    "By the end of this write up, I'm going to end up with this:\n",
    "\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=32)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X))))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=valid_datasource,\n",
    "    optimizer=Adam(network, lr=1e-3), \n",
    "    loss_fn=sparse_cross_entropy, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "Epoch 1/5\n",
    "1874/1875  [===============================] - 2s  1.65ms/batch - loss: 0.4956  - accuracy: 0.8250     - val_loss: 0.4446 - val_accuracy: 0.8396\n",
    "Epoch 2/5\n",
    "1874/1875  [===============================] - 2s  1.40ms/batch - loss: 0.3716  - accuracy: 0.8654     - val_loss: 0.4271 - val_accuracy: 0.8446\n",
    "Epoch 3/5\n",
    "1874/1875  [===============================] - 2s  1.39ms/batch - loss: 0.3330  - accuracy: 0.8791     - val_loss: 0.4034 - val_accuracy: 0.8528\n",
    "Epoch 4/5\n",
    "1874/1875  [===============================] - 2s  1.46ms/batch - loss: 0.3079  - accuracy: 0.8871     - val_loss: 0.3797 - val_accuracy: 0.8616\n",
    "Epoch 5/5\n",
    "1874/1875  [===============================] - 2s  1.97ms/batch - loss: 0.2891  - accuracy: 0.8935     - val_loss: 0.3599 - val_accuracy: 0.8710\n",
    "```\n",
    "\n",
    "It's pretty standard stuff.  Grab some data,  prepare the data for model training and validation, build a network with a commonly used *sequential* API, \n",
    "and finally train the model.  Besides using the Keras datasets library, everything will be written from scratch.  My goal is to demonstrate how simple it is to build the begininnings of a deep-learning library when you're working with the right set of tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-panic",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cognitive-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "from typing import Tuple, List, Any, Dict, Callable\n",
    "\n",
    "eps = jnp.finfo(jnp.float64).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-samuel",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The `Dataset` and `Dataloader` classes shown below are stripped down versions of the Pytorch versions.  For now, a `Dataset` is a container for JAX feature and label arrays.  In fact, besides having a length ( because it has a `__len__` method), it's pretty useless.  The `Dataloader` gives you the ability to iterate over batches of data from a `Dataset`, which is very important for effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loose-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return jnp.shape(self.X)[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i,:], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "structural-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, dataset: Dataset, batchsize=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batchsize): \n",
    "            yield self.dataset[i:i+self.batchsize]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batchsize\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-accountability",
   "metadata": {},
   "source": [
    "Let's go through an example of using `Dataset` and `Dataloader`.  First grab the Fashion MNIST data using the Keras dataset library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satellite-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number training examples = 60000\n",
      "Number testing examples = 10000\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "print(f'Number training examples = {len(y_train)}')\n",
    "print(f'Number testing examples = {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-familiar",
   "metadata": {},
   "source": [
    "There are ten articles of clothing represented in the dataset.  Here's the array that map an index to its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "responsible-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-trailer",
   "metadata": {},
   "source": [
    "Now let's put the training data into a dataloader configured to generate batches with 8 images, and show the first set of images with their labels. First I need to import the `Image` module from the `PIL` library.  Looping over `dataloader` will give you all 8-image batches in the training set.  Putting a `break` statement at the end of the loop, we're limited to looking at the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "historical-digit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "dataloader = Dataloader(Dataset(X_train, y_train), batchsize=8)\n",
    "\n",
    "\n",
    "for X_batch, y_batch in dataloader:\n",
    "    plt.figure(1, figsize=(20,5))\n",
    "    for i, (X, y) in enumerate(zip(X_batch, y_batch)):\n",
    "        img = Image.fromarray(X)\n",
    "        plt.subplot(2,4,i+1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'{class_labels[y]} / {y}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-favorite",
   "metadata": {},
   "source": [
    "## The Sequential Model\n",
    "\n",
    "As was shown above, we're going to build a *sequential* model.  It is made from a list of `Callable` objects that is evaluated by calling each members `__call__` method in order.  Each list element is a registered pytree, but I also wanted to have the flexibility to pass ordinary functions to the `Sequential` constructor and have everything just work.  As you'll soon see this feature was implemented by modifying the `__init__` method. Before building `Sequential` though, we need some basic layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-plymouth",
   "metadata": {},
   "source": [
    "### `Linear` Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-norfolk",
   "metadata": {},
   "source": [
    "The `Linear` layer defined below is very similar to implementations (but less general) you'd find in other non-JAX neural network libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "serial-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Linear:\n",
    "    w: jnp.ndarray \n",
    "    b: jnp.ndarray\n",
    "    ni: int \n",
    "    no: int \n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, build=True, seed=1234):\n",
    "        self.ni = num_inputs \n",
    "        self.no = num_outputs \n",
    "        # want to add seed as internal object\n",
    "        if build:\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs)\n",
    "            self.b = jnp.zeros(num_outputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear(num_inputs={self.ni}, num_outputs={self.no})'\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return jnp.dot(x, self.w) + self.b\n",
    "        \n",
    "    def params(self):\n",
    "        return {'w': self.w, 'b': self.b}\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return (self.w, self.b), (self.ni, self.no)\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layer = cls(*aux_data, build=False)\n",
    "        layer.w, layer.b = children\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-michael",
   "metadata": {},
   "source": [
    "The one glaring difference is the decorator and the two *tree* methods.  As the JAX documentation explains, the `register_pytree_node_class`, `tree_flatten` method, and `tree_unflatten` class methods are required to make a user-defined class into a pytree.  Once added, JAX will know how to transform back and forth between objects that the rest of JAX system can efficiently operate on, and objects that are specific to your application.    \n",
    "\n",
    "The `tree_flatten` method returns a two-element tuple consisting of the parameters you want to expose to JAX, and any meta-data that can help reconstruct the object.  Because JAX embraces the functional paradigm of immutable data structures, I thought it might be better to express the parameters as a tuple.  For `Linear`, the parameters are the weights and biases of the neural network.  For now, the only meta-data that seems helpful are the number of inputs and outputs (although this could be derived from the shape of the weights.  \n",
    "\n",
    "Another noteworthy thing about `Linear` is the `build` attribute.  Most of the time, you want to initialize the weights and biases at creation time time.  However, you don't want to do this when JAX reconstructs the object from it's flattened representation.  You probably just want to plop the parameters right into a freshly constructed object. The `build` attribute gives you some flexibility in that regard.  \n",
    "\n",
    "Let's take a look at a small example to see what we have.  Here's a linear layer that maps 5-element arrays to 1-element arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liable-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = Linear(5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-angel",
   "metadata": {},
   "source": [
    "Because `Linear` is a pytree, it can be passed to `jax.tree_flatten` to get it's weights and biases, and it's metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sublime-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = [[ 0.5734188 ]\n",
      " [-0.597884  ]\n",
      " [ 0.05189713]\n",
      " [-1.1660179 ]\n",
      " [ 0.29061902]], biases = [0.]\n",
      "metadata = PyTreeDef(CustomNode(<class '__main__.Linear'>[(5, 1)], [*, *]))\n"
     ]
    }
   ],
   "source": [
    "params, metadata = jax.tree_flatten(lin)\n",
    "print(f'weights = {params[0]}, biases = {params[1]}')\n",
    "print(f'metadata = {metadata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-conversion",
   "metadata": {},
   "source": [
    "Finally, the metadata and parameters from `jax.tree_flatten` can be used to generate a clone of the `lin`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fewer-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin2 = jax.tree_unflatten(metadata, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-halifax",
   "metadata": {},
   "source": [
    "Just to make sure that the weights, biases, and model outputs match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "banner-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jnp.all(jnp.isclose(lin.w, lin2.w))\n",
    "assert jnp.all(jnp.isclose(lin.b, lin2.b))\n",
    "\n",
    "x = np.random.randn(10,5)\n",
    "assert jnp.all(jnp.isclose(lin(x), lin2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-communications",
   "metadata": {},
   "source": [
    "### `Function` Layer\n",
    "\n",
    "The `Function` class fills the same need that `Lambda` layers do in Keras: being able to conveniently plug functions into models.  As the `tree_flatten` method shows, classes registered as pytrees can be parameter-free. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pregnant-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Function:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Function({self.fn.__name__})'\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return [], self.fn\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        return cls(aux_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-offense",
   "metadata": {},
   "source": [
    "### Helpful Functions\n",
    "\n",
    "Here are a few functions that will be *lifted* to `Function` layer in the `Sequential` model.  One common approach for improving classification accuracy is to normalize your input data.  When working with gray-scale images, this typically means rescaling the pixels from $[0,255]$ to $[0,1]$.  This is what `rescale_image` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excess-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_image(x): return x / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-proof",
   "metadata": {},
   "source": [
    "The model built in this post operates on batches of two-dimensional gray-scale images.  Each batch is a three-dimensional array and can be interpreted as a vertical stack of 2D images, where the height of the vertical stack is the number of images.  The `flatten` goes through each slice of the vertical stack and transforms the 2D array into a one-dimensional array.  In the process, the 3D input becomes a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stainless-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    shape = jnp.shape(x)\n",
    "    assert len(shape) == 3, 'x must represent a batch of two-dimensional gray-scale images' \n",
    "    batch_size = shape[0]\n",
    "    return jnp.reshape(x, (batch_size, -1)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-distribution",
   "metadata": {},
   "source": [
    "The last two functions we'll implement in this section are `relu` and `softmax`.  A couple of tests are also provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dress-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return jnp.clip(x, a_min=0)    \n",
    "\n",
    "x = np.random.randn(10,10)\n",
    "assert np.all(np.isclose(relu(x), tf.nn.relu(x))), 'test failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "endangered-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = jnp.exp(x)\n",
    "    return ex / jnp.sum(ex, axis=-1, keepdims=True)    \n",
    "\n",
    "x = np.random.randn(5, 10)\n",
    "assert np.all(np.isclose(softmax(x), tf.nn.softmax(x))), 'test failed'\n",
    "assert np.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-holmes",
   "metadata": {},
   "source": [
    "### Sequential Model\n",
    "\n",
    "With the supporting pieces implemented, the `Sequential` class can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "atlantic-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_pytree_node_class\n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'tree_flatten'):\n",
    "                self.layers.append(layer)\n",
    "            elif callable(layer):\n",
    "                self.layers.append(Function(layer))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = ''\n",
    "        for layer in self.layers:\n",
    "            string += (repr(layer) + '\\n')\n",
    "        return string\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        aux_data, children = [], []\n",
    "        for layer in self.layers:\n",
    "            params, extra_stuff = jax.tree_flatten(layer)\n",
    "            aux_data.append(extra_stuff)\n",
    "            children.append(params)\n",
    "        return children, aux_data\n",
    "    \n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, aux_data, children):\n",
    "        layers = []\n",
    "        for params, spec in zip(children, aux_data):\n",
    "            layers.append(jax.tree_unflatten(spec, params))\n",
    "        return Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-cattle",
   "metadata": {},
   "source": [
    "Based on the previous high-level description, it should not be surprising that `Sequential` is essentially a wrapper around a list of layers.  The `__init__` method loops through the list of input objects and does one of the following things:\n",
    "\n",
    "1. if it is a pytree, adds the layer to the list of layers\n",
    "2. if it is a function, but not a pytree, wraps the function in a `Function` object and adds the pytree to the list of layers.\n",
    "\n",
    "Either way, at the end of construction, each `Sequential` instance is a list of pytrees.  At this point, `Sequential` is made into a pytree in the usual way: by implementing `tree_flatten` and `tree_unflatten` which loop over the layers, calling each layer's flatten of unflatten method, and collecting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-recipient",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "Cross Entropy is one of the most common loss functions for classification problems.  I'm going to spare you the long-winded mathematical justification of why it's a useful function.  Let me just say that it measures how close two probability distributions are. \n",
    "\n",
    "Here are two different versions of cross-entropy.  The first version (`cross_entropy`) assumes that `y_true` is one-hot encoded while the second version (`sparse_cross_entropy`) assumes that `y_true` is an array of indices.  I like the sparse version because it seems more efficient and less dependent on knowing the number of categories in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "driven-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def cross_entropy(y_true, probs):\n",
    "    batch_size = jnp.shape(probs)[0]\n",
    "    return -jnp.sum(jnp.log(probs + eps) * y_true) / batch_size\n",
    "\n",
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "assert np.all(np.isclose(cross_entropy(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "distant-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sparse_cross_entropy(y_true, probs):\n",
    "    batch_size = len(y_true)\n",
    "    X = jnp.log(probs + eps)[jnp.arange(batch_size), y_true]\n",
    "    return -jnp.sum(X) / batch_size\n",
    "\n",
    "y_true = jnp.array([1, 2])\n",
    "y_pred = jnp.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]])\n",
    "keras_cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "assert np.all(np.isclose(sparse_cross_entropy(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), 'Not close'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-overhead",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Optimizers update model parameters at each minibatch.  The simplest optimizer, stochastic gradient descent (aka SGD), is shown below and updates the parameters ($W$) as follows: \n",
    "\n",
    "$$\n",
    "    W \\leftarrow W - \\alpha \\frac{\\partial \\ell}{\\partial W}\n",
    "$$\n",
    "\n",
    "where $\\ell$ represents the loss-function used in model training and $\\alpha$ is the learning rate.  \n",
    "\n",
    "While certainly simple and fast, SGD is not the *go-to* optmizer these days.  That title seems to go to *Adam*, as it tends to be used in most examples I've seen.  From their original paper, Kingma and Ba describe Adam as \n",
    "\n",
    "> \"an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\"\n",
    "\n",
    "For now, an optimizer has an `__init__` and `step` method.  The `__init__` method initializes the parameters and `step` updates the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "organized-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ruled-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.lr = lr \n",
    "    def step(self, model, grads):\n",
    "        return jax.tree_map(lambda p, g: p - self.lr*g, model, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "saved-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7):\n",
    "        self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps\n",
    "        self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) \n",
    "        self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model)\n",
    "        self.k = 0 \n",
    "    def step(self, model, grad):\n",
    "        lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps\n",
    "        v, s = self.v, self.s\n",
    "        k = self.k = self.k+1\n",
    "        self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grad)\n",
    "        self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grad)\n",
    "        v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v)\n",
    "        s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s)\n",
    "        new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat)\n",
    "        return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-cambridge",
   "metadata": {},
   "source": [
    "This code illustrate why pytree registration is so important.  Consider the following line taken from `Adam`'s `step` method:\n",
    "\n",
    "```python\n",
    "self.v = jax.tree_map(lambda v, g: v_decay*v + (1-v_decay)*g, v, grad)\n",
    "```\n",
    "\n",
    "The passed in `lambda` is a function of two JAX arrays; `v` and `grads` are pytrees.  Internally, `jax.tree_map` executes 3 steps:\n",
    "\n",
    "1. pytrees are flattened\n",
    "2. function called on each item in the flattened pytree\n",
    "3. results unflattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-squad",
   "metadata": {},
   "source": [
    "## Progress Bar\n",
    "\n",
    "I really like how Keras logs information to the screen during model training, and decided to mimic the style.  Here's my version of the progress bar.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "quality-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(percentage, total=30):\n",
    "    x = int(percentage*total)\n",
    "    if x < total:\n",
    "        r = '[' + ''.join(['=']*x) + '>' + ''.join(['.']*(total-x)) + ']' \n",
    "    else:\n",
    "        r = '[' + ''.join(['=']*(total+1)) + ']' \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-sheep",
   "metadata": {},
   "source": [
    "It has space for 31 characters sandwiched between an opening and closing bracket.  Examples at various completion percentages are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-turkey",
   "metadata": {},
   "source": [
    "* 0% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "elder-census",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>..............................]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-pollution",
   "metadata": {},
   "source": [
    "* 10% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sapphire-hollywood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===>...........................]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-missouri",
   "metadata": {},
   "source": [
    "* 100% progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mineral-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================]\n"
     ]
    }
   ],
   "source": [
    "print(progress_bar(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-glance",
   "metadata": {},
   "source": [
    "## The Training Loop  \n",
    "\n",
    "The training loop is pretty simple, but I admit that it looks a little cluttered.  Most frameworks and libraries split the it into several separate functions, and incorporate a callback system that allows a user to customize it's functionality.  Without the flexibility that a callback system offers, you'd probably need to implement a new training loop for every project.  Besides that, callbacks lend themselved to less cluttered (and therefore less buggy) code.  I'll likely work on some semi-simple callback system in the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "computational-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_datasource, valid_datasource, optimizer, loss_fn, grad_fn, model):\n",
    "    history = {'loss':[], 'accuracy':[]}\n",
    "    \n",
    "    if valid_datasource is not None:\n",
    "        history = {**history, 'valid_loss': [], 'valid_accuracy': []}\n",
    "\n",
    "    train_num_batches = len(train_datasource)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # TRAINING PHASE\n",
    "        train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0\n",
    "        \n",
    "        num_steps = 0 \n",
    "        \n",
    "        epoch_duration = 0.0\n",
    "        \n",
    "        # we know how many batches there are ... keep track\n",
    "        for i, (X_train, y_train) in enumerate(train_datasource):\n",
    "            \n",
    "            # logging\n",
    "            batch_start = time.time()\n",
    "\n",
    "            num_steps += 1\n",
    "            \n",
    "            # training loss and gradients for this particular batch\n",
    "            probs = model(X_train)\n",
    "            loss = loss_fn(y_train, probs)\n",
    "            grad = grad_fn(model, X_train, y_train)\n",
    "            model = optimizer.step(model, grad)\n",
    "            \n",
    "            \n",
    "            # Results aggregation\n",
    "            num_correct = jnp.sum(jnp.argmax(probs, axis=-1) == y_train)\n",
    "            train_loss_accum += loss \n",
    "            train_batch_size += len(y_train)\n",
    "            train_accuracy_accum += num_correct\n",
    "            train_accuracy = train_accuracy_accum / train_batch_size\n",
    "            train_loss = train_loss_accum / train_num_batches # average loss per batch\n",
    "\n",
    "            # Logging ....\n",
    "            batch_duration = time.time() - batch_start\n",
    "            epoch_duration += batch_duration \n",
    "            log_batch_count = f'{i}/{train_num_batches}'\n",
    "            log_epoch_time = f'{int(epoch_duration)}s'\n",
    "            log_batch_time = f'{1_000*batch_duration:.2f}ms/batch'\n",
    "            log_batch_loss = f'loss: {train_loss:.4f}'\n",
    "            log_batch_accuracy = f'accuracy: {train_accuracy:.4f}'\n",
    "            log_string =  f'{log_batch_count:<10s} {progress_bar((i+1)/train_num_batches)} - {log_epoch_time:<3s} {log_batch_time:<5s} - {log_batch_loss:<13s} - {log_batch_accuracy:<20s}'\n",
    "            print(log_string, end='\\r') \n",
    "\n",
    "        # \n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_accuracy)      \n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        if valid_datasource is not None:\n",
    "            valid_num_batches = len(valid_datasource)\n",
    "            valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 \n",
    "\n",
    "            # Run validation step ...\n",
    "            for i, (X_valid, y_valid) in enumerate(valid_datasource):\n",
    "                num_steps += 1\n",
    "                probs = model(X_valid)\n",
    "                loss = loss_fn(y_valid, probs)\n",
    "                \n",
    "                valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid)\n",
    "\n",
    "                valid_loss_accum += loss\n",
    "                valid_batch_size += len(y_valid)\n",
    "\n",
    "            epoch_valid_loss = valid_loss_accum / valid_num_batches\n",
    "            epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size\n",
    "            log_valid_loss = f'val_loss: {epoch_valid_loss:.4f}'\n",
    "            log_valid_accuracy = f'val_accuracy: {epoch_valid_accuracy:.4f}'\n",
    "            log_string += f' - {log_valid_loss:<13s} - {log_valid_accuracy:<20s}' \n",
    "            \n",
    "            history['loss'].append(epoch_valid_loss)\n",
    "            history['accuracy'].append(epoch_valid_accuracy)\n",
    "        \n",
    "        # this log_string should include validation results\n",
    "        print(log_string, end='\\n')\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-collins",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Okay, time to put everything together and train the model.  One thing that might require a little explanation is the line\n",
    "\n",
    "```python\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X))))\n",
    "```\n",
    "\n",
    "The inner `lambda` is the function that uses the model, image data, and labels to calculate the cross-entropy loss.  By default `jax.grad` will calculate the gradient with respect to this function's first argument, `model` (or equivalently it's flattened pytree representation).  This is precisely what we want.  To speed up the calculation the gradient is compiled with `jax.jit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sustainable-franchise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1874/1875  [===============================] - 3s  1.51ms/batch - loss: 0.4956  - accuracy: 0.8250     - val_loss: 0.4446 - val_accuracy: 0.8396\n",
      "Epoch 2/5\n",
      "1874/1875  [===============================] - 2s  1.31ms/batch - loss: 0.3716  - accuracy: 0.8654     - val_loss: 0.4271 - val_accuracy: 0.8446\n",
      "Epoch 3/5\n",
      "1874/1875  [===============================] - 2s  1.49ms/batch - loss: 0.3330  - accuracy: 0.8791     - val_loss: 0.4034 - val_accuracy: 0.8528\n",
      "Epoch 4/5\n",
      "1874/1875  [===============================] - 2s  1.48ms/batch - loss: 0.3079  - accuracy: 0.8871     - val_loss: 0.3797 - val_accuracy: 0.8616\n",
      "Epoch 5/5\n",
      "1874/1875  [===============================] - 2s  1.38ms/batch - loss: 0.2891  - accuracy: 0.8935     - val_loss: 0.3599 - val_accuracy: 0.8710\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "valid_dataset = Dataset(X_test, y_test)\n",
    "train_datasource = Dataloader(train_dataset, batchsize=32)\n",
    "valid_datasource = Dataloader(valid_dataset, batchsize=32)\n",
    "\n",
    "network = Sequential(\n",
    "    rescale_image,\n",
    "    flatten,\n",
    "    Linear(28*28, 128),\n",
    "    relu,\n",
    "    Linear(128, 10),\n",
    "    softmax  \n",
    ")\n",
    "\n",
    "grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X))))\n",
    "\n",
    "history = train(\n",
    "    num_epochs=5, \n",
    "    train_datasource=train_datasource, \n",
    "    valid_datasource=valid_datasource,\n",
    "    optimizer=Adam(network, lr=1e-3), \n",
    "    loss_fn=sparse_cross_entropy, \n",
    "    model=network,\n",
    "    grad_fn=grad_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-edition",
   "metadata": {},
   "source": [
    "## Keras Execution\n",
    "\n",
    "For comparison, here's the training results for Keras on the same dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "environmental-controversy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 988us/step - loss: 0.4928 - accuracy: 0.8269 - val_loss: 0.4533 - val_accuracy: 0.8408\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 930us/step - loss: 0.3749 - accuracy: 0.8657 - val_loss: 0.4262 - val_accuracy: 0.8467\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 947us/step - loss: 0.3381 - accuracy: 0.8777 - val_loss: 0.3721 - val_accuracy: 0.8667\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 932us/step - loss: 0.3130 - accuracy: 0.8853 - val_loss: 0.3600 - val_accuracy: 0.8677\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 918us/step - loss: 0.2956 - accuracy: 0.8909 - val_loss: 0.3501 - val_accuracy: 0.8738\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() \n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1/255.0),\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)                          \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-indicator",
   "metadata": {},
   "source": [
    "I'm pretty pleased with how closely my output follows the Keras output. The reason there isn't an even better match possibly stems from different parameter initialization techniques.  I'm currently limited to Kaiming initialization, whereas Dense layers in Keras use Kaiming uniform by default.  I could test this hypothesis by directly loading the parameters in my model with the Keras parameters.  Maybe I'll try this in another day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-token",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "JAX is a potent piece of technology.  I'm planning to use it, and explore how it works, in more detail over the coming months.  Overtime, I hope to take the parts and pieces implemented here and build a more cohesive library that can be used to solve problems that interest me.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9eb3d55570fa83a5d0e75ab0fc9e29d7692aa2022c5c86ae3b4c36003072d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
