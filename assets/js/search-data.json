{
  
    
        "post0": {
            "title": "Checking Understanding of Convolutional and Pooling Layers",
            "content": "Introduction . The purpose of this post is to make sure I understand how convolutional and pooling layers work. Once again, I&#39;ll use Keras to double check all my work. . Import Libraries . import jax import jax.numpy as jnp import numpy as np import tensorflow as tf import pandas as pd . Here&#39;s a small sequential model consisting of a convolutional layer and max-pooling layer. . model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(filters=4, kernel_size=(2, 2), strides=(2,2), padding=&#39;VALID&#39;), tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)) ]) . For this experiment, I don&#39;t care what the model input is. So I&#39;ll apply model to a 1-element batch consisting of a random 28-by-28 array with 3 channels. As you can see, outputs is a 1-element batch consisting of a 7-by-7 array with 4 channels. Here&#39;s a little table comapring the shape of inputs and outputs. . inputs = np.random.randn(1,28,28,3) outputs = model(inputs) print(f&#39;Feature Mapping: {inputs.shape} -&gt; {outputs.shape}&#39;) . Feature Mapping: (1, 28, 28, 3) -&gt; (1, 7, 7, 4) . Getting the pooling layer output is easy; it&#39;s the same as outputs. However, it&#39;s not immediately obvious how to get the intermediate convolution output. tion, the recommended way to extract the outputs from all the layers in your model is to use the so-called Functional API. . features from all layers in the model: basically use Keras&#39; functional API to build a model that returns the outputs from each layer, given an input batch. . outputs = [layer.output for layer in model.layers] layer_output_model = tf.keras.Model(inputs=model.input, outputs=outputs) keras_features = layer_output_model(inputs) . Because model has two layers, the keras_features array has two elements. Here are the shapes of each: . print(f&#39;Conv2D output shape = {keras_features[0].shape}&#39;) print(f&#39;MaxPool2D output shape = {keras_features[1].shape}&#39;) . Conv2D output shape = (1, 14, 14, 4) MaxPool2D output shape = (1, 7, 7, 4) . kernels, biases = model.layers[0].get_weights() print(f&#39;kernels shape = {kernels.shape}, biases shape = {biases.shape}&#39;) . kernels shape = (2, 2, 3, 4), biases shape = (4,) . Here&#39;s a faily inefficient way to duplicate the evaluation of the conv_layer defined in above. . Convolutional Layer . Convolution is essentially a rolling dot-product, and this is exactly how the implementation below works. For now, conv2d is a function of a single, multi-channel image x, kernel is one of the output filters, and strides is a pair of integers defining the number of vertical and horizontal positions . I don&#39;t have a good technical reason for making the output array y one dimensional and reshaping at the end; it just looks cleaner than the alternatives. . def conv2d(x, kernel, strides): xm, xn, _ = x.shape km, kn, _ = kernel.shape sm, sn = strides ym, yn = 1 + ((xm - km + 1)//sm), 1 + ((xn - kn + 1)//sn) y = np.zeros(ym*yn) k = 0 for i in range(0, xm-km+1, sm): for j in range(0, xn-kn+1, sn): y[k] = np.sum(kernel * x[i:i+km,j:j+kn,:]) k += 1 return np.reshape(y, (ym, yn)) . class MyConv2D: def __init__(self, w, b, strides): self.w = w self.b = b self.strides = strides def __call__(self, inputs): biases, kernels, strides = self.b, self.w, self.strides num_output_channels = len(biases) # get the first image inputs = inputs[0,...] # get the list of output feature maps outputs = [conv2d(inputs, kernels[...,i], strides) + biases[i] for i in range(num_output_channels)] # horizontally stacking the 2D images outputs = np.stack(outputs, axis=-1) # Add the batch dimension outputs = outputs[np.newaxis,...] return outputs . Check . kernels, biases = model.layers[0].get_weights() strides = model.layers[0].strides keras_conv_features = keras_features[0] . my_layer = MyConv2D(kernels, biases, strides=strides) my_conv_features = my_layer(inputs) . jax_result = jax_conv2d( lhs=inputs, rhs=kernels, window_strides=strides, padding=&#39;valid&#39;, dimension_numbers=(&#39;NHWC&#39;, &#39;HWIO&#39;, &#39;NHWC&#39;) ) . (1, 14, 14, 4) . assert np.all(np.isclose(keras_conv_features, my_conv_features, atol=1e-6)) assert np.all(np.isclose(jax_result, my_conv_features, atol=1e-6)) . Convolution in JAX . a = jnp.zeros((2,3)) np.array(a) . array([[0., 0., 0.], [0., 0., 0.]], dtype=float32) . jax_result.shape . (1, 14, 14, 4) . jax_conv2d = jax.lax.conv_general_dilated jax_conv2d?? . Signature: jax_conv2d( lhs: Any, rhs: Any, window_strides: Sequence[int], padding: Union[str, Sequence[Tuple[int, int]]], lhs_dilation: Union[Sequence[int], NoneType] = None, rhs_dilation: Union[Sequence[int], NoneType] = None, dimension_numbers: Union[NoneType, jax._src.lax.convolution.ConvDimensionNumbers, Tuple[str, str, str]] = None, feature_group_count: int = 1, batch_group_count: int = 1, precision: Union[NoneType, str, jax._src.lax.lax.Precision, Tuple[str, str], Tuple[jax._src.lax.lax.Precision, jax._src.lax.lax.Precision]] = None, preferred_element_type: Union[Any, NoneType] = None, ) -&gt; Any Source: def conv_general_dilated( lhs: Array, rhs: Array, window_strides: Sequence[int], padding: Union[str, Sequence[Tuple[int, int]]], lhs_dilation: Optional[Sequence[int]] = None, rhs_dilation: Optional[Sequence[int]] = None, dimension_numbers: ConvGeneralDilatedDimensionNumbers = None, feature_group_count: int = 1, batch_group_count: int = 1, precision: lax.PrecisionLike = None, preferred_element_type: Optional[DType] = None) -&gt; Array: &#34;&#34;&#34;General n-dimensional convolution operator, with optional dilation. Wraps XLA&#39;s `Conv &lt;https://www.tensorflow.org/xla/operation_semantics#conv_convolution&gt;`_ operator. Args: lhs: a rank `n+2` dimensional input array. rhs: a rank `n+2` dimensional array of kernel weights. window_strides: a sequence of `n` integers, representing the inter-window strides. padding: either the string `&#39;SAME&#39;`, the string `&#39;VALID&#39;`, or a sequence of `n` `(low, high)` integer pairs that give the padding to apply before and after each spatial dimension. lhs_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of `lhs`. LHS dilation is also known as transposed convolution. rhs_dilation: `None`, or a sequence of `n` integers, giving the dilation factor to apply in each spatial dimension of `rhs`. RHS dilation is also known as atrous convolution. dimension_numbers: either `None`, a ``ConvDimensionNumbers`` object, or a 3-tuple ``(lhs_spec, rhs_spec, out_spec)``, where each element is a string of length `n+2`. feature_group_count: integer, default 1. See XLA HLO docs. batch_group_count: integer, default 1. See XLA HLO docs. precision: Optional. Either ``None``, which means the default precision for the backend, a :class:`~jax.lax.Precision` enum value (``Precision.DEFAULT``, ``Precision.HIGH`` or ``Precision.HIGHEST``), a string (e.g. &#39;highest&#39; or &#39;fastest&#39;, see the ``jax.default_matmul_precision`` context manager), or a tuple of two :class:`~jax.lax.Precision` enums or strings indicating precision of ``lhs`` and ``rhs``. preferred_element_type: Optional. Either ``None``, which means the default accumulation type for the input types, or a datatype, indicating to accumulate results to and return a result with that datatype. Returns: An array containing the convolution result. In the string case of ``dimension_numbers``, each character identifies by position: - the batch dimensions in ``lhs``, ``rhs``, and the output with the character &#39;N&#39;, - the feature dimensions in `lhs` and the output with the character &#39;C&#39;, - the input and output feature dimensions in rhs with the characters &#39;I&#39; and &#39;O&#39; respectively, and - spatial dimension correspondences between lhs, rhs, and the output using any distinct characters. For example, to indicate dimension numbers consistent with the ``conv`` function with two spatial dimensions, one could use ``(&#39;NCHW&#39;, &#39;OIHW&#39;, &#39;NCHW&#39;)``. As another example, to indicate dimension numbers consistent with the TensorFlow Conv2D operation, one could use ``(&#39;NHWC&#39;, &#39;HWIO&#39;, &#39;NHWC&#39;)``. When using the latter form of convolution dimension specification, window strides are associated with spatial dimension character labels according to the order in which the labels appear in the ``rhs_spec`` string, so that ``window_strides[0]`` is matched with the dimension corresponding to the first character appearing in rhs_spec that is not ``&#39;I&#39;`` or ``&#39;O&#39;``. If ``dimension_numbers`` is ``None``, the default is ``(&#39;NCHW&#39;, &#39;OIHW&#39;, &#39;NCHW&#39;)`` (for a 2D convolution). &#34;&#34;&#34; dnums = conv_dimension_numbers(lhs.shape, rhs.shape, dimension_numbers) if lhs_dilation is None: lhs_dilation = (1,) * (lhs.ndim - 2) elif isinstance(padding, str) and not len(lhs_dilation) == lhs_dilation.count(1): raise ValueError( &#34;String padding is not implemented for transposed convolution &#34; &#34;using this op. Please either exactly specify the required padding or &#34; &#34;use conv_transpose.&#34;) if rhs_dilation is None: rhs_dilation = (1,) * (rhs.ndim - 2) if isinstance(padding, str): lhs_perm, rhs_perm, _ = dnums rhs_shape = np.take(rhs.shape, rhs_perm)[2:] # type: ignore[index] effective_rhs_shape = [(k-1) * r + 1 for k, r in zip(rhs_shape, rhs_dilation)] padding = lax.padtype_to_pads( np.take(lhs.shape, lhs_perm)[2:], effective_rhs_shape, # type: ignore[index] window_strides, padding) else: try: padding = tuple((operator.index(lo), operator.index(hi)) for lo, hi in padding) except (ValueError, TypeError) as e: raise ValueError( &#34;padding argument to conv_general_dilated should be a string or a &#34; f&#34;sequence of (low, high) pairs, got {padding}&#34;) from e preferred_element_type = ( None if preferred_element_type is None else dtypes.canonicalize_dtype(np.dtype(preferred_element_type))) return conv_general_dilated_p.bind( lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding), lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation), dimension_numbers=dnums, feature_group_count=feature_group_count, batch_group_count=batch_group_count, lhs_shape=lhs.shape, rhs_shape=rhs.shape, precision=lax.canonicalize_precision(precision), preferred_element_type=preferred_element_type) File: ~/anaconda3/lib/python3.8/site-packages/jax/_src/lax/convolution.py Type: function . Pooling Layer . pooling_layer = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)) . yy = pooling_layer(feature_maps) print(f&#39;{feature_maps.shape} -&gt; {yy.shape}&#39;) . (1, 3, 3, 2) -&gt; (1, 1, 1, 2) . yy . &lt;tf.Tensor: shape=(1, 1, 1, 2), dtype=float32, numpy=array([[[[0.9654248, 1.2988867]]]], dtype=float32)&gt; . print(feature_maps) . tf.Tensor( [[[[ 0.89900464 -1.0616233 ] [-0.10247962 1.2988867 ] [-0.8515937 0.6712394 ]] [[ 0.9654248 0.23607667] [-0.43404913 0.27193436] [-0.5712364 -0.73882663]] [[ 0.6847748 -0.8419535 ] [ 0.46285468 0.7197448 ] [-1.3353215 0.47066653]]]], shape=(1, 3, 3, 2), dtype=float32) . print(yy) . tf.Tensor( [[[[ 0.9654248 1.2988867 ] [-0.10247962 1.2988867 ]] [[ 0.9654248 0.7197448 ] [ 0.46285468 0.7197448 ]]]], shape=(1, 2, 2, 2), dtype=float32) . print(feature_maps[0,:,:,1]) . tf.Tensor( [[-1.0616233 1.2988867 0.6712394 ] [ 0.23607667 0.27193436 -0.73882663] [-0.8419535 0.7197448 0.47066653]], shape=(3, 3), dtype=float32) . yy[0,:,:,1] . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1.2988867, 1.2988867], [0.7197448, 0.7197448]], dtype=float32)&gt; . input_batch.shape . (1, 4, 4, 3) . list(range(0,10,2)) . [0, 2, 4, 6, 8] . v = np.zeros((3,3)) v[0] . array([0., 0., 0.]) . def pool2D(x, pool_size=(2,2), strides=(1,1), fn=np.max): xm, xn = x.shape pm, pn = pool_size sm, sn = strides ym, yn = 1 + (xm-pm+1) // sm, 1 + (xn-pn+1) // sn y = np.zeros((ym, yn)) ii = 0 for i in range(0, xm-pm+1, sm): jj = 0 for j in range(0, xn-pn+1, sn): y[ii,jj] = fn(x[i:i+pm,j:j+pn]) jj += 1 ii += 1 return y . x = np.random.randn(3,3) b = pool2D(x, strides=(2,2)) print(a) print(b) print(x) . NameError Traceback (most recent call last) /tmp/ipykernel_243202/2927589611.py in &lt;module&gt; 1 x = np.random.randn(3,3) 2 b = pool2D(x, strides=(2,2)) -&gt; 3 print(a) 4 print(b) 5 print(x) NameError: name &#39;a&#39; is not defined . def pooling(features, pool_size=(2,2), strides=(2,2)): px, py = pool_size sm, sn = strides width, height, chans = features.shape m, n = (width - px + 1) // sm, (height - py + 1) // sn features_ = np.zeros((m, n, chans)) # Note that we&#39;re not changing the number of features for chan in range(chans): features_[:,:,chan] = pool2D(features[:,:,chan], pool_size, strides) return features_ . pooling(feature_maps[0,:,:,:]) . array([[[ 1.3011235 , -0.24245605]]]) . np.stack([np.random.randn(10,10), np.random.randn(10,10)], axis=-1).shape . (10, 10, 2) .",
            "url": "https://jrreed83.github.io/technicalities/jax/convolution/pooling/2022/07/26/jax-01-how-cnns-work.html",
            "relUrl": "/jax/convolution/pooling/2022/07/26/jax-01-how-cnns-work.html",
            "date": " • Jul 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fashion MNIST with Vanilla JAX",
            "content": "Introduction . In this post, I&#39;m going to implement a basic Fashion-MNIST classifier using JAX. JAX is an array-processing library that uses Google&#39;s XLA (Accelerated Linear Algebra) compiler to generate high-performance code that can run on a variety of hardware platforms. It feels a lot like numpy, with a number of advantages including built in automatic differentiation, vectorization and parallelization, and just-in-time compilation. . By the end of this write up, I&#39;m going to end up with this: . (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() train_dataset = Dataset(X_train, y_train) valid_dataset = Dataset(X_test, y_test) train_datasource = Dataloader(train_dataset, batchsize=32) valid_datasource = Dataloader(valid_dataset, batchsize=32) network = Sequential( rescale_image, flatten, Linear(28*28, 128), relu, Linear(128, 10), softmax ) grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X)))) history = train( num_epochs=5, train_datasource=train_datasource, valid_datasource=valid_datasource, optimizer=Adam(network, lr=1e-3), loss_fn=sparse_cross_entropy, model=network, grad_fn=grad_fn ) . Epoch 1/5 1874/1875 [===============================] - 2s 1.65ms/batch - loss: 0.4956 - accuracy: 0.8250 - val_loss: 0.4446 - val_accuracy: 0.8396 Epoch 2/5 1874/1875 [===============================] - 2s 1.40ms/batch - loss: 0.3716 - accuracy: 0.8654 - val_loss: 0.4271 - val_accuracy: 0.8446 Epoch 3/5 1874/1875 [===============================] - 2s 1.39ms/batch - loss: 0.3330 - accuracy: 0.8791 - val_loss: 0.4034 - val_accuracy: 0.8528 Epoch 4/5 1874/1875 [===============================] - 2s 1.46ms/batch - loss: 0.3079 - accuracy: 0.8871 - val_loss: 0.3797 - val_accuracy: 0.8616 Epoch 5/5 1874/1875 [===============================] - 2s 1.97ms/batch - loss: 0.2891 - accuracy: 0.8935 - val_loss: 0.3599 - val_accuracy: 0.8710 . It&#39;s pretty standard stuff. Grab some data, prepare the data for model training and validation, build a network with a commonly used sequential API, and finally train the model. Besides using the Keras datasets library, everything will be written from scratch. My goal is to demonstrate how simple it is to build the begininnings of a deep-learning library when you&#39;re working with the right set of tools. . Load Libraries . import jax import jax.numpy as jnp import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import time from typing import Tuple, List, Any, Dict, Callable eps = jnp.finfo(jnp.float64).eps . Loading the Data . The Dataset and Dataloader classes shown below are stripped down versions of the Pytorch versions. For now, a Dataset is a container for JAX feature and label arrays. In fact, besides having a length ( because it has a __len__ method), it&#39;s pretty useless. The Dataloader gives you the ability to iterate over batches of data from a Dataset, which is very important for effective model training. . class Dataset: def __init__(self, X, y): self.X, self.y = X, y def __len__(self): return jnp.shape(self.X)[0] def __getitem__(self, i): return self.X[i,:], self.y[i] . class Dataloader: def __init__(self, dataset: Dataset, batchsize=32, shuffle=False): self.dataset = dataset self.batchsize = batchsize self.shuffle = shuffle def __iter__(self): for i in range(0, len(self.dataset), self.batchsize): yield self.dataset[i:i+self.batchsize] def __len__(self): return len(self.dataset) // self.batchsize . Let&#39;s go through an example of using Dataset and Dataloader. First grab the Fashion MNIST data using the Keras dataset library. . (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() print(f&#39;Number training examples = {len(y_train)}&#39;) print(f&#39;Number testing examples = {len(y_test)}&#39;) . Number training examples = 60000 Number testing examples = 10000 . There are ten articles of clothing represented in the dataset. Here&#39;s the array that map an index to its name. . class_labels = [ &#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39; ] . Now let&#39;s put the training data into a dataloader configured to generate batches with 8 images, and show the first set of images with their labels. First I need to import the Image module from the PIL library. Looping over dataloader will give you all 8-image batches in the training set. Putting a break statement at the end of the loop, we&#39;re limited to looking at the first batch. . from PIL import Image dataloader = Dataloader(Dataset(X_train, y_train), batchsize=8) for X_batch, y_batch in dataloader: plt.figure(1, figsize=(20,5)) for i, (X, y) in enumerate(zip(X_batch, y_batch)): img = Image.fromarray(X) plt.subplot(2,4,i+1) plt.imshow(img, cmap=&#39;gray&#39;) plt.title(f&#39;{class_labels[y]} / {y}&#39;) plt.tight_layout() break . The Sequential Model . As was shown above, we&#39;re going to build a sequential model. It is made from a list of Callable objects that is evaluated by calling each members __call__ method in order. Each list element is a registered pytree, but I also wanted to have the flexibility to pass ordinary functions to the Sequential constructor and have everything just work. As you&#39;ll soon see this feature was implemented by modifying the __init__ method. Before building Sequential though, we need some basic layers. . Linear Layer . The Linear layer defined below is very similar to implementations (but less general) you&#39;d find in other non-JAX neural network libraries. . @jax.tree_util.register_pytree_node_class class Linear: w: jnp.ndarray b: jnp.ndarray ni: int no: int def __init__(self, num_inputs, num_outputs, build=True, seed=1234): self.ni = num_inputs self.no = num_outputs # want to add seed as internal object if build: key = jax.random.PRNGKey(seed) self.w = jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2.0 / num_inputs) self.b = jnp.zeros(num_outputs) def __repr__(self): return f&#39;Linear(num_inputs={self.ni}, num_outputs={self.no})&#39; def __call__(self, x): return jnp.dot(x, self.w) + self.b def params(self): return {&#39;w&#39;: self.w, &#39;b&#39;: self.b} def tree_flatten(self): return (self.w, self.b), (self.ni, self.no) @classmethod def tree_unflatten(cls, aux_data, children): layer = cls(*aux_data, build=False) layer.w, layer.b = children return layer . The one glaring difference is the decorator and the two tree methods. As the JAX documentation explains, the register_pytree_node_class, tree_flatten method, and tree_unflatten class methods are required to make a user-defined class into a pytree. Once added, JAX will know how to transform back and forth between objects that the rest of JAX system can efficiently operate on, and objects that are specific to your application. . The tree_flatten method returns a two-element tuple consisting of the parameters you want to expose to JAX, and any meta-data that can help reconstruct the object. Because JAX embraces the functional paradigm of immutable data structures, I thought it might be better to express the parameters as a tuple. For Linear, the parameters are the weights and biases of the neural network. For now, the only meta-data that seems helpful are the number of inputs and outputs (although this could be derived from the shape of the weights. . Another noteworthy thing about Linear is the build attribute. Most of the time, you want to initialize the weights and biases at creation time time. However, you don&#39;t want to do this when JAX reconstructs the object from it&#39;s flattened representation. You probably just want to plop the parameters right into a freshly constructed object. The build attribute gives you some flexibility in that regard. . Let&#39;s take a look at a small example to see what we have. Here&#39;s a linear layer that maps 5-element arrays to 1-element arrays . lin = Linear(5, 1) . Because Linear is a pytree, it can be passed to jax.tree_flatten to get it&#39;s weights and biases, and it&#39;s metadata: . params, metadata = jax.tree_flatten(lin) print(f&#39;weights = {params[0]}, biases = {params[1]}&#39;) print(f&#39;metadata = {metadata}&#39;) . weights = [[ 0.5734188 ] [-0.597884 ] [ 0.05189713] [-1.1660179 ] [ 0.29061902]], biases = [0.] metadata = PyTreeDef(CustomNode(&lt;class &#39;__main__.Linear&#39;&gt;[(5, 1)], [*, *])) . Finally, the metadata and parameters from jax.tree_flatten can be used to generate a clone of the lin: . lin2 = jax.tree_unflatten(metadata, params) . Just to make sure that the weights, biases, and model outputs match: . assert jnp.all(jnp.isclose(lin.w, lin2.w)) assert jnp.all(jnp.isclose(lin.b, lin2.b)) x = np.random.randn(10,5) assert jnp.all(jnp.isclose(lin(x), lin2(x))) . Function Layer . The Function class fills the same need that Lambda layers do in Keras: being able to conveniently plug functions into models. As the tree_flatten method shows, classes registered as pytrees can be parameter-free. . @jax.tree_util.register_pytree_node_class class Function: def __init__(self, fn): self.fn = fn def __call__(self, x): return self.fn(x) def __repr__(self): return f&#39;Function({self.fn.__name__})&#39; def tree_flatten(self): return [], self.fn @classmethod def tree_unflatten(cls, aux_data, children): return cls(aux_data) . Helpful Functions . Here are a few functions that will be lifted to Function layer in the Sequential model. One common approach for improving classification accuracy is to normalize your input data. When working with gray-scale images, this typically means rescaling the pixels from $[0,255]$ to $[0,1]$. This is what rescale_image does. . def rescale_image(x): return x / 255.0 . The model built in this post operates on batches of two-dimensional gray-scale images. Each batch is a three-dimensional array and can be interpreted as a vertical stack of 2D images, where the height of the vertical stack is the number of images. The flatten goes through each slice of the vertical stack and transforms the 2D array into a one-dimensional array. In the process, the 3D input becomes a 2D array. . def flatten(x): shape = jnp.shape(x) assert len(shape) == 3, &#39;x must represent a batch of two-dimensional gray-scale images&#39; batch_size = shape[0] return jnp.reshape(x, (batch_size, -1)) . The last two functions we&#39;ll implement in this section are relu and softmax. A couple of tests are also provided. . def relu(x): return jnp.clip(x, a_min=0) x = np.random.randn(10,10) assert np.all(np.isclose(relu(x), tf.nn.relu(x))), &#39;test failed&#39; . def softmax(x): ex = jnp.exp(x) return ex / jnp.sum(ex, axis=-1, keepdims=True) x = np.random.randn(5, 10) assert np.all(np.isclose(softmax(x), tf.nn.softmax(x))), &#39;test failed&#39; assert np.isclose(jnp.sum(softmax(x)), jnp.shape(x)[0]) . Sequential Model . With the supporting pieces implemented, the Sequential class can be defined. . @jax.tree_util.register_pytree_node_class class Sequential: def __init__(self, *layers): self.layers = [] for layer in layers: if hasattr(layer, &#39;tree_flatten&#39;): self.layers.append(layer) elif callable(layer): self.layers.append(Function(layer)) def __call__(self, x): for layer in self.layers: x = layer(x) return x def __repr__(self): string = &#39;&#39; for layer in self.layers: string += (repr(layer) + &#39; n&#39;) return string def tree_flatten(self): aux_data, children = [], [] for layer in self.layers: params, extra_stuff = jax.tree_flatten(layer) aux_data.append(extra_stuff) children.append(params) return children, aux_data @classmethod def tree_unflatten(cls, aux_data, children): layers = [] for params, spec in zip(children, aux_data): layers.append(jax.tree_unflatten(spec, params)) return Sequential(*layers) . Based on the previous high-level description, it should not be surprising that Sequential is essentially a wrapper around a list of layers. The __init__ method loops through the list of input objects and does one of the following things: . if it is a pytree, adds the layer to the list of layers | if it is a function, but not a pytree, wraps the function in a Function object and adds the pytree to the list of layers. | Either way, at the end of construction, each Sequential instance is a list of pytrees. At this point, Sequential is made into a pytree in the usual way: by implementing tree_flatten and tree_unflatten which loop over the layers, calling each layer&#39;s flatten of unflatten method, and collecting the results. . Cross Entropy Loss . Cross Entropy is one of the most common loss functions for classification problems. I&#39;m going to spare you the long-winded mathematical justification of why it&#39;s a useful function. Let me just say that it measures how close two probability distributions are. . Here are two different versions of cross-entropy. The first version (cross_entropy) assumes that y_true is one-hot encoded while the second version (sparse_cross_entropy) assumes that y_true is an array of indices. I like the sparse version because it seems more efficient and less dependent on knowing the number of categories in the dataset. . @jax.jit def cross_entropy(y_true, probs): batch_size = jnp.shape(probs)[0] return -jnp.sum(jnp.log(probs + eps) * y_true) / batch_size y_true = np.array([[0, 1, 0], [0, 0, 1]]) y_pred = np.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]]) keras_cross_entropy = tf.keras.losses.CategoricalCrossentropy() assert np.all(np.isclose(cross_entropy(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), &#39;Not close&#39; . @jax.jit def sparse_cross_entropy(y_true, probs): batch_size = len(y_true) X = jnp.log(probs + eps)[jnp.arange(batch_size), y_true] return -jnp.sum(X) / batch_size y_true = jnp.array([1, 2]) y_pred = jnp.array([[0.05, 0.95, 0.0], [0.1, 0.8, 0.1]]) keras_cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy() assert np.all(np.isclose(sparse_cross_entropy(y_true, y_pred), keras_cross_entropy(y_true, y_pred))), &#39;Not close&#39; . Optimizers . Optimizers update model parameters at each minibatch. The simplest optimizer, stochastic gradient descent (aka SGD), is shown below and updates the parameters ($W$) as follows: . $$ W leftarrow W - alpha frac{ partial ell}{ partial W} $$where $ ell$ represents the loss-function used in model training and $ alpha$ is the learning rate. . While certainly simple and fast, SGD is not the go-to optmizer these days. That title seems to go to Adam, as it tends to be used in most examples I&#39;ve seen. From their original paper, Kingma and Ba describe Adam as . &quot;an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.&quot; . For now, an optimizer has an __init__ and step method. The __init__ method initializes the parameters and step updates the model parameters. . class Optimizer: pass . class SGD(Optimizer): def __init__(self, model, lr=1e-3): self.lr = lr def step(self, model, grads): return jax.tree_map(lambda p, g: p - self.lr*g, model, grads) . class Adam(Optimizer): def __init__(self, model, lr=1e-3, v_decay=0.9, s_decay=0.999, eps=1e-7): self.lr, self.v_decay, self.s_decay, self.eps = lr, v_decay, s_decay, eps self.v = jax.tree_map(lambda x: jnp.zeros_like(x), model) self.s = jax.tree_map(lambda x: jnp.zeros_like(x), model) self.k = 0 def step(self, model, grad): lr, v_decay, s_decay, eps = self.lr, self.v_decay, self.s_decay, self.eps v, s = self.v, self.s k = self.k = self.k+1 self.v = jax.tree_map(lambda v, g: v_decay*v +(1-v_decay)*g, v, grad) self.s = jax.tree_map(lambda s, g: s_decay*s +(1-s_decay)*g*g, s, grad) v_hat = jax.tree_map(lambda v: v / (1-v_decay**k), self.v) s_hat = jax.tree_map(lambda s: s / (1-s_decay**k), self.s) new_model = jax.tree_map(lambda params, v_hat, s_hat: params - (lr*v_hat)/(jnp.sqrt(s_hat) + eps), model, v_hat, s_hat) return new_model . This code illustrate why pytree registration is so important. Consider the following line taken from Adam&#39;s step method: . self.v = jax.tree_map(lambda v, g: v_decay*v + (1-v_decay)*g, v, grad) . The passed in lambda is a function of two JAX arrays; v and grads are pytrees. Internally, jax.tree_map executes 3 steps: . pytrees are flattened | function called on each item in the flattened pytree | results unflattened | Progress Bar . I really like how Keras logs information to the screen during model training, and decided to mimic the style. Here&#39;s my version of the progress bar. . def progress_bar(percentage, total=30): x = int(percentage*total) if x &lt; total: r = &#39;[&#39; + &#39;&#39;.join([&#39;=&#39;]*x) + &#39;&gt;&#39; + &#39;&#39;.join([&#39;.&#39;]*(total-x)) + &#39;]&#39; else: r = &#39;[&#39; + &#39;&#39;.join([&#39;=&#39;]*(total+1)) + &#39;]&#39; return r . It has space for 31 characters sandwiched between an opening and closing bracket. Examples at various completion percentages are shown below. . 0% progress | . print(progress_bar(0)) . [&gt;..............................] . 10% progress | . print(progress_bar(0.1)) . [===&gt;...........................] . 100% progress | . print(progress_bar(1)) . [===============================] . The Training Loop . The training loop is pretty simple, but I admit that it looks a little cluttered. Most frameworks and libraries split the it into several separate functions, and incorporate a callback system that allows a user to customize it&#39;s functionality. Without the flexibility that a callback system offers, you&#39;d probably need to implement a new training loop for every project. Besides that, callbacks lend themselved to less cluttered (and therefore less buggy) code. I&#39;ll likely work on some semi-simple callback system in the future. . def train(num_epochs, train_datasource, valid_datasource, optimizer, loss_fn, grad_fn, model): history = {&#39;loss&#39;:[], &#39;accuracy&#39;:[]} if valid_datasource is not None: history = {**history, &#39;valid_loss&#39;: [], &#39;valid_accuracy&#39;: []} train_num_batches = len(train_datasource) for epoch in range(num_epochs): print(f&#39;Epoch {epoch+1}/{num_epochs}&#39;) # TRAINING PHASE train_loss_accum, train_accuracy_accum, train_batch_size = 0, 0, 0 num_steps = 0 epoch_duration = 0.0 # we know how many batches there are ... keep track for i, (X_train, y_train) in enumerate(train_datasource): # logging batch_start = time.time() num_steps += 1 # training loss and gradients for this particular batch probs = model(X_train) loss = loss_fn(y_train, probs) grad = grad_fn(model, X_train, y_train) model = optimizer.step(model, grad) # Results aggregation num_correct = jnp.sum(jnp.argmax(probs, axis=-1) == y_train) train_loss_accum += loss train_batch_size += len(y_train) train_accuracy_accum += num_correct train_accuracy = train_accuracy_accum / train_batch_size train_loss = train_loss_accum / train_num_batches # average loss per batch # Logging .... batch_duration = time.time() - batch_start epoch_duration += batch_duration log_batch_count = f&#39;{i}/{train_num_batches}&#39; log_epoch_time = f&#39;{int(epoch_duration)}s&#39; log_batch_time = f&#39;{1_000*batch_duration:.2f}ms/batch&#39; log_batch_loss = f&#39;loss: {train_loss:.4f}&#39; log_batch_accuracy = f&#39;accuracy: {train_accuracy:.4f}&#39; log_string = f&#39;{log_batch_count:&lt;10s} {progress_bar((i+1)/train_num_batches)} - {log_epoch_time:&lt;3s} {log_batch_time:&lt;5s} - {log_batch_loss:&lt;13s} - {log_batch_accuracy:&lt;20s}&#39; print(log_string, end=&#39; r&#39;) # history[&#39;loss&#39;].append(train_loss) history[&#39;accuracy&#39;].append(train_accuracy) # VALIDATION PHASE if valid_datasource is not None: valid_num_batches = len(valid_datasource) valid_loss_accum, valid_accuracy_accum, valid_batch_size = 0, 0, 0 # Run validation step ... for i, (X_valid, y_valid) in enumerate(valid_datasource): num_steps += 1 probs = model(X_valid) loss = loss_fn(y_valid, probs) valid_accuracy_accum += jnp.sum(jnp.argmax(probs, axis=-1) == y_valid) valid_loss_accum += loss valid_batch_size += len(y_valid) epoch_valid_loss = valid_loss_accum / valid_num_batches epoch_valid_accuracy = valid_accuracy_accum / valid_batch_size log_valid_loss = f&#39;val_loss: {epoch_valid_loss:.4f}&#39; log_valid_accuracy = f&#39;val_accuracy: {epoch_valid_accuracy:.4f}&#39; log_string += f&#39; - {log_valid_loss:&lt;13s} - {log_valid_accuracy:&lt;20s}&#39; history[&#39;loss&#39;].append(epoch_valid_loss) history[&#39;accuracy&#39;].append(epoch_valid_accuracy) # this log_string should include validation results print(log_string, end=&#39; n&#39;) return history . Execution . Okay, time to put everything together and train the model. One thing that might require a little explanation is the line . grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X)))) . The inner lambda is the function that uses the model, image data, and labels to calculate the cross-entropy loss. By default jax.grad will calculate the gradient with respect to this function&#39;s first argument, model (or equivalently it&#39;s flattened pytree representation). This is precisely what we want. To speed up the calculation the gradient is compiled with jax.jit. . (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() train_dataset = Dataset(X_train, y_train) valid_dataset = Dataset(X_test, y_test) train_datasource = Dataloader(train_dataset, batchsize=32) valid_datasource = Dataloader(valid_dataset, batchsize=32) network = Sequential( rescale_image, flatten, Linear(28*28, 128), relu, Linear(128, 10), softmax ) grad_fn = jax.jit(jax.grad(lambda model, X, y: sparse_cross_entropy(y, model(X)))) history = train( num_epochs=5, train_datasource=train_datasource, valid_datasource=valid_datasource, optimizer=Adam(network, lr=1e-3), loss_fn=sparse_cross_entropy, model=network, grad_fn=grad_fn ) . Epoch 1/5 1874/1875 [===============================] - 3s 1.51ms/batch - loss: 0.4956 - accuracy: 0.8250 - val_loss: 0.4446 - val_accuracy: 0.8396 Epoch 2/5 1874/1875 [===============================] - 2s 1.31ms/batch - loss: 0.3716 - accuracy: 0.8654 - val_loss: 0.4271 - val_accuracy: 0.8446 Epoch 3/5 1874/1875 [===============================] - 2s 1.49ms/batch - loss: 0.3330 - accuracy: 0.8791 - val_loss: 0.4034 - val_accuracy: 0.8528 Epoch 4/5 1874/1875 [===============================] - 2s 1.48ms/batch - loss: 0.3079 - accuracy: 0.8871 - val_loss: 0.3797 - val_accuracy: 0.8616 Epoch 5/5 1874/1875 [===============================] - 2s 1.38ms/batch - loss: 0.2891 - accuracy: 0.8935 - val_loss: 0.3599 - val_accuracy: 0.8710 . Keras Execution . For comparison, here&#39;s the training results for Keras on the same dataset. . (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() model = tf.keras.Sequential([ tf.keras.layers.Rescaling(1/255.0), tf.keras.layers.Flatten(input_shape=(28,28)), tf.keras.layers.Dense(128, activation=tf.keras.activations.relu), tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax) ]) model.compile( loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[&#39;accuracy&#39;]) history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5) . Epoch 1/5 1875/1875 [==============================] - 2s 988us/step - loss: 0.4928 - accuracy: 0.8269 - val_loss: 0.4533 - val_accuracy: 0.8408 Epoch 2/5 1875/1875 [==============================] - 2s 930us/step - loss: 0.3749 - accuracy: 0.8657 - val_loss: 0.4262 - val_accuracy: 0.8467 Epoch 3/5 1875/1875 [==============================] - 2s 947us/step - loss: 0.3381 - accuracy: 0.8777 - val_loss: 0.3721 - val_accuracy: 0.8667 Epoch 4/5 1875/1875 [==============================] - 2s 932us/step - loss: 0.3130 - accuracy: 0.8853 - val_loss: 0.3600 - val_accuracy: 0.8677 Epoch 5/5 1875/1875 [==============================] - 2s 918us/step - loss: 0.2956 - accuracy: 0.8909 - val_loss: 0.3501 - val_accuracy: 0.8738 . I&#39;m pretty pleased with how closely my output follows the Keras output. The reason there isn&#39;t an even better match possibly stems from different parameter initialization techniques. I&#39;m currently limited to Kaiming initialization, whereas Dense layers in Keras use Kaiming uniform by default. I could test this hypothesis by directly loading the parameters in my model with the Keras parameters. Maybe I&#39;ll try this in another day. . Conclusion . JAX is a potent piece of technology. I&#39;m planning to use it, and explore how it works, in more detail over the coming months. Overtime, I hope to take the parts and pieces implemented here and build a more cohesive library that can be used to solve problems that interest me. .",
            "url": "https://jrreed83.github.io/technicalities/deep%20learning/jax/2022/07/26/jax-00-fashion-mnist.html",
            "relUrl": "/deep%20learning/jax/2022/07/26/jax-00-fashion-mnist.html",
            "date": " • Jul 26, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Linear Regression with JAX",
            "content": "In this notebook, JAX will be used to solve a basic linear regression problem: given inputs $X$ and targets $y$, find the weights $w$ and bias $b$ that minimizes the loss function. . $$ ell (w, b) = | X w + b - y | ^ 2 $$In the process of solving this problem, I&#39;ll slowly build up a basic neural network library. Let me say upfront that I&#39;m new to JAX, but it looks like a very elegant library. . import jax import jax.numpy as jnp import numpy as np import matplotlib.pyplot as plt . Generate Data . The first thing I need to to is generate some mock data satisfying a linear hyperplane . $$ y = x cdot w + b $$Here&#39;s a little utility function we can use for this purpose. . def generate_data(w, b, num_samples = 10): seed = 3551 key = jax.random.PRNGKey(seed) X = jax.random.normal(key, (num_samples, 3)) y = jnp.dot(X, w) + b return X, y . Model Definition . We use a simple linear model, with the parameters (weights and biases) stored in a simple python dictionary, named params. Although it may not be clear in the function definition, the x argument should be interpreted as a single input sample, rather than a mini-batch of samples. Applying the model to a batch of samples will be handled in the loss function below. . def linear_model(params, x): w = params[&#39;w&#39;] b = params[&#39;b&#39;] return jnp.dot(x, w) + b . Loss Function . The loss function takes the current set of parameters, the model, and the data and calculates the distance between the model predictions and the targets. . @jax.value_and_grad def mse_loss(params, model, X, y): l = jax.vmap(model, in_axes=(None, 0))(params, X) return jnp.mean((l - y)**2) . By decorating the loss with @jax.value_and_grad annotation, we&#39;re telling the JAX system that the value and gradient should be returned. Note also that the model passed into the loss is transformed into a vectorized model with jax.vmap. The in_axes argument is a tuple whose length matches the number of arguments passed into model (in this case 2). Each tuple element indicates which dimension should be vectorized for the corresponding function argument. In our case, X is an $m times 3$ array ($m$ indicating the number of samples in the batch). Therefore, in_axes is set to (None, 0) because we don&#39;t want to vectorize over params, but we do want to vectorize the zeroth dimension of X. . As an aside, I don&#39;t like the fact that params and model are both passed as arguments to the loss function. Eventually, I&#39;d like to keep the params dictionary in some sort of model object, pass the model to the loss, and have the model implementation handle all the parameter specific stuff. This is how all of the JAX-based neural libraries (Haiku, Flax, Equinox, etc seem to work.) . Training Loop . The training loop tunes the model parameters for a specified number of epochs. For now the parameters are adjusted once per epoch, after the entire dataset is consumed by the model. . def train(data, lr=1e-2, model=linear_model, num_epochs=50, loss=mse_loss): X, y = data loss_vals = np.zeros(num_epochs) # initialize the model parameters params = {&#39;w&#39;: jnp.array([0,0,0], dtype=jnp.float32), &#39;b&#39;: 0.0} for i in range(num_epochs): loss_i, grad_params_i = loss(params, model, X, y) for key in params: params[key] -= lr*grad_params_i[key] loss_vals[i] = loss_i return loss_vals, params . X, y = generate_data(w = jnp.array([1.0, 2.0, 3.0]), b=4.0, num_samples=100) num_epochs, lr = 500, 1e-2 loss_values, params = train(data=(X, y), lr=lr, num_epochs=num_epochs, model=linear_model) . print(f&#39;weights = {params[&quot;w&quot;]}, bias = {params[&quot;b&quot;]}&#39;) . weights = [0.99990535 1.9992862 3.0005229 ], bias = 3.999452590942383 . It looks like the learned parameters are pretty close to the parameters used to generate the initial dataset. This provides a little assurance that the training loop is working properly. . Performance Curve . Let&#39;s see the trend in the loss function. . plt.rcParams[&#39;font.size&#39;] = 20 plt.figure(1, figsize=(20,5)) plt.plot(np.arange(num_epochs), loss_values, linewidth=2) plt.grid() plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Loss&#39;) plt.xlim(0, num_epochs) plt.tight_layout() . Conclusion . In future JAX-related posts, I&#39;m planning on using it to build more sophisticated systems. This might include solving concocted toy problems, implementing models from journal articles, or doing something new. I realize there are plenty of fairly mature libraries out there that can do this sort of thing, but I&#39;d rather implement software that I can customize to meet my needs. Of course this doesn&#39;t mean that I won&#39;t read code and borrow ideas from other developers. .",
            "url": "https://jrreed83.github.io/technicalities/jax/2022/07/09/2022-JAX-LinearRegression.html",
            "relUrl": "/jax/2022/07/09/2022-JAX-LinearRegression.html",
            "date": " • Jul 9, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jrreed83.github.io/technicalities/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jrreed83.github.io/technicalities/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Joey. I work for a small, but very capable company that builds situationally aware sensor systems using ideas from communications theory, signal processing, and machine learning. . This website is an archive of technical stuff that I want to remember. Every effort will be taken to make each article clear, concise, and useful. .",
          "url": "https://jrreed83.github.io/technicalities/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jrreed83.github.io/technicalities/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}